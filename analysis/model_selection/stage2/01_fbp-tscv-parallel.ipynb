{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross validation: Facebook's Prophet\n",
    "---\n",
    "\n",
    "Facebook Prophet with New Year holiday.\n",
    "\n",
    "This notebook conducts cross validation of the method using a rolling forecast origin method.\n",
    "\n",
    "**The notebook outputs:**\n",
    "* MASE, RMSE and MAPE at 7 day intervals from 7 to 84 days.\n",
    "* 80 and 95% prediction intervals between 7 and 84 days.\n",
    "\n",
    "These are saved into the folder `results/model_selection/stage2/`\n",
    "\n",
    "---\n",
    "\n",
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#for parallel running of CV\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "#error measures\n",
    "from forecast_tools.metrics import (mean_absolute_scaled_error, \n",
    "                                    root_mean_squared_error,\n",
    "                                    symmetric_mean_absolute_percentage_error)\n",
    "\n",
    "#models\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from fbprophet import Prophet\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to select exceptionally busy days as covariates.\n",
    "from amb_forecast.feature_engineering import (regular_busy_calender_days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#custom ensemble class\n",
    "from amb_forecast.ensemble import (Ensemble, UnweightedVote)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Input\n",
    "\n",
    "The constants `TOP_LEVEL`, `STAGE`, `REGION`,`TRUST` and `METHOD` are used to control data selection and the directory for outputting results.  \n",
    "\n",
    "> Output file is `f'{TOP_LEVEL}/{STAGE}/{REGION}-{METHOD}_{metric}.csv'`.  where metric will be smape, rmse, mase, coverage_80 and coverage_95. Note: `REGION`: is also used to select the correct data from the input dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SW subregions (notebook runs CV for subregions in parallel)\n",
    "regions = ['BNSSG',\n",
    "           'Cornwall',\n",
    "           'Devon',\n",
    "           'Dorset',\n",
    "           'Gloucestershire',\n",
    "           'Somerset',\n",
    "           'Wiltshire']\n",
    "\n",
    "TOP_LEVEL = '../../../results/model_selection'\n",
    "STAGE = 'stage2'\n",
    "TRUST = 'south_west'  \n",
    "METHOD = 'fbp'  # fbp-arima: ensemble; fbp: prophet only\n",
    "\n",
    "FILE_NAME = 'Daily_Responses_5_Years_2019_full.csv'\n",
    "\n",
    "#london data is tab delimited '\\t', the rest are comma ','\n",
    "DELIMITER = '\\t'\n",
    "\n",
    "#methods to include in ensemble\n",
    "INCLUDE_ARIMA = False\n",
    "INCLUDE_FBP = True\n",
    "\n",
    "#split training and validation data.\n",
    "TEST_SPLIT_DATE = '2019-01-01'\n",
    "\n",
    "#second subdivide: train and val\n",
    "VAL_SPLIT_DATE = '2017-07-01'\n",
    "\n",
    "#discard data after 2020 due to coronavirus\n",
    "#this is the subject of a seperate study.\n",
    "DISCARD_DATE = '2020-01-01'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../../data/Daily_Responses_5_Years_2019_full.csv\n"
     ]
    }
   ],
   "source": [
    "#read in path\n",
    "path = f'../../../data/{FILE_NAME}'\n",
    "print(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process_daily_data(path, index_col, by_col, \n",
    "                           values, dayfirst=False,sep=','):\n",
    "    '''\n",
    "    Daily data is stored in long format.  Read in \n",
    "    and pivot to wide format so that there is a single \n",
    "    colmumn for each regions time series.\n",
    "    '''\n",
    "    df = pd.read_csv(path, \n",
    "                     sep=sep,\n",
    "                     index_col=index_col, \n",
    "                     parse_dates=True, \n",
    "                     dayfirst=dayfirst)\n",
    "    \n",
    "    df.columns = map(str.lower, df.columns)\n",
    "    df.index.rename(str(df.index.name).lower(), inplace=True)\n",
    "    \n",
    "    clean_table = pd.pivot_table(df, values=values.lower(), \n",
    "                                 index=[index_col.lower()],\n",
    "                                 columns=[by_col.lower()], aggfunc=np.sum)\n",
    "    \n",
    "    clean_table.index.freq = 'D'\n",
    "    \n",
    "    return clean_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>ora</th>\n",
       "      <th>BNSSG</th>\n",
       "      <th>Cornwall</th>\n",
       "      <th>Devon</th>\n",
       "      <th>Dorset</th>\n",
       "      <th>Gloucestershire</th>\n",
       "      <th>OOA</th>\n",
       "      <th>Somerset</th>\n",
       "      <th>Trust</th>\n",
       "      <th>Wiltshire</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>actual_dt</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2013-12-30</th>\n",
       "      <td>415.0</td>\n",
       "      <td>220.0</td>\n",
       "      <td>502.0</td>\n",
       "      <td>336.0</td>\n",
       "      <td>129.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>183.0</td>\n",
       "      <td>2042.0</td>\n",
       "      <td>255.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-12-31</th>\n",
       "      <td>420.0</td>\n",
       "      <td>236.0</td>\n",
       "      <td>468.0</td>\n",
       "      <td>302.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>180.0</td>\n",
       "      <td>1996.0</td>\n",
       "      <td>260.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-01-01</th>\n",
       "      <td>549.0</td>\n",
       "      <td>341.0</td>\n",
       "      <td>566.0</td>\n",
       "      <td>392.0</td>\n",
       "      <td>157.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>213.0</td>\n",
       "      <td>2570.0</td>\n",
       "      <td>351.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-01-02</th>\n",
       "      <td>450.0</td>\n",
       "      <td>218.0</td>\n",
       "      <td>499.0</td>\n",
       "      <td>301.0</td>\n",
       "      <td>115.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>167.0</td>\n",
       "      <td>2013.0</td>\n",
       "      <td>258.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-01-03</th>\n",
       "      <td>419.0</td>\n",
       "      <td>229.0</td>\n",
       "      <td>503.0</td>\n",
       "      <td>304.0</td>\n",
       "      <td>135.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>195.0</td>\n",
       "      <td>2056.0</td>\n",
       "      <td>269.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "ora         BNSSG  Cornwall  Devon  Dorset  Gloucestershire  OOA  Somerset  \\\n",
       "actual_dt                                                                    \n",
       "2013-12-30  415.0     220.0  502.0   336.0            129.0  NaN     183.0   \n",
       "2013-12-31  420.0     236.0  468.0   302.0            128.0  NaN     180.0   \n",
       "2014-01-01  549.0     341.0  566.0   392.0            157.0  NaN     213.0   \n",
       "2014-01-02  450.0     218.0  499.0   301.0            115.0  NaN     167.0   \n",
       "2014-01-03  419.0     229.0  503.0   304.0            135.0  NaN     195.0   \n",
       "\n",
       "ora          Trust  Wiltshire  \n",
       "actual_dt                      \n",
       "2013-12-30  2042.0      255.0  \n",
       "2013-12-31  1996.0      260.0  \n",
       "2014-01-01  2570.0      351.0  \n",
       "2014-01-02  2013.0      258.0  \n",
       "2014-01-03  2056.0      269.0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean = pre_process_daily_data(path, 'Actual_dt', 'ORA', 'Actual_Value', \n",
    "                               dayfirst=False)\n",
    "clean.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ts_train_test_split(data, split_date):\n",
    "    '''\n",
    "    Split time series into training and test data\n",
    "    \n",
    "    Parameters:\n",
    "    -------\n",
    "    data - pd.DataFrame - time series data.  Index expected as datatimeindex\n",
    "    split_date - the date on which to split the time series\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple (len=2) \n",
    "    0. pandas.DataFrame - training dataset\n",
    "    1. pandas.DataFrame - test dataset\n",
    "    '''\n",
    "    train = data.loc[data.index < split_date]\n",
    "    test = data.loc[data.index >= split_date]\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = ts_train_test_split(clean, split_date=TEST_SPLIT_DATE)\n",
    "\n",
    "#exclude data after 2020 due to coronavirus.\n",
    "test, discard = ts_train_test_split(test, split_date=DISCARD_DATE)\n",
    "\n",
    "#train split into train and validation\n",
    "train, val = ts_train_test_split(train, split_date=VAL_SPLIT_DATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1279, 9)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#amount of training data\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(549, 9)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#amount of validation data\n",
    "val.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New years day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "exceptional = regular_busy_calender_days(train[regions[0]], quantile=0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_year = pd.DataFrame({\n",
    "                         'holiday': 'new_year',\n",
    "                         'ds': pd.date_range(start=exceptional[0], \n",
    "                                             periods=20, \n",
    "                                             freq='YS')\n",
    "                        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>holiday</th>\n",
       "      <th>ds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>new_year</td>\n",
       "      <td>2013-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>new_year</td>\n",
       "      <td>2014-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>new_year</td>\n",
       "      <td>2015-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>new_year</td>\n",
       "      <td>2016-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>new_year</td>\n",
       "      <td>2017-01-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    holiday         ds\n",
       "0  new_year 2013-01-01\n",
       "1  new_year 2014-01-01\n",
       "2  new_year 2015-01-01\n",
       "3  new_year 2016-01-01\n",
       "4  new_year 2017-01-01"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_year.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wrapper classes for Prophet and statsmodels ARIMA\n",
    "\n",
    "Adapter/wrapper classes to enable usage within `Ensemble` class and work with cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FbProphetWrapper:\n",
    "    '''\n",
    "    Facade for FBProphet object - so that it can be\n",
    "    used within Ensemble with methods from other packages\n",
    "\n",
    "    '''\n",
    "    def __init__(self, training_index, holidays=None, interval_width=0.8,\n",
    "                 mcmc_samples=0, changepoint_prior_scale=0.05):\n",
    "        self._training_index = training_index\n",
    "        self._holidays = holidays\n",
    "        self._interval_width = interval_width\n",
    "        self._mcmc_samples = mcmc_samples\n",
    "        self._cp_prior_scale = changepoint_prior_scale\n",
    "\n",
    "    def _get_resids(self):\n",
    "        return self._train - self._forecast['yhat'][:-self._h]\n",
    "\n",
    "    def _get_preds(self):\n",
    "        return self._forecast['yhat'][:-self._h].to_numpy()\n",
    "\n",
    "    def fit(self, train):\n",
    "        \n",
    "        self._model = Prophet(holidays=self._holidays, \n",
    "                              interval_width=self._interval_width,\n",
    "                              mcmc_samples=self._mcmc_samples,\n",
    "                              changepoint_prior_scale=self._cp_prior_scale,\n",
    "                              daily_seasonality=False)\n",
    "        \n",
    "        \n",
    "        self._model.fit(self._pre_process_training(train))\n",
    "        self._t = len(train)\n",
    "        self._train = train\n",
    "        self.predict(len(train))\n",
    "\n",
    "    def _pre_process_training(self, train):\n",
    "\n",
    "        if len(train.shape) > 1:\n",
    "            y_train = train[:, 0]\n",
    "        else:\n",
    "            y_train = train\n",
    "\n",
    "        y_train = np.asarray(y_train)\n",
    "            \n",
    "        #extend the training index\n",
    "        if len(y_train) > len(self._training_index):\n",
    "            self._training_index = pd.date_range(start=self._training_index[0], \n",
    "                                                 periods=len(y_train),\n",
    "                                                 freq=self._training_index.freq)\n",
    "        \n",
    "        \n",
    "        prophet_train = pd.DataFrame(self._training_index)\n",
    "        prophet_train['y'] = y_train\n",
    "        prophet_train.columns = ['ds', 'y']\n",
    "        \n",
    "        return prophet_train\n",
    "\n",
    "    def predict(self, h, return_conf_int=False, alpha=0.2):\n",
    "        '''\n",
    "        forecast h steps ahead.\n",
    "        \n",
    "        Params:\n",
    "        ------\n",
    "        h: int\n",
    "            h-step forecast\n",
    "        \n",
    "        return_conf_int: bool, optional (default=False)\n",
    "            return 1 - alpha PI\n",
    "        \n",
    "        alpha: float, optional (default=0.2)\n",
    "            return 1 - alpha PI\n",
    "                       \n",
    "        Returns:\n",
    "        -------\n",
    "        np.array\n",
    "            If return_conf_int = False returns preds only\n",
    "            \n",
    "        np.array, np.array\n",
    "            If return_conf_int = True returns tuple of preds, pred_ints\n",
    "        '''\n",
    "        if isinstance(h, (np.ndarray, pd.DataFrame)):\n",
    "            h = len(h)\n",
    "        \n",
    "        self._h = h\n",
    "        future = self._model.make_future_dataframe(periods=h)\n",
    "        self._forecast = self._model.predict(future)\n",
    "\n",
    "        if return_conf_int:\n",
    "            return (self._forecast['yhat'][-h:].to_numpy(), \n",
    "                    self._forecast[['yhat_lower', 'yhat_upper']][-h:].to_numpy())\n",
    "        else:\n",
    "            return self._forecast['yhat'][-h:].to_numpy()\n",
    "            \n",
    "\n",
    "    fittedvalues = property(_get_preds)\n",
    "    resid = property(_get_resids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ARIMAWrapper(object):\n",
    "    '''\n",
    "    Facade for statsmodels.statespace\n",
    "    '''\n",
    "    def __init__(self, order, seasonal_order, training_index, holidays=None):\n",
    "        self._order = order\n",
    "        self._seasonal_order = seasonal_order\n",
    "        self._training_index = training_index\n",
    "        self._holidays = holidays\n",
    "\n",
    "    def _get_resids(self):\n",
    "        return self._fitted.resid\n",
    "\n",
    "    def _get_preds(self):\n",
    "        return self._fitted.fittedvalues\n",
    "    \n",
    "    def _encode_holidays(self, holidays, idx):\n",
    "        dummy = idx.isin(holidays).astype(int)\n",
    "        dummy = pd.DataFrame(dummy)\n",
    "        dummy.columns = ['holiday']\n",
    "        dummy.index = idx\n",
    "        return dummy\n",
    "\n",
    "    def fit(self, y_train):\n",
    "        \n",
    "        #extend training index\n",
    "        if len(y_train) > len(self._training_index):\n",
    "\n",
    "            self._training_index = pd.date_range(start=self._training_index[0], \n",
    "                                                 periods=len(y_train),\n",
    "                                                 freq=self._training_index.freq)\n",
    "            \n",
    "        holiday_train = None\n",
    "        if not self._holidays is None:\n",
    "            holiday_train = self._encode_holidays(self._holidays, \n",
    "                                                  self._training_index)\n",
    "    \n",
    "        \n",
    "        self._model = ARIMA(endog=y_train,\n",
    "                            exog=holiday_train,\n",
    "                            order=self._order, \n",
    "                            seasonal_order=self._seasonal_order,\n",
    "                            enforce_stationarity=False)\n",
    "        \n",
    "        #could be modified to catch LU decomp error?\n",
    "        self._fitted = self._model.fit()\n",
    "        self._t = len(train)\n",
    "        \n",
    "    \n",
    "    def predict(self, horizon, return_conf_int=False, alpha=0.2):\n",
    "        '''\n",
    "        forecast h steps ahead.\n",
    "        \n",
    "        Params:\n",
    "        ------\n",
    "        h: int\n",
    "            h-step forecast\n",
    "        \n",
    "        return_conf_int: bool, optional (default=False)\n",
    "            return 1 - alpha PI\n",
    "        \n",
    "        alpha: float, optional (default=0.2)\n",
    "            return 1 - alpha PI\n",
    "                       \n",
    "        Returns:\n",
    "        -------\n",
    "        np.array\n",
    "            If return_conf_int = False returns preds only\n",
    "            \n",
    "        np.array, np.array\n",
    "            If return_conf_int = True returns tuple of preds, pred_ints\n",
    "        '''\n",
    "        \n",
    "        #+1 to date range then trim off the first value\n",
    "\n",
    "        f_idx = pd.date_range(start=self._training_index[-1], \n",
    "                              periods=horizon+1,\n",
    "                              freq=self._training_index.freq)[1:]\n",
    "        \n",
    "        #encode holidays if included.\n",
    "        exog_holiday = None\n",
    "        if not self._holidays is None:\n",
    "            exog_holiday = self._encode_holidays(self._holidays, f_idx)\n",
    "        \n",
    "    \n",
    "        forecast = self._fitted.get_forecast(horizon, exog=exog_holiday)\n",
    "        mean_forecast = forecast.summary_frame()['mean'].to_numpy()\n",
    "        \n",
    "        if return_conf_int:\n",
    "            df = forecast.summary_frame(alpha=alpha)\n",
    "            pi = df[['mean_ci_lower', 'mean_ci_upper']].to_numpy()\n",
    "            return mean_forecast, pi\n",
    "            \n",
    "        \n",
    "        else:\n",
    "            return mean_forecast\n",
    "\n",
    "    fittedvalues = property(_get_preds)\n",
    "    resid = property(_get_resids)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example of fitting the ensemble\n",
    "1. Regression with New Year Holiday and Auto ARIMA errors\n",
    "2. FBProphet with new years day holiday.\n",
    "\n",
    "The code below demonstrates how to fit the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1 = FbProphetWrapper(training_index=train.index, \n",
    "                           holidays=new_year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimators = {'fbp': model_1}\n",
    "ens = Ensemble(estimators, UnweightedVote())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit to training data in chosen region\n",
    "ens.fit(train[regions[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict 7 days ahead\n",
    "H = 7\n",
    "ens_preds = ens.predict(horizon=H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([521.68570355, 509.71188428, 481.68412198, 460.80217661,\n",
       "       459.26660331, 465.93804988, 477.92087287])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#view predictions\n",
    "ens_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with prediction intervals\n",
    "ens_preds, pi = ens.predict(horizon=H, return_conf_int=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([521.68570355, 509.71188428, 481.68412198, 460.80217661,\n",
       "       459.26660331, 465.93804988, 477.92087287])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ens_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[491.28619579, 555.90432881],\n",
       "       [478.18449613, 542.03855281],\n",
       "       [447.43024087, 514.50529019],\n",
       "       [429.34934024, 492.13412941],\n",
       "       [425.79561608, 494.12547608],\n",
       "       [433.67033282, 497.93074704],\n",
       "       [448.28731408, 513.3702458 ]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross validation functions\n",
    "\n",
    "`time_series_cv` implements rolling forecast origin cross validation for time series.  \n",
    "It does not calculate forecast error, but instead returns the predictions, pred intervals and actuals in an array that can be passed to any forecast error function. (this is for efficiency and allows additional metrics to be calculated if needed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_series_cv(model, train, val, horizons, alpha=0.2, step=1):\n",
    "    '''\n",
    "    Time series cross validation across multiple horizons for a single model.\n",
    "\n",
    "    Incrementally adds additional training data to the model and tests\n",
    "    across a provided list of forecast horizons. Note that function tests a\n",
    "    model only against complete validation sets.  E.g. if horizon = 15 and \n",
    "    len(val) = 12 then no testing is done.  In the case of multiple horizons\n",
    "    e.g. [7, 14, 28] then the function will use the maximum forecast horizon\n",
    "    to calculate the number of iterations i.e if len(val) = 365 and step = 1\n",
    "    then no. iterations = len(val) - max(horizon) = 365 - 28 = 337.\n",
    "    \n",
    "    Parameters:\n",
    "    --------\n",
    "    model - forecasting model\n",
    "\n",
    "    train - np.array - vector of training data\n",
    "\n",
    "    val - np.array - vector of validation data\n",
    "\n",
    "    horizons - list of ints, forecast horizon e.g. [7, 14, 28] days\n",
    "    \n",
    "    alpha - float, optional (default=0.2)\n",
    "        1 - alpha prediction interval specification\n",
    "\n",
    "    step -- int, optional (default=1)\n",
    "            step taken in cross validation \n",
    "            e.g. 1 in next cross validation training data includes next point \n",
    "            from the validation set.\n",
    "            e.g. 7 in the next cross validation training data includes next 7 points\n",
    "            (default=1)\n",
    "            \n",
    "    Returns:\n",
    "    -------\n",
    "    np.array, np.array, np.array\n",
    "        - cv_preds, cv_test, cv_intervals\n",
    "    '''\n",
    "    \n",
    "    #point forecasts\n",
    "    cv_preds = [] \n",
    "    #ground truth observations\n",
    "    cv_actuals = [] \n",
    "    #prediction intervals\n",
    "    cv_pis = []\n",
    "    \n",
    "    split = 0\n",
    "\n",
    "    print('split => ', end=\"\")\n",
    "    for i in range(0, len(val) - max(horizons) + 1, step):\n",
    "        split += 1\n",
    "        print(f'{split}, ', end=\"\")\n",
    "                \n",
    "        train_cv = np.concatenate([train, val[:i]], axis=0)\n",
    "        model.fit(train_cv)\n",
    "        \n",
    "        #predict the maximum horizon \n",
    "        preds, pis = model.predict(horizon=len(val[i:i+max(horizons)]), \n",
    "                                   return_conf_int=True,\n",
    "                                   alpha=alpha)        \n",
    "        cv_h_preds = []\n",
    "        cv_test = []\n",
    "        cv_h_pis = []\n",
    "        \n",
    "        #sub horizon calculations\n",
    "        for h in horizons:\n",
    "            #store the h-step prediction\n",
    "            cv_h_preds.append(preds[:h])\n",
    "            #store the h-step actual value\n",
    "            cv_test.append(val.iloc[i:i+h])    \n",
    "            cv_h_pis.append(pis[:h])\n",
    "                     \n",
    "        cv_preds.append(cv_h_preds)\n",
    "        cv_actuals.append(cv_test)\n",
    "        cv_pis.append(cv_h_pis)\n",
    "        \n",
    "    print('done.\\n')        \n",
    "    return cv_preds, cv_actuals, cv_pis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom functions for calculating CV scores for point predictions and coverage.\n",
    "\n",
    "These functions have been written to work with the output of `time_series_cv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_cv_error(cv_preds, cv_test, error_func):\n",
    "    '''\n",
    "    Forecast error in the current split\n",
    "    \n",
    "    Params:\n",
    "    -----\n",
    "    cv_preds, np.array\n",
    "        Split predictions\n",
    "        \n",
    "    \n",
    "    cv_test: np.array\n",
    "        acutal ground truth observations\n",
    "        \n",
    "    error_func: object\n",
    "        function with signature (y_true, y_preds)\n",
    "        \n",
    "    Returns:\n",
    "    -------\n",
    "        np.ndarray\n",
    "            cross validation errors for split\n",
    "    '''\n",
    "    n_splits = len(cv_preds)\n",
    "    cv_errors = []\n",
    "    \n",
    "    for split in range(n_splits):\n",
    "        pred_error = error_func(cv_test[split], cv_preds[split])\n",
    "        cv_errors.append(pred_error)\n",
    "        \n",
    "    return np.array(cv_errors)\n",
    "\n",
    "def forecast_errors_cv(cv_preds, cv_test, error_func):\n",
    "    '''\n",
    "    Forecast errors by forecast horizon\n",
    "    \n",
    "    Params:\n",
    "    ------\n",
    "    cv_preds: np.ndarray\n",
    "        Array of arrays.  Each array is of size h representing\n",
    "        the forecast horizon specified.\n",
    "        \n",
    "    cv_test: np.ndarray\n",
    "        Array of arrays.  Each array is of size h representing\n",
    "        the forecast horizon specified.\n",
    "        \n",
    "    error_func: object\n",
    "        function with signature (y_true, y_preds)\n",
    "        \n",
    "    Returns:\n",
    "    -------\n",
    "    np.ndarray\n",
    "        \n",
    "    '''\n",
    "    cv_test = np.array(cv_test)\n",
    "    cv_preds = np.array(cv_preds)\n",
    "    n_horizons = len(cv_test)    \n",
    "    \n",
    "    horizon_errors = []\n",
    "    for h in range(n_horizons):\n",
    "        split_errors = split_cv_error(cv_preds[h], cv_test[h], error_func)\n",
    "        horizon_errors.append(split_errors)\n",
    "\n",
    "    return np.array(horizon_errors)\n",
    "\n",
    "def split_coverage(cv_test, cv_intervals):\n",
    "    n_splits = len(cv_test)\n",
    "    cv_errors = []\n",
    "        \n",
    "    for split in range(n_splits):\n",
    "        val = np.asarray(cv_test[split])\n",
    "        lower = cv_intervals[split].T[0]\n",
    "        upper = cv_intervals[split].T[1]\n",
    "        \n",
    "        coverage = len(np.where((val > lower) & (val < upper))[0])\n",
    "        coverage = coverage / len(val)\n",
    "        \n",
    "        cv_errors.append(coverage)\n",
    "        \n",
    "    return np.array(cv_errors)\n",
    "    \n",
    "    \n",
    "def prediction_int_coverage_cv(cv_test, cv_intervals):\n",
    "    cv_test = np.array(cv_test)\n",
    "    cv_intervals = np.array(cv_intervals)\n",
    "    n_horizons = len(cv_test)    \n",
    "    \n",
    "    horizon_coverage = []\n",
    "    for h in range(n_horizons):\n",
    "        split_coverages = split_coverage(cv_test[h], cv_intervals[h])\n",
    "        horizon_coverage.append(split_coverages)\n",
    "\n",
    "    return np.array(horizon_coverage)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_cv_error_scaled(cv_preds, cv_test, y_train):\n",
    "    n_splits = len(cv_preds)\n",
    "    cv_errors = []\n",
    "    \n",
    "    for split in range(n_splits):\n",
    "        pred_error = mean_absolute_scaled_error(cv_test[split], cv_preds[split], \n",
    "                                                y_train, period=7)\n",
    "        \n",
    "        cv_errors.append(pred_error)\n",
    "        \n",
    "    return np.array(cv_errors)\n",
    "\n",
    "def forecast_errors_cv_scaled(cv_preds, cv_test, y_train):\n",
    "    cv_test = np.array(cv_test)\n",
    "    cv_preds = np.array(cv_preds)\n",
    "    n_horizons = len(cv_test)    \n",
    "    \n",
    "    horizon_errors = []\n",
    "    for h in range(n_horizons):\n",
    "        split_errors = split_cv_error_scaled(cv_preds[h], cv_test[h], y_train)\n",
    "        horizon_errors.append(split_errors)\n",
    "        \n",
    "    return np.array(horizon_errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ensemble(fb_interval=0.8, include_arima=True,\n",
    "                 include_prophet=True):\n",
    "    '''\n",
    "    Create ensemble model. Simple average.\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    fb_interval: float, optional (default=0.8)\n",
    "        precision of confidence interval\n",
    "        \n",
    "    include_arima: bool, optional (default=True)\n",
    "        Ensemble model include Regression with ARIMA errors (new years day)\n",
    "        \n",
    "    include_prophet: bool, optional (default=True)\n",
    "        Ensemble model includes Prophet model\n",
    "        \n",
    "    Returns:\n",
    "    ------\n",
    "    Ensemble\n",
    "    \n",
    "    '''\n",
    "        \n",
    "    model_1 = ARIMAWrapper(order=(1,1,3), seasonal_order=(1,0,1,7), \n",
    "                          training_index=train.index,\n",
    "                          holidays=new_year['ds'].tolist())\n",
    "        \n",
    "    model_2 = FbProphetWrapper(training_index=train.index, \n",
    "                           holidays=new_year, interval_width=fb_interval)\n",
    "    \n",
    "    estimators = {}\n",
    "    \n",
    "    if include_arima:\n",
    "        estimators['arima'] = model_1\n",
    "    \n",
    "    if include_prophet:\n",
    "        estimators['fbp'] = model_2\n",
    "        \n",
    "    return Ensemble(estimators, UnweightedVote())\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run cross validation\n",
    "This is run twices once each for 80 and 95% prediction intervals.\n",
    "\n",
    "> note this version of the notebook runs ALL subregions specified in the `regions` list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_region_cv(train, val, region, horizons, alpha, step=7, output_pf=True,\n",
    "                     output_cov=True):\n",
    "    '''\n",
    "    Rolling origin forecast time series cross validation of a single geo region.\n",
    "    \n",
    "    Params:\n",
    "    -------\n",
    "    train - pd.Series\n",
    "        Initial training data\n",
    "        \n",
    "    val: pd.Series\n",
    "        Validation data (gradually used up as CV executes)\n",
    "    \n",
    "    region: str\n",
    "        The name of the region\n",
    "        \n",
    "    horizons: list\n",
    "        A list of integer time steps to predict ahead.\n",
    "    \n",
    "    alpha: float\n",
    "        1 - alpha prediction interval to generate\n",
    "    \n",
    "    step: int, optional (default=7)\n",
    "        stride of cross validation.  I.e. spacing between folds\n",
    "        \n",
    "    output_pf: bool, optional (default=True)\n",
    "        write point forecast results to file\n",
    "        \n",
    "    output_cov: bool, optional (default=True)\n",
    "        write pi coverage to file\n",
    "    \n",
    "    Returns:\n",
    "    -------\n",
    "    dict\n",
    "        str: pd.DataFrame\n",
    "        where str is the name of the metric.\n",
    "    '''\n",
    "    #get the model\n",
    "    model = get_ensemble(fb_interval=1-alpha,\n",
    "                         include_arima=INCLUDE_ARIMA, \n",
    "                         include_prophet=INCLUDE_FBP)\n",
    "\n",
    "    #cv\n",
    "    results = time_series_cv(model, train, val, horizons, \n",
    "                             alpha=alpha, step=step)\n",
    "    \n",
    "    #smape\n",
    "    cv_preds, cv_test, cv_intervals = results\n",
    "    cv_errors = forecast_errors_cv(cv_preds, cv_test, \n",
    "                                   symmetric_mean_absolute_percentage_error)\n",
    "    df_smape = pd.DataFrame(cv_errors)\n",
    "    df_smape.columns = horizons\n",
    "\n",
    "    #rmse\n",
    "    cv_errors = forecast_errors_cv(cv_preds, cv_test, root_mean_squared_error)\n",
    "    df_rmse = pd.DataFrame(cv_errors)\n",
    "    df_rmse.columns = horizons\n",
    "    \n",
    "    #mase\n",
    "    cv_errors = forecast_errors_cv_scaled(cv_preds, cv_test, train)\n",
    "    df_mase = pd.DataFrame(cv_errors)\n",
    "    df_mase.columns = horizons\n",
    "    df_mase.describe()\n",
    "    \n",
    "    #coverage\n",
    "    cov = int((1 - alpha) * 100)\n",
    "    cv_coverage = prediction_int_coverage_cv(cv_test, cv_intervals)\n",
    "    df_cov = pd.DataFrame(cv_coverage)\n",
    "    df_cov.columns = horizons\n",
    "    \n",
    "    #write to file.\n",
    "    if output_pf:\n",
    "        df_smape.to_csv(f'{TOP_LEVEL}/{STAGE}/{region}-{METHOD}_smape.csv')\n",
    "        df_rmse.to_csv(f'{TOP_LEVEL}/{STAGE}/{region}-{METHOD}_rmse.csv')\n",
    "        df_mase.to_csv(f'{TOP_LEVEL}/{STAGE}/{region}-{METHOD}_mase.csv')\n",
    "        \n",
    "    if output_cov:\n",
    "        df_cov.to_csv(f'{TOP_LEVEL}/{STAGE}/{region}-{METHOD}_coverage_{cov}.csv')\n",
    "        \n",
    "    results_in_dict = {'smape': df_smape,\n",
    "                       'rmse': df_rmse,\n",
    "                       'mase': df_mase,\n",
    "                       'coverage_{cov}': df_cov}\n",
    "    \n",
    "    return results_in_dict\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_region_cv(regions, train, val, horizons, alpha, step=7, n_jobs=-1,\n",
    "                    output_pf=True, output_cov=True):\n",
    "    '''\n",
    "    Parallel execution of CV across multiple regions.\n",
    "    Rolling origin forecast time series cross validation.\n",
    "    \n",
    "    \n",
    "    Params:\n",
    "    -------\n",
    "    train - pd.Series\n",
    "        Initial training data\n",
    "        \n",
    "    val: pd.Series\n",
    "        Validation data (gradually used up as CV executes)\n",
    "        \n",
    "    alpha: float\n",
    "        1 - alpha prediction interval to generate\n",
    "    \n",
    "    step: int, optional (default=7)\n",
    "        stride of cross validation.  I.e. spacing between folds\n",
    "        \n",
    "    output_pf: bool, optional (default=True)\n",
    "        write point forecast results to file\n",
    "        \n",
    "    output_cov: bool, optional (default=True)\n",
    "        write pi coverage to file\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    list of results ordered by region.  Each list item is a dict:\n",
    "        str: pd.DataFrame\n",
    "        where str is the name of the metric.\n",
    "    '''\n",
    "    #parallel execution\n",
    "    res = Parallel(n_jobs=n_jobs)(delayed(single_region_cv)(train[region], \n",
    "                   val[region], region, horizons, alpha, step, output_pf, \n",
    "                   output_cov) for region in regions)\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Script to run trust level TSCV in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running TSCV: with 80% prediction intervals\n"
     ]
    }
   ],
   "source": [
    "horizons = [7, 14, 21, 28, 35, 42, 49, 56, 63, 70, 77, 84]\n",
    "\n",
    "print('Running TSCV: with 80% prediction intervals')\n",
    "\n",
    "#with 80% prediction intervals\n",
    "results_80 = multi_region_cv(regions, train, val, horizons, alpha=0.2, step=7, \n",
    "                             n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running TSCV: with 95% prediction intervals\n",
      "Analysis complete\n"
     ]
    }
   ],
   "source": [
    "print('Running TSCV: with 95% prediction intervals')\n",
    "\n",
    "#with 95% prediction intervals\n",
    "results_95 = multi_region_cv(regions, train, val, horizons, alpha=0.05, step=7, \n",
    "                             n_jobs=-1)\n",
    "\n",
    "print('Analysis complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
