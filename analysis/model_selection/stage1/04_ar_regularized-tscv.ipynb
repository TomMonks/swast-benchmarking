{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time series cross validation: Autoregression with seasonal indexes and holidays (with regularization)\n",
    "\n",
    "This notebook develops Autoregression models to forecast daily Ambulance Response numbers.  \n",
    "\n",
    "Autoregressive models have the following features:\n",
    "\n",
    "* They contain lagged dependent variables\n",
    "* As the model is a regression it can contain season dummy variables at multiple levels in order to capture multiple seasonalitys\n",
    "\n",
    "Note within this Notebook we will ignore correlated errors.  The most likely impact of this is that prediction intervals\n",
    "are too narrow.  A less likely, but possible outcome is that there may also be additional information that improve forecating in the errors.\n",
    "\n",
    "The final section of the notebook analyses prediction interval coverage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "from pandas.plotting import lag_plot\n",
    "import seaborn as sns\n",
    "\n",
    "#forecast error metrics\n",
    "from forecast_tools.metrics import (mean_absolute_scaled_error, \n",
    "                                    root_mean_squared_error,\n",
    "                                    symmetric_mean_absolute_percentage_error)\n",
    "\n",
    "\n",
    "#'sklearn'\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import Lasso, LinearRegression\n",
    "\n",
    "#statsmodels api\n",
    "import statsmodels.api as sm\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.11.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sm.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from amb_forecast.feature_engineering import featurize_time_series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Input\n",
    "\n",
    "The constants `TOP_LEVEL`, `STAGE`, `REGION`,`TRUST` and `METHOD` are used to control data selection and the directory for outputting results.  \n",
    "\n",
    "> Output file is `f'{TOP_LEVEL}/{STAGE}/{REGION}-{METHOD}_{metric}.csv'.csv`.  where metric will be smape, rmse, mase, coverage_80 and coverage_95. Note: `REGION`: is also used to select the correct data from the input dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOP_LEVEL = '../../../results/model_selection'\n",
    "STAGE = 'stage1'\n",
    "REGION = 'Trust'\n",
    "METHOD = 'elastic-net'\n",
    "\n",
    "FILE_NAME = 'Daily_Responses_5_Years_2019_full.csv'\n",
    "\n",
    "#split training and test data.\n",
    "TEST_SPLIT_DATE = '2019-01-01'\n",
    "\n",
    "#second subdivide: train and val\n",
    "VAL_SPLIT_DATE = '2017-07-01'\n",
    "\n",
    "#discard data after 2020 due to coronavirus\n",
    "#this is the subject of a seperate study.\n",
    "DISCARD_DATE = '2020-01-01'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in path\n",
    "path = f'../../../data/{FILE_NAME}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process_daily_data(path, index_col, by_col, \n",
    "                           values, dayfirst=False):\n",
    "    '''\n",
    "    Daily data is stored in long format.  Read in \n",
    "    and pivot to wide format so that there is a single \n",
    "    colmumn for each regions time series.\n",
    "    '''\n",
    "    df = pd.read_csv(path, index_col=index_col, parse_dates=True, \n",
    "                     dayfirst=dayfirst)\n",
    "    df.columns = map(str.lower, df.columns)\n",
    "    df.index.rename(str(df.index.name).lower(), inplace=True)\n",
    "    \n",
    "    clean_table = pd.pivot_table(df, values=values.lower(), \n",
    "                                 index=[index_col.lower()],\n",
    "                                 columns=[by_col.lower()], aggfunc=np.sum)\n",
    "    \n",
    "    clean_table.index.freq = 'D'\n",
    "    \n",
    "    return clean_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>ora</th>\n",
       "      <th>BNSSG</th>\n",
       "      <th>Cornwall</th>\n",
       "      <th>Devon</th>\n",
       "      <th>Dorset</th>\n",
       "      <th>Gloucestershire</th>\n",
       "      <th>OOA</th>\n",
       "      <th>Somerset</th>\n",
       "      <th>Trust</th>\n",
       "      <th>Wiltshire</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>actual_dt</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2013-12-30</th>\n",
       "      <td>415.0</td>\n",
       "      <td>220.0</td>\n",
       "      <td>502.0</td>\n",
       "      <td>336.0</td>\n",
       "      <td>129.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>183.0</td>\n",
       "      <td>2042.0</td>\n",
       "      <td>255.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-12-31</th>\n",
       "      <td>420.0</td>\n",
       "      <td>236.0</td>\n",
       "      <td>468.0</td>\n",
       "      <td>302.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>180.0</td>\n",
       "      <td>1996.0</td>\n",
       "      <td>260.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-01-01</th>\n",
       "      <td>549.0</td>\n",
       "      <td>341.0</td>\n",
       "      <td>566.0</td>\n",
       "      <td>392.0</td>\n",
       "      <td>157.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>213.0</td>\n",
       "      <td>2570.0</td>\n",
       "      <td>351.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-01-02</th>\n",
       "      <td>450.0</td>\n",
       "      <td>218.0</td>\n",
       "      <td>499.0</td>\n",
       "      <td>301.0</td>\n",
       "      <td>115.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>167.0</td>\n",
       "      <td>2013.0</td>\n",
       "      <td>258.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-01-03</th>\n",
       "      <td>419.0</td>\n",
       "      <td>229.0</td>\n",
       "      <td>503.0</td>\n",
       "      <td>304.0</td>\n",
       "      <td>135.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>195.0</td>\n",
       "      <td>2056.0</td>\n",
       "      <td>269.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "ora         BNSSG  Cornwall  Devon  Dorset  Gloucestershire  OOA  Somerset  \\\n",
       "actual_dt                                                                    \n",
       "2013-12-30  415.0     220.0  502.0   336.0            129.0  NaN     183.0   \n",
       "2013-12-31  420.0     236.0  468.0   302.0            128.0  NaN     180.0   \n",
       "2014-01-01  549.0     341.0  566.0   392.0            157.0  NaN     213.0   \n",
       "2014-01-02  450.0     218.0  499.0   301.0            115.0  NaN     167.0   \n",
       "2014-01-03  419.0     229.0  503.0   304.0            135.0  NaN     195.0   \n",
       "\n",
       "ora          Trust  Wiltshire  \n",
       "actual_dt                      \n",
       "2013-12-30  2042.0      255.0  \n",
       "2013-12-31  1996.0      260.0  \n",
       "2014-01-01  2570.0      351.0  \n",
       "2014-01-02  2013.0      258.0  \n",
       "2014-01-03  2056.0      269.0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean = pre_process_daily_data(path, 'Actual_dt', 'ORA', 'Actual_Value', \n",
    "                               dayfirst=False)\n",
    "clean.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ts_train_test_split(data, split_date):\n",
    "    '''\n",
    "    Split time series into training and test data\n",
    "    \n",
    "    Parameters:\n",
    "    -------\n",
    "    data - pd.DataFrame - time series data.  Index expected as datatimeindex\n",
    "    split_date - the date on which to split the time series\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple (len=2) \n",
    "    0. pandas.DataFrame - training dataset\n",
    "    1. pandas.DataFrame - test dataset\n",
    "    '''\n",
    "    train = data.loc[data.index < split_date]\n",
    "    test = data.loc[data.index >= split_date]\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = ts_train_test_split(clean, split_date=TEST_SPLIT_DATE)\n",
    "\n",
    "#exclude data after 2020 due to coronavirus.\n",
    "test, discard = ts_train_test_split(test, split_date=DISCARD_DATE)\n",
    "\n",
    "#split into train and val AFTER creating new years day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1828, 9)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(365, 9)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# autoregressive lags, seasonal indexes and New years day\n",
    "\n",
    "Generate lags + new binary categorical feature representing new years day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exclude interaction as point forecasts are less accurate.\n",
    "lagged, calendar_dummies, new_year = featurize_time_series(train[REGION], \n",
    "                                                    max_lags=7, \n",
    "                                                    include_interactions=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rename column and drop quarters and t from seasonal indexes\n",
    "new_year.columns = ['new_year']\n",
    "calendar_dummies = calendar_dummies[calendar_dummies.columns[:-4]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>actual</th>\n",
       "      <th>Trust_lag1</th>\n",
       "      <th>Trust_lag2</th>\n",
       "      <th>Trust_lag3</th>\n",
       "      <th>Trust_lag4</th>\n",
       "      <th>Trust_lag5</th>\n",
       "      <th>Trust_lag6</th>\n",
       "      <th>Trust_lag7</th>\n",
       "      <th>m_2</th>\n",
       "      <th>m_3</th>\n",
       "      <th>...</th>\n",
       "      <th>m_10</th>\n",
       "      <th>m_11</th>\n",
       "      <th>m_12</th>\n",
       "      <th>dow_1</th>\n",
       "      <th>dow_2</th>\n",
       "      <th>dow_3</th>\n",
       "      <th>dow_4</th>\n",
       "      <th>dow_5</th>\n",
       "      <th>dow_6</th>\n",
       "      <th>new_year</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>actual_dt</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2013-12-30</th>\n",
       "      <td>2042.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-12-31</th>\n",
       "      <td>1996.0</td>\n",
       "      <td>2042.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-01-01</th>\n",
       "      <td>2570.0</td>\n",
       "      <td>1996.0</td>\n",
       "      <td>2042.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-01-02</th>\n",
       "      <td>2013.0</td>\n",
       "      <td>2570.0</td>\n",
       "      <td>1996.0</td>\n",
       "      <td>2042.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-01-03</th>\n",
       "      <td>2056.0</td>\n",
       "      <td>2013.0</td>\n",
       "      <td>2570.0</td>\n",
       "      <td>1996.0</td>\n",
       "      <td>2042.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            actual  Trust_lag1  Trust_lag2  Trust_lag3  Trust_lag4  \\\n",
       "actual_dt                                                            \n",
       "2013-12-30  2042.0         NaN         NaN         NaN         NaN   \n",
       "2013-12-31  1996.0      2042.0         NaN         NaN         NaN   \n",
       "2014-01-01  2570.0      1996.0      2042.0         NaN         NaN   \n",
       "2014-01-02  2013.0      2570.0      1996.0      2042.0         NaN   \n",
       "2014-01-03  2056.0      2013.0      2570.0      1996.0      2042.0   \n",
       "\n",
       "            Trust_lag5  Trust_lag6  Trust_lag7  m_2  m_3  ...  m_10  m_11  \\\n",
       "actual_dt                                                 ...               \n",
       "2013-12-30         NaN         NaN         NaN    0    0  ...     0     0   \n",
       "2013-12-31         NaN         NaN         NaN    0    0  ...     0     0   \n",
       "2014-01-01         NaN         NaN         NaN    0    0  ...     0     0   \n",
       "2014-01-02         NaN         NaN         NaN    0    0  ...     0     0   \n",
       "2014-01-03         NaN         NaN         NaN    0    0  ...     0     0   \n",
       "\n",
       "            m_12  dow_1  dow_2  dow_3  dow_4  dow_5  dow_6  new_year  \n",
       "actual_dt                                                             \n",
       "2013-12-30     1      0      0      0      0      0      0         0  \n",
       "2013-12-31     1      1      0      0      0      0      0         0  \n",
       "2014-01-01     0      0      1      0      0      0      0         1  \n",
       "2014-01-02     0      0      0      1      0      0      0         0  \n",
       "2014-01-03     0      0      0      0      1      0      0         0  \n",
       "\n",
       "[5 rows x 26 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#combined to single dataframe\n",
    "processed = pd.concat([train[REGION], lagged, calendar_dummies, new_year], \n",
    "                      axis=1)\n",
    "processed.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train validation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train split into train and validation\n",
    "train, val = ts_train_test_split(processed, split_date=VAL_SPLIT_DATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1279, 26)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(549, 26)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation\n",
    "\n",
    "`time_series_cv` implements rolling forecast origin cross validation for time series.  \n",
    "It does not calculate forecast error, but instead returns the predictions, pred intervals and actuals in an array that can be passed to any forecast error function. (this is for efficiency and allows additional metrics to be calculated if needed).\n",
    "\n",
    "\n",
    "> Note: prediction uses an iterative method where ground truth inputs are gradually replaced with forecast values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_series_cv(train, val, horizons, lags, step=1, alpha=0.2, \n",
    "                   regularised=False, reg_weight=0.1):\n",
    "    '''\n",
    "    Time series cross validation across multiple horizons for a autoregressive\n",
    "    model.\n",
    "\n",
    "    Incrementally adds additional training data to the model and tests\n",
    "    across a provided list of forecast horizons. Note that function tests a\n",
    "    model only against complete validation sets.  E.g. if horizon = 15 and \n",
    "    len(val) = 12 then no testing is done.  In the case of multiple horizons\n",
    "    e.g. [7, 14, 28] then the function will use the maximum forecast horizon\n",
    "    to calculate the number of iterations i.e if len(val) = 365 and step = 1\n",
    "    then no. iterations = len(val) - max(horizon) = 365 - 28 = 337.\n",
    "    \n",
    "    Note that when the forecast horizon exceeds the lag number of the model\n",
    "    forecasts will be based of previous forecasts.  I.e. for a lag 7 model the first 7 \n",
    "    forecasts will include at least one ground truth observation, but from step 8 onwards all \n",
    "    forecasts iwll be based on the previous 7 predicted y values.\n",
    "    \n",
    "    Parameters:\n",
    "    --------\n",
    "    train - np.array - vector of training data\n",
    "\n",
    "    val - np.array - vector of validation data\n",
    "\n",
    "    horizon - list of ints, forecast horizon e.g. [7, 14, 28] days\n",
    "\n",
    "    step -- step taken in cross validation \n",
    "            e.g. 1 in next cross validation training data includes next point \n",
    "            from the validation set.\n",
    "            e.g. 7 in the next cross validation training data includes next 7 points\n",
    "            (default=1)\n",
    "            \n",
    "    Returns:\n",
    "    -------\n",
    "    np.array - vector of forecast errors from the CVs.\n",
    "    '''\n",
    "    cv_preds = []\n",
    "    cv_actuals = []\n",
    "    cv_intervals = []\n",
    "    MAX_LAG = lags\n",
    "    split = 1\n",
    "    \n",
    "    print('split => ', end=\"\")\n",
    "    for i in range(0, len(val) - max(horizons) + 1, step):\n",
    "        print(f'{split}, ', end=\"\")\n",
    "        split += 1\n",
    "        \n",
    "        #create and fit on a new training set (train + val)\n",
    "        train_cv = np.concatenate([train, val.iloc[:i, 0:]], axis=0) \n",
    "        X_train, y_train = train_cv[:,1:], train_cv[:,0]       \n",
    "        \n",
    "        # Fit and summarize OLS model\n",
    "        X_train = sm.add_constant(X_train, prepend=False, has_constant='add')\n",
    "        model = sm.OLS(endog=y_train, exog=X_train)\n",
    "        \n",
    "        if not regularised:\n",
    "            results = model.fit()\n",
    "        else:    \n",
    "            results = model.fit_regularized(method='elastic_net',\n",
    "                                            alpha=reg_weight, refit=True)\n",
    "\n",
    "        preds = []\n",
    "        intervals = []\n",
    "        current_batch = val.iloc[i, 1:].to_numpy()\n",
    "\n",
    "        #iteratively predict the next data point\n",
    "        #and update the lags.  Remember that the\n",
    "        #actual lags are replaced by FORECAST values \n",
    "        #as time goes on.\n",
    "        for j in range(max(horizons)): \n",
    "            \n",
    "            #one timestep ahead of historical points\n",
    "            batch_exog = current_batch\n",
    "\n",
    "            batch_exog = sm.add_constant(batch_exog.reshape(-1,1).T, \n",
    "                                         prepend=False,\n",
    "                                         has_constant='add')\n",
    "                       \n",
    "            predictions = results.get_prediction(batch_exog)\n",
    "            \n",
    "            df = predictions.summary_frame(alpha=alpha)\n",
    "            \n",
    "            y_pred = df['mean'].to_numpy()\n",
    "            preds.append(y_pred)\n",
    "            \n",
    "            #prediction intervals\n",
    "            y_intervals = df[['obs_ci_lower', 'obs_ci_upper']].to_numpy()\n",
    "            intervals.append(y_intervals)\n",
    "            \n",
    "            #remove tail lagged value add prediction to head \n",
    "            current_batch[:MAX_LAG] = np.append(y_pred, current_batch[:MAX_LAG-1])\n",
    "            \n",
    "            #get next set of seasonal dummies\n",
    "            current_batch[MAX_LAG:] = val.iloc[i+j+1, MAX_LAG+1:].to_numpy()\n",
    "        \n",
    "        cv_h_preds = []\n",
    "        cv_h_intervals = []\n",
    "        cv_test = []\n",
    "                \n",
    "        for h in horizons:\n",
    "            #store the h-step mean prediction\n",
    "            cv_h_preds.append(np.concatenate(preds[:h]))\n",
    "            \n",
    "            #store the h-step prediction intervals\n",
    "            cv_h_intervals.append(np.concatenate(intervals[:h]))\n",
    "            #store the h-step actual value\n",
    "            cv_test.append(val.iloc[i:i+h,0])                 \n",
    "                     \n",
    "        cv_preds.append(cv_h_preds)\n",
    "        cv_intervals.append(cv_h_intervals)\n",
    "        cv_actuals.append(cv_test)\n",
    "        \n",
    "    \n",
    "    print('done.\\n')\n",
    "    return cv_preds, cv_intervals, cv_actuals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross validation (**with** regularization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time horizons evaluated (in days) are 7, 14, 28, 56, 84, 365.\n",
    "\n",
    "Note: These are basic regression models and do not have an ARIMA error process.  This means that interpretation of the coefficients is problematic and should be done with caution.\n",
    "\n",
    "**All models contain the seasonal dummy variables, t and exceptional days.**\n",
    "\n",
    "Models evaluated:\n",
    "\n",
    "* lags 1\n",
    "* lags 1, 2\n",
    "* lags 1, 2, 3\n",
    "* lags 1, 2, 3, .... `MAX_LAGS`\n",
    "\n",
    "Notes: the cross validation function is returning the error measure over the different time horizons.  So the summary statistics refer to the distrubution of the test statistic.  Not the individual forecast errors of each point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_cv_error(cv_preds, cv_test, error_func):\n",
    "    n_splits = len(cv_preds)\n",
    "    cv_errors = []\n",
    "    \n",
    "    for split in range(n_splits):\n",
    "        pred_error = error_func(cv_test[split], cv_preds[split])\n",
    "        cv_errors.append(pred_error)\n",
    "        \n",
    "    return np.array(cv_errors)\n",
    "\n",
    "def forecast_errors_cv(cv_preds, cv_test, error_func):\n",
    "    cv_test = np.array(cv_test)\n",
    "    cv_preds = np.array(cv_preds)\n",
    "    n_horizons = len(cv_test)    \n",
    "    \n",
    "    horizon_errors = []\n",
    "    for h in range(n_horizons):\n",
    "        split_errors = split_cv_error(cv_preds[h], cv_test[h], error_func)\n",
    "        horizon_errors.append(split_errors)\n",
    "\n",
    "    return np.array(horizon_errors)\n",
    "\n",
    "def split_coverage(cv_test, cv_intervals):\n",
    "    n_splits = len(cv_test)\n",
    "    cv_errors = []\n",
    "        \n",
    "    for split in range(n_splits):\n",
    "        val = np.asarray(cv_test[split])\n",
    "        lower = cv_intervals[split].T[0]\n",
    "        upper = cv_intervals[split].T[1]\n",
    "        \n",
    "        coverage = len(np.where((val > lower) & (val < upper))[0])\n",
    "        coverage = coverage / len(val)\n",
    "        \n",
    "        cv_errors.append(coverage)\n",
    "        \n",
    "    return np.array(cv_errors)\n",
    "    \n",
    "    \n",
    "def prediction_int_coverage_cv(cv_test, cv_intervals):\n",
    "    cv_test = np.array(cv_test)\n",
    "    cv_intervals = np.array(cv_intervals)\n",
    "    n_horizons = len(cv_test)    \n",
    "    \n",
    "    horizon_coverage = []\n",
    "    for h in range(n_horizons):\n",
    "        split_coverages = split_coverage(cv_test[h], cv_intervals[h])\n",
    "        horizon_coverage.append(split_coverages)\n",
    "\n",
    "    return np.array(horizon_coverage)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_cv_error_scaled(cv_preds, cv_test, y_train):\n",
    "    n_splits = len(cv_preds)\n",
    "    cv_errors = []\n",
    "    \n",
    "    for split in range(n_splits):\n",
    "        pred_error = mean_absolute_scaled_error(cv_test[split], cv_preds[split], \n",
    "                                                y_train, period=7)\n",
    "        \n",
    "        cv_errors.append(pred_error)\n",
    "        \n",
    "    return np.array(cv_errors)\n",
    "\n",
    "def forecast_errors_cv_scaled(cv_preds, cv_test, y_train):\n",
    "    cv_test = np.array(cv_test)\n",
    "    cv_preds = np.array(cv_preds)\n",
    "    n_horizons = len(cv_test)    \n",
    "    \n",
    "    horizon_errors = []\n",
    "    for h in range(n_horizons):\n",
    "        split_errors = split_cv_error_scaled(cv_preds[h], cv_test[h], y_train)\n",
    "        horizon_errors.append(split_errors)\n",
    "        \n",
    "    return np.array(horizon_errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "split => 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, done.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "MAX_LAG = 7\n",
    "horizons = [7, 14, 21, 28, 35, 42, 49, 56, 63, 70, 77, 84, 365]\n",
    "STEP = 7\n",
    "PREFIX = 'Trust'\n",
    "\n",
    "y_lags = [PREFIX+'_lag' + str(i) for i in range(1, MAX_LAG+1)]\n",
    "\n",
    "select_cv = ['actual'] + y_lags + list(calendar_dummies.columns) + \\\n",
    "    list(new_year.columns)\n",
    "\n",
    "\n",
    "cv_preds, cv_intervals, cv_test  = time_series_cv(train=train[select_cv][MAX_LAG+1:], \n",
    "                                                  val=val[select_cv], \n",
    "                                                  horizons=horizons,\n",
    "                                                  lags=len(y_lags),\n",
    "                                                  step=STEP,\n",
    "                                                  alpha=0.2, \n",
    "                                                  regularised=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>7</th>\n",
       "      <th>14</th>\n",
       "      <th>21</th>\n",
       "      <th>28</th>\n",
       "      <th>35</th>\n",
       "      <th>42</th>\n",
       "      <th>49</th>\n",
       "      <th>56</th>\n",
       "      <th>63</th>\n",
       "      <th>70</th>\n",
       "      <th>77</th>\n",
       "      <th>84</th>\n",
       "      <th>365</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.264534</td>\n",
       "      <td>3.629701</td>\n",
       "      <td>3.854670</td>\n",
       "      <td>4.067859</td>\n",
       "      <td>4.238041</td>\n",
       "      <td>4.367489</td>\n",
       "      <td>4.473390</td>\n",
       "      <td>4.571771</td>\n",
       "      <td>4.646257</td>\n",
       "      <td>4.723101</td>\n",
       "      <td>4.816078</td>\n",
       "      <td>4.904241</td>\n",
       "      <td>4.968646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.200287</td>\n",
       "      <td>1.056591</td>\n",
       "      <td>0.888823</td>\n",
       "      <td>0.829506</td>\n",
       "      <td>0.907699</td>\n",
       "      <td>0.935649</td>\n",
       "      <td>0.935111</td>\n",
       "      <td>0.906036</td>\n",
       "      <td>0.897536</td>\n",
       "      <td>0.895387</td>\n",
       "      <td>0.897089</td>\n",
       "      <td>0.864706</td>\n",
       "      <td>0.245960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.531603</td>\n",
       "      <td>1.861050</td>\n",
       "      <td>2.449089</td>\n",
       "      <td>2.527198</td>\n",
       "      <td>2.565245</td>\n",
       "      <td>2.722428</td>\n",
       "      <td>2.717989</td>\n",
       "      <td>2.724723</td>\n",
       "      <td>2.752764</td>\n",
       "      <td>2.949292</td>\n",
       "      <td>3.186569</td>\n",
       "      <td>3.462700</td>\n",
       "      <td>4.488560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2.441225</td>\n",
       "      <td>2.863237</td>\n",
       "      <td>3.322752</td>\n",
       "      <td>3.302709</td>\n",
       "      <td>3.674638</td>\n",
       "      <td>3.796552</td>\n",
       "      <td>4.099345</td>\n",
       "      <td>4.055521</td>\n",
       "      <td>4.135640</td>\n",
       "      <td>4.239725</td>\n",
       "      <td>4.224898</td>\n",
       "      <td>4.201346</td>\n",
       "      <td>4.745840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3.096386</td>\n",
       "      <td>3.497830</td>\n",
       "      <td>3.743643</td>\n",
       "      <td>4.238427</td>\n",
       "      <td>4.426438</td>\n",
       "      <td>4.470558</td>\n",
       "      <td>4.721285</td>\n",
       "      <td>4.868604</td>\n",
       "      <td>4.809179</td>\n",
       "      <td>4.829757</td>\n",
       "      <td>4.800177</td>\n",
       "      <td>5.061050</td>\n",
       "      <td>5.021345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3.799949</td>\n",
       "      <td>4.175783</td>\n",
       "      <td>4.488454</td>\n",
       "      <td>4.562801</td>\n",
       "      <td>4.866238</td>\n",
       "      <td>5.097712</td>\n",
       "      <td>5.174862</td>\n",
       "      <td>5.213002</td>\n",
       "      <td>5.202153</td>\n",
       "      <td>5.299259</td>\n",
       "      <td>5.498609</td>\n",
       "      <td>5.553106</td>\n",
       "      <td>5.199302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>6.953349</td>\n",
       "      <td>6.040567</td>\n",
       "      <td>5.429808</td>\n",
       "      <td>5.390963</td>\n",
       "      <td>5.938833</td>\n",
       "      <td>5.783728</td>\n",
       "      <td>5.724250</td>\n",
       "      <td>5.761650</td>\n",
       "      <td>6.117779</td>\n",
       "      <td>6.045818</td>\n",
       "      <td>6.344108</td>\n",
       "      <td>6.561931</td>\n",
       "      <td>5.274809</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             7          14         21         28         35         42   \\\n",
       "count  27.000000  27.000000  27.000000  27.000000  27.000000  27.000000   \n",
       "mean    3.264534   3.629701   3.854670   4.067859   4.238041   4.367489   \n",
       "std     1.200287   1.056591   0.888823   0.829506   0.907699   0.935649   \n",
       "min     1.531603   1.861050   2.449089   2.527198   2.565245   2.722428   \n",
       "25%     2.441225   2.863237   3.322752   3.302709   3.674638   3.796552   \n",
       "50%     3.096386   3.497830   3.743643   4.238427   4.426438   4.470558   \n",
       "75%     3.799949   4.175783   4.488454   4.562801   4.866238   5.097712   \n",
       "max     6.953349   6.040567   5.429808   5.390963   5.938833   5.783728   \n",
       "\n",
       "             49         56         63         70         77         84   \\\n",
       "count  27.000000  27.000000  27.000000  27.000000  27.000000  27.000000   \n",
       "mean    4.473390   4.571771   4.646257   4.723101   4.816078   4.904241   \n",
       "std     0.935111   0.906036   0.897536   0.895387   0.897089   0.864706   \n",
       "min     2.717989   2.724723   2.752764   2.949292   3.186569   3.462700   \n",
       "25%     4.099345   4.055521   4.135640   4.239725   4.224898   4.201346   \n",
       "50%     4.721285   4.868604   4.809179   4.829757   4.800177   5.061050   \n",
       "75%     5.174862   5.213002   5.202153   5.299259   5.498609   5.553106   \n",
       "max     5.724250   5.761650   6.117779   6.045818   6.344108   6.561931   \n",
       "\n",
       "             365  \n",
       "count  27.000000  \n",
       "mean    4.968646  \n",
       "std     0.245960  \n",
       "min     4.488560  \n",
       "25%     4.745840  \n",
       "50%     5.021345  \n",
       "75%     5.199302  \n",
       "max     5.274809  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#CV point predictions smape\n",
    "cv_errors = forecast_errors_cv(cv_preds, cv_test, \n",
    "                               symmetric_mean_absolute_percentage_error)\n",
    "df = pd.DataFrame(cv_errors)\n",
    "df.columns = horizons\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../../results/model_selection/stage1/Trust-elastic-net_smape.csv\n"
     ]
    }
   ],
   "source": [
    "#output sMAPE results to file\n",
    "metric = 'smape'\n",
    "print(f'{TOP_LEVEL}/{STAGE}/{REGION}-{METHOD}_{metric}.csv')\n",
    "df.to_csv(f'{TOP_LEVEL}/{STAGE}/{REGION}-{METHOD}_{metric}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>7</th>\n",
       "      <th>14</th>\n",
       "      <th>21</th>\n",
       "      <th>28</th>\n",
       "      <th>35</th>\n",
       "      <th>42</th>\n",
       "      <th>49</th>\n",
       "      <th>56</th>\n",
       "      <th>63</th>\n",
       "      <th>70</th>\n",
       "      <th>77</th>\n",
       "      <th>84</th>\n",
       "      <th>365</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>84.672582</td>\n",
       "      <td>95.801026</td>\n",
       "      <td>103.261059</td>\n",
       "      <td>109.281396</td>\n",
       "      <td>113.651879</td>\n",
       "      <td>116.979000</td>\n",
       "      <td>119.595559</td>\n",
       "      <td>122.094250</td>\n",
       "      <td>123.815270</td>\n",
       "      <td>125.692436</td>\n",
       "      <td>127.835242</td>\n",
       "      <td>129.937203</td>\n",
       "      <td>132.904192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>29.287007</td>\n",
       "      <td>29.174879</td>\n",
       "      <td>26.462292</td>\n",
       "      <td>24.949116</td>\n",
       "      <td>25.438864</td>\n",
       "      <td>25.206511</td>\n",
       "      <td>24.459545</td>\n",
       "      <td>23.404578</td>\n",
       "      <td>23.107934</td>\n",
       "      <td>23.229848</td>\n",
       "      <td>23.382414</td>\n",
       "      <td>22.703598</td>\n",
       "      <td>5.093162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>46.346995</td>\n",
       "      <td>59.999095</td>\n",
       "      <td>58.831933</td>\n",
       "      <td>63.799066</td>\n",
       "      <td>64.341211</td>\n",
       "      <td>69.978269</td>\n",
       "      <td>76.028616</td>\n",
       "      <td>76.181564</td>\n",
       "      <td>75.337689</td>\n",
       "      <td>79.541636</td>\n",
       "      <td>86.850600</td>\n",
       "      <td>93.913877</td>\n",
       "      <td>120.788786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>64.016862</td>\n",
       "      <td>77.229424</td>\n",
       "      <td>88.204354</td>\n",
       "      <td>92.638312</td>\n",
       "      <td>98.664548</td>\n",
       "      <td>103.127133</td>\n",
       "      <td>110.397491</td>\n",
       "      <td>108.788294</td>\n",
       "      <td>110.007951</td>\n",
       "      <td>112.102011</td>\n",
       "      <td>111.928653</td>\n",
       "      <td>111.416966</td>\n",
       "      <td>128.493728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>83.950904</td>\n",
       "      <td>88.392662</td>\n",
       "      <td>98.568948</td>\n",
       "      <td>109.908012</td>\n",
       "      <td>117.676628</td>\n",
       "      <td>116.967411</td>\n",
       "      <td>124.596746</td>\n",
       "      <td>126.967198</td>\n",
       "      <td>125.493156</td>\n",
       "      <td>125.405315</td>\n",
       "      <td>127.281057</td>\n",
       "      <td>128.173934</td>\n",
       "      <td>134.452155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>100.560635</td>\n",
       "      <td>108.447312</td>\n",
       "      <td>113.001245</td>\n",
       "      <td>125.555263</td>\n",
       "      <td>132.305288</td>\n",
       "      <td>138.282168</td>\n",
       "      <td>140.912787</td>\n",
       "      <td>140.990855</td>\n",
       "      <td>142.101462</td>\n",
       "      <td>141.576774</td>\n",
       "      <td>147.500462</td>\n",
       "      <td>151.237079</td>\n",
       "      <td>137.435720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>185.824420</td>\n",
       "      <td>179.924873</td>\n",
       "      <td>163.606330</td>\n",
       "      <td>160.348159</td>\n",
       "      <td>152.167748</td>\n",
       "      <td>152.234559</td>\n",
       "      <td>147.892787</td>\n",
       "      <td>150.161993</td>\n",
       "      <td>160.014482</td>\n",
       "      <td>159.985463</td>\n",
       "      <td>165.347550</td>\n",
       "      <td>170.119965</td>\n",
       "      <td>138.524387</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              7           14          21          28          35          42   \\\n",
       "count   27.000000   27.000000   27.000000   27.000000   27.000000   27.000000   \n",
       "mean    84.672582   95.801026  103.261059  109.281396  113.651879  116.979000   \n",
       "std     29.287007   29.174879   26.462292   24.949116   25.438864   25.206511   \n",
       "min     46.346995   59.999095   58.831933   63.799066   64.341211   69.978269   \n",
       "25%     64.016862   77.229424   88.204354   92.638312   98.664548  103.127133   \n",
       "50%     83.950904   88.392662   98.568948  109.908012  117.676628  116.967411   \n",
       "75%    100.560635  108.447312  113.001245  125.555263  132.305288  138.282168   \n",
       "max    185.824420  179.924873  163.606330  160.348159  152.167748  152.234559   \n",
       "\n",
       "              49          56          63          70          77          84   \\\n",
       "count   27.000000   27.000000   27.000000   27.000000   27.000000   27.000000   \n",
       "mean   119.595559  122.094250  123.815270  125.692436  127.835242  129.937203   \n",
       "std     24.459545   23.404578   23.107934   23.229848   23.382414   22.703598   \n",
       "min     76.028616   76.181564   75.337689   79.541636   86.850600   93.913877   \n",
       "25%    110.397491  108.788294  110.007951  112.102011  111.928653  111.416966   \n",
       "50%    124.596746  126.967198  125.493156  125.405315  127.281057  128.173934   \n",
       "75%    140.912787  140.990855  142.101462  141.576774  147.500462  151.237079   \n",
       "max    147.892787  150.161993  160.014482  159.985463  165.347550  170.119965   \n",
       "\n",
       "              365  \n",
       "count   27.000000  \n",
       "mean   132.904192  \n",
       "std      5.093162  \n",
       "min    120.788786  \n",
       "25%    128.493728  \n",
       "50%    134.452155  \n",
       "75%    137.435720  \n",
       "max    138.524387  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#CV point predictions rmse - no interactions\n",
    "cv_errors = forecast_errors_cv(cv_preds, cv_test, root_mean_squared_error)\n",
    "df = pd.DataFrame(cv_errors)\n",
    "df.columns = horizons\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../../results/model_selection/stage1/Trust-elastic-net_rmse.csv\n"
     ]
    }
   ],
   "source": [
    "#output rmse\n",
    "metric = 'rmse'\n",
    "print(f'{TOP_LEVEL}/{STAGE}/{REGION}-{METHOD}_{metric}.csv')\n",
    "df.to_csv(f'{TOP_LEVEL}/{STAGE}/{REGION}-{METHOD}_{metric}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>7</th>\n",
       "      <th>14</th>\n",
       "      <th>21</th>\n",
       "      <th>28</th>\n",
       "      <th>35</th>\n",
       "      <th>42</th>\n",
       "      <th>49</th>\n",
       "      <th>56</th>\n",
       "      <th>63</th>\n",
       "      <th>70</th>\n",
       "      <th>77</th>\n",
       "      <th>84</th>\n",
       "      <th>365</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.875557</td>\n",
       "      <td>0.974249</td>\n",
       "      <td>1.034381</td>\n",
       "      <td>1.091215</td>\n",
       "      <td>1.136499</td>\n",
       "      <td>1.171237</td>\n",
       "      <td>1.199963</td>\n",
       "      <td>1.226954</td>\n",
       "      <td>1.247489</td>\n",
       "      <td>1.268792</td>\n",
       "      <td>1.294367</td>\n",
       "      <td>1.318510</td>\n",
       "      <td>1.325109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.344332</td>\n",
       "      <td>0.309016</td>\n",
       "      <td>0.263814</td>\n",
       "      <td>0.248322</td>\n",
       "      <td>0.266788</td>\n",
       "      <td>0.272738</td>\n",
       "      <td>0.272362</td>\n",
       "      <td>0.265703</td>\n",
       "      <td>0.264200</td>\n",
       "      <td>0.264409</td>\n",
       "      <td>0.264985</td>\n",
       "      <td>0.256285</td>\n",
       "      <td>0.065677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.391448</td>\n",
       "      <td>0.471597</td>\n",
       "      <td>0.614331</td>\n",
       "      <td>0.636797</td>\n",
       "      <td>0.649322</td>\n",
       "      <td>0.697388</td>\n",
       "      <td>0.694161</td>\n",
       "      <td>0.695716</td>\n",
       "      <td>0.703499</td>\n",
       "      <td>0.756920</td>\n",
       "      <td>0.820900</td>\n",
       "      <td>0.894166</td>\n",
       "      <td>1.191687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.632881</td>\n",
       "      <td>0.763286</td>\n",
       "      <td>0.876999</td>\n",
       "      <td>0.854999</td>\n",
       "      <td>0.952392</td>\n",
       "      <td>0.988170</td>\n",
       "      <td>1.068129</td>\n",
       "      <td>1.057499</td>\n",
       "      <td>1.081228</td>\n",
       "      <td>1.110839</td>\n",
       "      <td>1.107718</td>\n",
       "      <td>1.098212</td>\n",
       "      <td>1.267105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.838845</td>\n",
       "      <td>0.938353</td>\n",
       "      <td>1.007739</td>\n",
       "      <td>1.121042</td>\n",
       "      <td>1.194395</td>\n",
       "      <td>1.209091</td>\n",
       "      <td>1.251750</td>\n",
       "      <td>1.330002</td>\n",
       "      <td>1.313653</td>\n",
       "      <td>1.306614</td>\n",
       "      <td>1.328366</td>\n",
       "      <td>1.355144</td>\n",
       "      <td>1.340336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.034533</td>\n",
       "      <td>1.121813</td>\n",
       "      <td>1.198767</td>\n",
       "      <td>1.282232</td>\n",
       "      <td>1.328619</td>\n",
       "      <td>1.392184</td>\n",
       "      <td>1.422596</td>\n",
       "      <td>1.417552</td>\n",
       "      <td>1.445886</td>\n",
       "      <td>1.451745</td>\n",
       "      <td>1.508858</td>\n",
       "      <td>1.526987</td>\n",
       "      <td>1.385565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2.026483</td>\n",
       "      <td>1.793052</td>\n",
       "      <td>1.507248</td>\n",
       "      <td>1.519510</td>\n",
       "      <td>1.573388</td>\n",
       "      <td>1.538543</td>\n",
       "      <td>1.532167</td>\n",
       "      <td>1.555245</td>\n",
       "      <td>1.669562</td>\n",
       "      <td>1.668598</td>\n",
       "      <td>1.714540</td>\n",
       "      <td>1.782709</td>\n",
       "      <td>1.406119</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             7          14         21         28         35         42   \\\n",
       "count  27.000000  27.000000  27.000000  27.000000  27.000000  27.000000   \n",
       "mean    0.875557   0.974249   1.034381   1.091215   1.136499   1.171237   \n",
       "std     0.344332   0.309016   0.263814   0.248322   0.266788   0.272738   \n",
       "min     0.391448   0.471597   0.614331   0.636797   0.649322   0.697388   \n",
       "25%     0.632881   0.763286   0.876999   0.854999   0.952392   0.988170   \n",
       "50%     0.838845   0.938353   1.007739   1.121042   1.194395   1.209091   \n",
       "75%     1.034533   1.121813   1.198767   1.282232   1.328619   1.392184   \n",
       "max     2.026483   1.793052   1.507248   1.519510   1.573388   1.538543   \n",
       "\n",
       "             49         56         63         70         77         84   \\\n",
       "count  27.000000  27.000000  27.000000  27.000000  27.000000  27.000000   \n",
       "mean    1.199963   1.226954   1.247489   1.268792   1.294367   1.318510   \n",
       "std     0.272362   0.265703   0.264200   0.264409   0.264985   0.256285   \n",
       "min     0.694161   0.695716   0.703499   0.756920   0.820900   0.894166   \n",
       "25%     1.068129   1.057499   1.081228   1.110839   1.107718   1.098212   \n",
       "50%     1.251750   1.330002   1.313653   1.306614   1.328366   1.355144   \n",
       "75%     1.422596   1.417552   1.445886   1.451745   1.508858   1.526987   \n",
       "max     1.532167   1.555245   1.669562   1.668598   1.714540   1.782709   \n",
       "\n",
       "             365  \n",
       "count  27.000000  \n",
       "mean    1.325109  \n",
       "std     0.065677  \n",
       "min     1.191687  \n",
       "25%     1.267105  \n",
       "50%     1.340336  \n",
       "75%     1.385565  \n",
       "max     1.406119  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#mase\n",
    "cv_errors = forecast_errors_cv_scaled(cv_preds, cv_test, train['actual'])\n",
    "df = pd.DataFrame(cv_errors)\n",
    "df.columns = horizons\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../../results/model_selection/stage1/Trust-elastic-net_mase.csv\n"
     ]
    }
   ],
   "source": [
    "metric = 'mase'\n",
    "print(f'{TOP_LEVEL}/{STAGE}/{REGION}-{METHOD}_{metric}.csv')\n",
    "df.to_csv(f'{TOP_LEVEL}/{STAGE}/{REGION}-{METHOD}_{metric}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>7</th>\n",
       "      <th>14</th>\n",
       "      <th>21</th>\n",
       "      <th>28</th>\n",
       "      <th>35</th>\n",
       "      <th>42</th>\n",
       "      <th>49</th>\n",
       "      <th>56</th>\n",
       "      <th>63</th>\n",
       "      <th>70</th>\n",
       "      <th>77</th>\n",
       "      <th>84</th>\n",
       "      <th>365</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.682540</td>\n",
       "      <td>0.608466</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.542328</td>\n",
       "      <td>0.522751</td>\n",
       "      <td>0.504409</td>\n",
       "      <td>0.488284</td>\n",
       "      <td>0.472222</td>\n",
       "      <td>0.457966</td>\n",
       "      <td>0.447090</td>\n",
       "      <td>0.433862</td>\n",
       "      <td>0.422399</td>\n",
       "      <td>0.447184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.228755</td>\n",
       "      <td>0.189366</td>\n",
       "      <td>0.158486</td>\n",
       "      <td>0.143555</td>\n",
       "      <td>0.152800</td>\n",
       "      <td>0.156274</td>\n",
       "      <td>0.154278</td>\n",
       "      <td>0.147614</td>\n",
       "      <td>0.142837</td>\n",
       "      <td>0.137699</td>\n",
       "      <td>0.131718</td>\n",
       "      <td>0.124393</td>\n",
       "      <td>0.042405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.214286</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.321429</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.261905</td>\n",
       "      <td>0.265306</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.253968</td>\n",
       "      <td>0.271429</td>\n",
       "      <td>0.246753</td>\n",
       "      <td>0.226190</td>\n",
       "      <td>0.394521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.476190</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.380952</td>\n",
       "      <td>0.357143</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.380952</td>\n",
       "      <td>0.357143</td>\n",
       "      <td>0.331169</td>\n",
       "      <td>0.339286</td>\n",
       "      <td>0.408219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.642857</td>\n",
       "      <td>0.523810</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.485714</td>\n",
       "      <td>0.452381</td>\n",
       "      <td>0.448980</td>\n",
       "      <td>0.446429</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.402597</td>\n",
       "      <td>0.392857</td>\n",
       "      <td>0.438356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.690476</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.614286</td>\n",
       "      <td>0.619048</td>\n",
       "      <td>0.591837</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>0.539683</td>\n",
       "      <td>0.507143</td>\n",
       "      <td>0.506494</td>\n",
       "      <td>0.523810</td>\n",
       "      <td>0.490411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.821429</td>\n",
       "      <td>0.828571</td>\n",
       "      <td>0.809524</td>\n",
       "      <td>0.755102</td>\n",
       "      <td>0.767857</td>\n",
       "      <td>0.761905</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.688312</td>\n",
       "      <td>0.654762</td>\n",
       "      <td>0.523288</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             7          14         21         28         35         42   \\\n",
       "count  27.000000  27.000000  27.000000  27.000000  27.000000  27.000000   \n",
       "mean    0.682540   0.608466   0.571429   0.542328   0.522751   0.504409   \n",
       "std     0.228755   0.189366   0.158486   0.143555   0.152800   0.156274   \n",
       "min     0.142857   0.214286   0.285714   0.321429   0.285714   0.261905   \n",
       "25%     0.571429   0.500000   0.476190   0.428571   0.428571   0.380952   \n",
       "50%     0.714286   0.642857   0.523810   0.500000   0.485714   0.452381   \n",
       "75%     0.857143   0.750000   0.690476   0.625000   0.614286   0.619048   \n",
       "max     1.000000   0.928571   0.857143   0.821429   0.828571   0.809524   \n",
       "\n",
       "             49         56         63         70         77         84   \\\n",
       "count  27.000000  27.000000  27.000000  27.000000  27.000000  27.000000   \n",
       "mean    0.488284   0.472222   0.457966   0.447090   0.433862   0.422399   \n",
       "std     0.154278   0.147614   0.142837   0.137699   0.131718   0.124393   \n",
       "min     0.265306   0.285714   0.253968   0.271429   0.246753   0.226190   \n",
       "25%     0.357143   0.375000   0.380952   0.357143   0.331169   0.339286   \n",
       "50%     0.448980   0.446429   0.428571   0.428571   0.402597   0.392857   \n",
       "75%     0.591837   0.562500   0.539683   0.507143   0.506494   0.523810   \n",
       "max     0.755102   0.767857   0.761905   0.714286   0.688312   0.654762   \n",
       "\n",
       "             365  \n",
       "count  27.000000  \n",
       "mean    0.447184  \n",
       "std     0.042405  \n",
       "min     0.394521  \n",
       "25%     0.408219  \n",
       "50%     0.438356  \n",
       "75%     0.490411  \n",
       "max     0.523288  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#80% PIs\n",
    "cv_coverage = prediction_int_coverage_cv(cv_test, cv_intervals)\n",
    "df = pd.DataFrame(cv_coverage)\n",
    "df.columns = horizons\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../../results/model_selection/stage1/Trust-elastic-net_coverage_80.csv\n"
     ]
    }
   ],
   "source": [
    "#write 80% coverage to file\n",
    "metric = 'coverage_80'\n",
    "print(f'{TOP_LEVEL}/{STAGE}/{REGION}-{METHOD}_{metric}.csv')\n",
    "df.to_csv(f'{TOP_LEVEL}/{STAGE}/{REGION}-{METHOD}_{metric}.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Repeat for 95% PIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "split => 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, done.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#95% PIs\n",
    "MAX_LAG = 7\n",
    "horizons = [7, 14, 21, 28, 35, 42, 49, 56, 63, 70, 77, 84, 365]\n",
    "STEP = 7\n",
    "PREFIX = 'Trust'\n",
    "\n",
    "y_lags = [PREFIX+'_lag' + str(i) for i in range(1, MAX_LAG+1)]\n",
    "\n",
    "select_cv = ['actual'] + y_lags + list(calendar_dummies.columns) + \\\n",
    "    list(new_year.columns)\n",
    "\n",
    "\n",
    "cv_preds, cv_intervals, cv_test  = time_series_cv(train=train[select_cv][MAX_LAG+1:], \n",
    "                                                  val=val[select_cv], \n",
    "                                                  horizons=horizons,\n",
    "                                                  lags=len(y_lags),\n",
    "                                                  step=STEP,\n",
    "                                                  alpha=0.05,\n",
    "                                                  regularised=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>7</th>\n",
       "      <th>14</th>\n",
       "      <th>21</th>\n",
       "      <th>28</th>\n",
       "      <th>35</th>\n",
       "      <th>42</th>\n",
       "      <th>49</th>\n",
       "      <th>56</th>\n",
       "      <th>63</th>\n",
       "      <th>70</th>\n",
       "      <th>77</th>\n",
       "      <th>84</th>\n",
       "      <th>365</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.894180</td>\n",
       "      <td>0.846561</td>\n",
       "      <td>0.818342</td>\n",
       "      <td>0.792328</td>\n",
       "      <td>0.769312</td>\n",
       "      <td>0.750441</td>\n",
       "      <td>0.736206</td>\n",
       "      <td>0.721561</td>\n",
       "      <td>0.713110</td>\n",
       "      <td>0.703704</td>\n",
       "      <td>0.693122</td>\n",
       "      <td>0.682540</td>\n",
       "      <td>0.690309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.161394</td>\n",
       "      <td>0.171227</td>\n",
       "      <td>0.148859</td>\n",
       "      <td>0.133277</td>\n",
       "      <td>0.136318</td>\n",
       "      <td>0.136195</td>\n",
       "      <td>0.135008</td>\n",
       "      <td>0.130309</td>\n",
       "      <td>0.129173</td>\n",
       "      <td>0.129034</td>\n",
       "      <td>0.130376</td>\n",
       "      <td>0.123718</td>\n",
       "      <td>0.032045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.523810</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.485714</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.510204</td>\n",
       "      <td>0.482143</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.471429</td>\n",
       "      <td>0.441558</td>\n",
       "      <td>0.404762</td>\n",
       "      <td>0.652055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.678571</td>\n",
       "      <td>0.657143</td>\n",
       "      <td>0.642857</td>\n",
       "      <td>0.622449</td>\n",
       "      <td>0.642857</td>\n",
       "      <td>0.650794</td>\n",
       "      <td>0.635714</td>\n",
       "      <td>0.590909</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.665753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>0.809524</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>0.771429</td>\n",
       "      <td>0.738095</td>\n",
       "      <td>0.734694</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.728571</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.726190</td>\n",
       "      <td>0.679452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.952381</td>\n",
       "      <td>0.910714</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.869048</td>\n",
       "      <td>0.846939</td>\n",
       "      <td>0.803571</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>0.771429</td>\n",
       "      <td>0.779221</td>\n",
       "      <td>0.761905</td>\n",
       "      <td>0.723288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.976190</td>\n",
       "      <td>0.959184</td>\n",
       "      <td>0.946429</td>\n",
       "      <td>0.952381</td>\n",
       "      <td>0.942857</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.747945</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             7          14         21         28         35         42   \\\n",
       "count  27.000000  27.000000  27.000000  27.000000  27.000000  27.000000   \n",
       "mean    0.894180   0.846561   0.818342   0.792328   0.769312   0.750441   \n",
       "std     0.161394   0.171227   0.148859   0.133277   0.136318   0.136195   \n",
       "min     0.428571   0.428571   0.523810   0.571429   0.485714   0.500000   \n",
       "25%     0.857143   0.750000   0.714286   0.678571   0.657143   0.642857   \n",
       "50%     1.000000   0.928571   0.809524   0.785714   0.771429   0.738095   \n",
       "75%     1.000000   1.000000   0.952381   0.910714   0.900000   0.869048   \n",
       "max     1.000000   1.000000   1.000000   1.000000   1.000000   0.976190   \n",
       "\n",
       "             49         56         63         70         77         84   \\\n",
       "count  27.000000  27.000000  27.000000  27.000000  27.000000  27.000000   \n",
       "mean    0.736206   0.721561   0.713110   0.703704   0.693122   0.682540   \n",
       "std     0.135008   0.130309   0.129173   0.129034   0.130376   0.123718   \n",
       "min     0.510204   0.482143   0.428571   0.471429   0.441558   0.404762   \n",
       "25%     0.622449   0.642857   0.650794   0.635714   0.590909   0.571429   \n",
       "50%     0.734694   0.714286   0.714286   0.728571   0.714286   0.726190   \n",
       "75%     0.846939   0.803571   0.785714   0.771429   0.779221   0.761905   \n",
       "max     0.959184   0.946429   0.952381   0.942857   0.909091   0.857143   \n",
       "\n",
       "             365  \n",
       "count  27.000000  \n",
       "mean    0.690309  \n",
       "std     0.032045  \n",
       "min     0.652055  \n",
       "25%     0.665753  \n",
       "50%     0.679452  \n",
       "75%     0.723288  \n",
       "max     0.747945  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#95% PIs\n",
    "cv_coverage = prediction_int_coverage_cv(cv_test, cv_intervals)\n",
    "df = pd.DataFrame(cv_coverage)\n",
    "df.columns = horizons\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../../results/model_selection/stage1/Trust-elastic-net_coverage_95.csv\n"
     ]
    }
   ],
   "source": [
    "#write 95% coverage to file\n",
    "metric = 'coverage_95'\n",
    "print(f'{TOP_LEVEL}/{STAGE}/{REGION}-{METHOD}_{metric}.csv')\n",
    "df.to_csv(f'{TOP_LEVEL}/{STAGE}/{REGION}-{METHOD}_{metric}.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
