{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble: Comb\n",
    "\n",
    "This model is a combination of SES, Holts Linear Method and Holt's damped trend.  Comb was included to allow for a \"standard\" benchmark combination forecast for the more \"complex\" variants such as ARIMA+Prophet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "\n",
    "#forecast error metrics\n",
    "from forecast_tools.metrics import (mean_absolute_scaled_error, \n",
    "                                    root_mean_squared_error,\n",
    "                                    symmetric_mean_absolute_percentage_error)\n",
    "\n",
    "import statsmodels as sm\n",
    "from statsmodels.tsa.statespace.exponential_smoothing import ExponentialSmoothing\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.11.0\n"
     ]
    }
   ],
   "source": [
    "print(sm.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ensemble learning\n",
    "from amb_forecast.ensemble import (Ensemble, UnweightedVote)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Input\n",
    "\n",
    "The constants `TOP_LEVEL`, `STAGE`, `REGION`,`TRUST` and `METHOD` are used to control data selection and the directory for outputting results.  \n",
    "\n",
    "> Output file is `f'{TOP_LEVEL}/{STAGE}/{REGION}-{METHOD}_{metric}.csv'.csv`.  where metric will be smape, rmse, mase, coverage_80 and coverage_95. Note: `REGION`: is also used to select the correct data from the input dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOP_LEVEL = '../../../results/model_selection'\n",
    "STAGE = 'stage1'\n",
    "REGION = 'Trust'\n",
    "METHOD = 'comb'\n",
    "\n",
    "FILE_NAME = 'Daily_Responses_5_Years_2019_full.csv'\n",
    "\n",
    "#split training and test data.\n",
    "TEST_SPLIT_DATE = '2019-01-01'\n",
    "\n",
    "#second subdivide: train and val\n",
    "VAL_SPLIT_DATE = '2017-07-01'\n",
    "\n",
    "#discard data after 2020 due to coronavirus\n",
    "#this is the subject of a seperate study.\n",
    "DISCARD_DATE = '2020-01-01'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in path\n",
    "path = f'../../../data/{FILE_NAME}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process_daily_data(path, index_col, by_col, \n",
    "                           values, dayfirst=False):\n",
    "    '''\n",
    "    Daily data is stored in long format.  Read in \n",
    "    and pivot to wide format so that there is a single \n",
    "    colmumn for each regions time series.\n",
    "    '''\n",
    "    df = pd.read_csv(path, index_col=index_col, parse_dates=True, \n",
    "                     dayfirst=dayfirst)\n",
    "    df.columns = map(str.lower, df.columns)\n",
    "    df.index.rename(str(df.index.name).lower(), inplace=True)\n",
    "    \n",
    "    clean_table = pd.pivot_table(df, values=values.lower(), \n",
    "                                 index=[index_col.lower()],\n",
    "                                 columns=[by_col.lower()], aggfunc=np.sum)\n",
    "    \n",
    "    clean_table.index.freq = 'D'\n",
    "    \n",
    "    return clean_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>ora</th>\n",
       "      <th>BNSSG</th>\n",
       "      <th>Cornwall</th>\n",
       "      <th>Devon</th>\n",
       "      <th>Dorset</th>\n",
       "      <th>Gloucestershire</th>\n",
       "      <th>OOA</th>\n",
       "      <th>Somerset</th>\n",
       "      <th>Trust</th>\n",
       "      <th>Wiltshire</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>actual_dt</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2013-12-30</th>\n",
       "      <td>415.0</td>\n",
       "      <td>220.0</td>\n",
       "      <td>502.0</td>\n",
       "      <td>336.0</td>\n",
       "      <td>129.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>183.0</td>\n",
       "      <td>2042.0</td>\n",
       "      <td>255.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-12-31</th>\n",
       "      <td>420.0</td>\n",
       "      <td>236.0</td>\n",
       "      <td>468.0</td>\n",
       "      <td>302.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>180.0</td>\n",
       "      <td>1996.0</td>\n",
       "      <td>260.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-01-01</th>\n",
       "      <td>549.0</td>\n",
       "      <td>341.0</td>\n",
       "      <td>566.0</td>\n",
       "      <td>392.0</td>\n",
       "      <td>157.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>213.0</td>\n",
       "      <td>2570.0</td>\n",
       "      <td>351.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-01-02</th>\n",
       "      <td>450.0</td>\n",
       "      <td>218.0</td>\n",
       "      <td>499.0</td>\n",
       "      <td>301.0</td>\n",
       "      <td>115.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>167.0</td>\n",
       "      <td>2013.0</td>\n",
       "      <td>258.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-01-03</th>\n",
       "      <td>419.0</td>\n",
       "      <td>229.0</td>\n",
       "      <td>503.0</td>\n",
       "      <td>304.0</td>\n",
       "      <td>135.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>195.0</td>\n",
       "      <td>2056.0</td>\n",
       "      <td>269.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "ora         BNSSG  Cornwall  Devon  Dorset  Gloucestershire  OOA  Somerset  \\\n",
       "actual_dt                                                                    \n",
       "2013-12-30  415.0     220.0  502.0   336.0            129.0  NaN     183.0   \n",
       "2013-12-31  420.0     236.0  468.0   302.0            128.0  NaN     180.0   \n",
       "2014-01-01  549.0     341.0  566.0   392.0            157.0  NaN     213.0   \n",
       "2014-01-02  450.0     218.0  499.0   301.0            115.0  NaN     167.0   \n",
       "2014-01-03  419.0     229.0  503.0   304.0            135.0  NaN     195.0   \n",
       "\n",
       "ora          Trust  Wiltshire  \n",
       "actual_dt                      \n",
       "2013-12-30  2042.0      255.0  \n",
       "2013-12-31  1996.0      260.0  \n",
       "2014-01-01  2570.0      351.0  \n",
       "2014-01-02  2013.0      258.0  \n",
       "2014-01-03  2056.0      269.0  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean = pre_process_daily_data(path, 'Actual_dt', 'ORA', 'Actual_Value', \n",
    "                               dayfirst=False)\n",
    "clean.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ts_train_test_split(data, split_date):\n",
    "    '''\n",
    "    Split time series into training and test data\n",
    "    \n",
    "    Parameters:\n",
    "    -------\n",
    "    data - pd.DataFrame - time series data.  Index expected as datatimeindex\n",
    "    split_date - the date on which to split the time series\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple (len=2) \n",
    "    0. pandas.DataFrame - training dataset\n",
    "    1. pandas.DataFrame - test dataset\n",
    "    '''\n",
    "    train = data.loc[data.index < split_date]\n",
    "    test = data.loc[data.index >= split_date]\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = ts_train_test_split(clean, split_date=TEST_SPLIT_DATE)\n",
    "\n",
    "#exclude data after 2020 due to coronavirus.\n",
    "test, discard = ts_train_test_split(test, split_date=DISCARD_DATE)\n",
    "\n",
    "#train split into train and validation\n",
    "train, val = ts_train_test_split(train, split_date=VAL_SPLIT_DATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1279, 9)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(549, 9)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Models in the Ensemble\n",
    "1. SES\n",
    "2. Holt's Linear Trend Method\n",
    "3. Holt's Damped Trend\n",
    "\n",
    "The class below is a 'wrapper' class that provides the same interfacew for all methods and works in the time series cross valiation code.  The prediction is simply the average of all methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExponentialSmoothingWrapper:\n",
    "    '''\n",
    "    Facade for statsmodels exponential smoothing models.  This wrapper\n",
    "    provides a common interface for all models and allow interop with\n",
    "    the custom time series cross validation code.\n",
    "    '''\n",
    "    def __init__(self, trend=False, damped_trend=False, seasonal=None):\n",
    "        self._trend = trend\n",
    "        self._seasonal= seasonal\n",
    "        self._damped_trend = damped_trend\n",
    "\n",
    "    def _get_resids(self):\n",
    "        return self._fitted.resid\n",
    "\n",
    "    def _get_preds(self):\n",
    "        return self._fitted.fittedvalues\n",
    "\n",
    "    def fit(self, train):\n",
    "        '''\n",
    "        Fit the model\n",
    "        \n",
    "        Parameters:\n",
    "        train: array-like\n",
    "            time series to fit.\n",
    "        '''\n",
    "        self._model = ExponentialSmoothing(endog=train,\n",
    "                                          trend=self._trend, \n",
    "                                          damped_trend=self._damped_trend,\n",
    "                                          seasonal=self._seasonal)\n",
    "        self._fitted = self._model.fit()\n",
    "        self._t = len(train)\n",
    "    \n",
    "    def predict(self, horizon, return_conf_int=False, alpha=0.2):\n",
    "        '''\n",
    "        Forecast the time series from the final point in the fitted series.\n",
    "        \n",
    "        Parameters:\n",
    "        ----------\n",
    "        \n",
    "        horizon: int\n",
    "            steps ahead to forecast \n",
    "            \n",
    "        return_conf_int: bool, optional (default=False)\n",
    "            Return prediction interval?  \n",
    "            \n",
    "        alpha: float\n",
    "            Used if return_conf_int=True. 100(1-alpha) interval.\n",
    "        '''\n",
    "        \n",
    "        forecast = self._fitted.get_forecast(horizon)\n",
    "        \n",
    "        mean_forecast = forecast.summary_frame()['mean'].to_numpy()\n",
    "        \n",
    "        if return_conf_int:\n",
    "            df = forecast.summary_frame(alpha=alpha)\n",
    "            pi = df[['mean_ci_lower', 'mean_ci_upper']].to_numpy()\n",
    "            return mean_forecast, pi        \n",
    "        else:\n",
    "            return mean_forecast\n",
    "\n",
    "    fittedvalues = property(_get_preds)\n",
    "    resid = property(_get_resids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example fitting and prediction with comb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#StateSpaceExponentialSmoothing is a class I have created to help with CV\n",
    "#It is an adaptor class for the standard Exp Smoothing methods\n",
    "model_1 = ExponentialSmoothingWrapper(trend=False, damped_trend=False)\n",
    "model_2 = ExponentialSmoothingWrapper(trend=True)\n",
    "model_3 = ExponentialSmoothingWrapper(trend=True, damped_trend=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimators = {'ses': model_1, 'trend': model_2, 'damped_trend': model_3}\n",
    "ens = Ensemble(estimators, UnweightedVote())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "ens.fit(train[REGION])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "H = 5\n",
    "ens_preds = ens.predict(horizon=H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "ens_preds, pi = ens.predict(horizon=H, return_conf_int=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2150.02589006, 2150.05203479, 2150.07824001, 2150.10449364,\n",
       "       2150.13078598])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ens_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2000.41163537, 2299.64014475],\n",
       "       [1964.21206722, 2335.89200236],\n",
       "       [1935.68854566, 2364.46793436],\n",
       "       [1911.3518445 , 2388.85714278],\n",
       "       [1889.75511668, 2410.50645528]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time Series Cross Validation\n",
    "\n",
    "`time_series_cv` implements rolling forecast origin cross validation for time series.  \n",
    "It does not calculate forecast error, but instead returns the predictions, pred intervals and actuals in an array that can be passed to any forecast error function. (this is for efficiency and allows additional metrics to be calculated if needed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_series_cv(model, train, val, horizons, alpha=0.2, step=1):\n",
    "    '''\n",
    "    Time series cross validation across multiple horizons for a single model.\n",
    "\n",
    "    Incrementally adds additional training data to the model and tests\n",
    "    across a provided list of forecast horizons. Note that function tests a\n",
    "    model only against complete validation sets.  E.g. if horizon = 15 and \n",
    "    len(val) = 12 then no testing is done.  In the case of multiple horizons\n",
    "    e.g. [7, 14, 28] then the function will use the maximum forecast horizon\n",
    "    to calculate the number of iterations i.e if len(val) = 365 and step = 1\n",
    "    then no. iterations = len(val) - max(horizon) = 365 - 28 = 337.\n",
    "    \n",
    "    Parameters:\n",
    "    --------\n",
    "    model - forecasting model\n",
    "\n",
    "    error_func - function to measure forecast error\n",
    "\n",
    "    train - np.array - vector of training data\n",
    "\n",
    "    val - np.array - vector of validation data\n",
    "\n",
    "    horizon - list of ints, forecast horizon e.g. [7, 14, 28] days\n",
    "\n",
    "    step -- step taken in cross validation \n",
    "            e.g. 1 in next cross validation training data includes next point \n",
    "            from the validation set.\n",
    "            e.g. 7 in the next cross validation training data includes next 7 points\n",
    "            (default=1)\n",
    "            \n",
    "    Returns:\n",
    "    -------\n",
    "    np.array - vector of forecast errors from the CVs.\n",
    "    '''\n",
    "    cv_preds = [] #mean forecast\n",
    "    cv_actuals = [] # actuals \n",
    "    cv_pis = [] #prediction intervals\n",
    "    split = 0\n",
    "\n",
    "    print('split => ', end=\"\")\n",
    "    for i in range(0, len(val) - max(horizons) + 1, step):\n",
    "        split += 1\n",
    "        print(f'{split}, ', end=\"\")\n",
    "                \n",
    "        train_cv = np.concatenate([train, val[:i]], axis=0)\n",
    "        model.fit(train_cv)\n",
    "        \n",
    "        #predict the maximum horizon \n",
    "        preds, pis = model.predict(horizon=len(val[i:i+max(horizons)]), \n",
    "                                   return_conf_int=True,\n",
    "                                   alpha=alpha)\n",
    "        \n",
    "        cv_h_preds = []\n",
    "        cv_test = []\n",
    "        cv_h_pis = []\n",
    "        \n",
    "        for h in horizons:\n",
    "            #store the h-step prediction\n",
    "            cv_h_preds.append(preds[:h])\n",
    "            #store the h-step actual value\n",
    "            cv_test.append(val.iloc[i:i+h])    \n",
    "            cv_h_pis.append(pis[:h])\n",
    "                     \n",
    "        cv_preds.append(cv_h_preds)\n",
    "        cv_actuals.append(cv_test)\n",
    "        cv_pis.append(cv_h_pis)\n",
    "        \n",
    "    print('done.\\n')        \n",
    "    return cv_preds, cv_actuals, cv_pis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom functions for calculating CV scores for point predictions and coverage.\n",
    "\n",
    "These functions have been written to work with the output of `time_series_cv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_cv_error(cv_preds, cv_test, error_func):\n",
    "    '''\n",
    "    Forecast error in the current split\n",
    "    \n",
    "    Params:\n",
    "    -----\n",
    "    cv_preds, np.array\n",
    "        Split predictions\n",
    "        \n",
    "    \n",
    "    cv_test: np.array\n",
    "        acutal ground truth observations\n",
    "        \n",
    "    error_func: object\n",
    "        function with signature (y_true, y_preds)\n",
    "        \n",
    "    Returns:\n",
    "    -------\n",
    "        np.ndarray\n",
    "            cross validation errors for split\n",
    "    '''\n",
    "    n_splits = len(cv_preds)\n",
    "    cv_errors = []\n",
    "    \n",
    "    for split in range(n_splits):\n",
    "        pred_error = error_func(cv_test[split], cv_preds[split])\n",
    "        cv_errors.append(pred_error)\n",
    "        \n",
    "    return np.array(cv_errors)\n",
    "\n",
    "def forecast_errors_cv(cv_preds, cv_test, error_func):\n",
    "    '''\n",
    "    Forecast errors by forecast horizon\n",
    "    \n",
    "    Params:\n",
    "    ------\n",
    "    cv_preds: np.ndarray\n",
    "        Array of arrays.  Each array is of size h representing\n",
    "        the forecast horizon specified.\n",
    "        \n",
    "    cv_test: np.ndarray\n",
    "        Array of arrays.  Each array is of size h representing\n",
    "        the forecast horizon specified.\n",
    "        \n",
    "    error_func: object\n",
    "        function with signature (y_true, y_preds)\n",
    "        \n",
    "    Returns:\n",
    "    -------\n",
    "    np.ndarray\n",
    "        \n",
    "    '''\n",
    "    cv_test = np.array(cv_test)\n",
    "    cv_preds = np.array(cv_preds)\n",
    "    n_horizons = len(cv_test)    \n",
    "    \n",
    "    horizon_errors = []\n",
    "    for h in range(n_horizons):\n",
    "        split_errors = split_cv_error(cv_preds[h], cv_test[h], error_func)\n",
    "        horizon_errors.append(split_errors)\n",
    "\n",
    "    return np.array(horizon_errors)\n",
    "\n",
    "def split_coverage(cv_test, cv_intervals):\n",
    "    n_splits = len(cv_test)\n",
    "    cv_errors = []\n",
    "        \n",
    "    for split in range(n_splits):\n",
    "        val = np.asarray(cv_test[split])\n",
    "        lower = cv_intervals[split].T[0]\n",
    "        upper = cv_intervals[split].T[1]\n",
    "        \n",
    "        coverage = len(np.where((val > lower) & (val < upper))[0])\n",
    "        coverage = coverage / len(val)\n",
    "        \n",
    "        cv_errors.append(coverage)\n",
    "        \n",
    "    return np.array(cv_errors)\n",
    "    \n",
    "    \n",
    "def prediction_int_coverage_cv(cv_test, cv_intervals):\n",
    "    cv_test = np.array(cv_test)\n",
    "    cv_intervals = np.array(cv_intervals)\n",
    "    n_horizons = len(cv_test)    \n",
    "    \n",
    "    horizon_coverage = []\n",
    "    for h in range(n_horizons):\n",
    "        split_coverages = split_coverage(cv_test[h], cv_intervals[h])\n",
    "        horizon_coverage.append(split_coverages)\n",
    "\n",
    "    return np.array(horizon_coverage)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_cv_error_scaled(cv_preds, cv_test, y_train):\n",
    "    n_splits = len(cv_preds)\n",
    "    cv_errors = []\n",
    "    \n",
    "    for split in range(n_splits):\n",
    "        pred_error = mean_absolute_scaled_error(cv_test[split], cv_preds[split], \n",
    "                                                y_train, period=7)\n",
    "        \n",
    "        cv_errors.append(pred_error)\n",
    "        \n",
    "    return np.array(cv_errors)\n",
    "\n",
    "def forecast_errors_cv_scaled(cv_preds, cv_test, y_train):\n",
    "    cv_test = np.array(cv_test)\n",
    "    cv_preds = np.array(cv_preds)\n",
    "    n_horizons = len(cv_test)    \n",
    "    \n",
    "    horizon_errors = []\n",
    "    for h in range(n_horizons):\n",
    "        split_errors = split_cv_error_scaled(cv_preds[h], cv_test[h], y_train)\n",
    "        horizon_errors.append(split_errors)\n",
    "        \n",
    "    return np.array(horizon_errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get model and conduct tscv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_comb():\n",
    "    '''\n",
    "    Create ensemble model\n",
    "    '''\n",
    "    model_1 = StateSpaceExponentialSmoothing(trend=False, damped_trend=False)\n",
    "    model_2 = StateSpaceExponentialSmoothing(trend=True)\n",
    "    model_3 = StateSpaceExponentialSmoothing(trend=True, damped_trend=True)\n",
    "    estimators = {'ses': model_1, 'trend': model_2, 'damped_trend': model_3}\n",
    "    return Ensemble(estimators, UnweightedVote())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "split => 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, done.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "horizons = [7, 14, 21, 28, 35, 42, 49, 56, 63, 70, 77, 84, 365]\n",
    "model = get_comb()\n",
    "\n",
    "results = time_series_cv(model, train[REGION], val[REGION], horizons, \n",
    "                         alpha=0.2, step=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>7</th>\n",
       "      <th>14</th>\n",
       "      <th>21</th>\n",
       "      <th>28</th>\n",
       "      <th>35</th>\n",
       "      <th>42</th>\n",
       "      <th>49</th>\n",
       "      <th>56</th>\n",
       "      <th>63</th>\n",
       "      <th>70</th>\n",
       "      <th>77</th>\n",
       "      <th>84</th>\n",
       "      <th>365</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>4.144404</td>\n",
       "      <td>4.387557</td>\n",
       "      <td>4.482939</td>\n",
       "      <td>4.567904</td>\n",
       "      <td>4.711091</td>\n",
       "      <td>4.831040</td>\n",
       "      <td>4.914774</td>\n",
       "      <td>4.998533</td>\n",
       "      <td>5.123216</td>\n",
       "      <td>5.208575</td>\n",
       "      <td>5.291513</td>\n",
       "      <td>5.387491</td>\n",
       "      <td>6.123737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.492429</td>\n",
       "      <td>1.432373</td>\n",
       "      <td>1.333328</td>\n",
       "      <td>1.163322</td>\n",
       "      <td>1.205066</td>\n",
       "      <td>1.235896</td>\n",
       "      <td>1.181915</td>\n",
       "      <td>1.142638</td>\n",
       "      <td>1.138796</td>\n",
       "      <td>1.084850</td>\n",
       "      <td>1.005869</td>\n",
       "      <td>1.047530</td>\n",
       "      <td>1.747465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.783327</td>\n",
       "      <td>2.400819</td>\n",
       "      <td>2.533738</td>\n",
       "      <td>2.849314</td>\n",
       "      <td>3.110957</td>\n",
       "      <td>3.380114</td>\n",
       "      <td>3.400292</td>\n",
       "      <td>3.627686</td>\n",
       "      <td>3.705861</td>\n",
       "      <td>3.765601</td>\n",
       "      <td>3.866121</td>\n",
       "      <td>3.989448</td>\n",
       "      <td>4.344736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>3.082884</td>\n",
       "      <td>3.624873</td>\n",
       "      <td>3.730435</td>\n",
       "      <td>3.952215</td>\n",
       "      <td>3.890757</td>\n",
       "      <td>4.049973</td>\n",
       "      <td>4.096396</td>\n",
       "      <td>4.110047</td>\n",
       "      <td>4.161016</td>\n",
       "      <td>4.296811</td>\n",
       "      <td>4.582256</td>\n",
       "      <td>4.627237</td>\n",
       "      <td>4.552741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>4.216862</td>\n",
       "      <td>4.159924</td>\n",
       "      <td>4.333012</td>\n",
       "      <td>4.352103</td>\n",
       "      <td>4.456228</td>\n",
       "      <td>4.480138</td>\n",
       "      <td>4.756983</td>\n",
       "      <td>4.797748</td>\n",
       "      <td>4.950297</td>\n",
       "      <td>5.105147</td>\n",
       "      <td>5.271748</td>\n",
       "      <td>5.348539</td>\n",
       "      <td>5.610691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>4.818735</td>\n",
       "      <td>4.719601</td>\n",
       "      <td>4.857971</td>\n",
       "      <td>4.662025</td>\n",
       "      <td>5.311139</td>\n",
       "      <td>5.358166</td>\n",
       "      <td>5.429152</td>\n",
       "      <td>5.377739</td>\n",
       "      <td>5.714238</td>\n",
       "      <td>5.970226</td>\n",
       "      <td>5.948374</td>\n",
       "      <td>5.940844</td>\n",
       "      <td>7.050673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>8.936688</td>\n",
       "      <td>8.949729</td>\n",
       "      <td>8.980781</td>\n",
       "      <td>8.500660</td>\n",
       "      <td>8.773975</td>\n",
       "      <td>8.732895</td>\n",
       "      <td>8.647544</td>\n",
       "      <td>8.394020</td>\n",
       "      <td>8.672909</td>\n",
       "      <td>8.428173</td>\n",
       "      <td>8.119109</td>\n",
       "      <td>8.436276</td>\n",
       "      <td>10.739689</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             7          14         21         28         35         42   \\\n",
       "count  27.000000  27.000000  27.000000  27.000000  27.000000  27.000000   \n",
       "mean    4.144404   4.387557   4.482939   4.567904   4.711091   4.831040   \n",
       "std     1.492429   1.432373   1.333328   1.163322   1.205066   1.235896   \n",
       "min     1.783327   2.400819   2.533738   2.849314   3.110957   3.380114   \n",
       "25%     3.082884   3.624873   3.730435   3.952215   3.890757   4.049973   \n",
       "50%     4.216862   4.159924   4.333012   4.352103   4.456228   4.480138   \n",
       "75%     4.818735   4.719601   4.857971   4.662025   5.311139   5.358166   \n",
       "max     8.936688   8.949729   8.980781   8.500660   8.773975   8.732895   \n",
       "\n",
       "             49         56         63         70         77         84   \\\n",
       "count  27.000000  27.000000  27.000000  27.000000  27.000000  27.000000   \n",
       "mean    4.914774   4.998533   5.123216   5.208575   5.291513   5.387491   \n",
       "std     1.181915   1.142638   1.138796   1.084850   1.005869   1.047530   \n",
       "min     3.400292   3.627686   3.705861   3.765601   3.866121   3.989448   \n",
       "25%     4.096396   4.110047   4.161016   4.296811   4.582256   4.627237   \n",
       "50%     4.756983   4.797748   4.950297   5.105147   5.271748   5.348539   \n",
       "75%     5.429152   5.377739   5.714238   5.970226   5.948374   5.940844   \n",
       "max     8.647544   8.394020   8.672909   8.428173   8.119109   8.436276   \n",
       "\n",
       "             365  \n",
       "count  27.000000  \n",
       "mean    6.123737  \n",
       "std     1.747465  \n",
       "min     4.344736  \n",
       "25%     4.552741  \n",
       "50%     5.610691  \n",
       "75%     7.050673  \n",
       "max    10.739689  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_preds, cv_test, cv_intervals = results\n",
    "#CV point predictions smape\n",
    "cv_errors = forecast_errors_cv(cv_preds, cv_test, \n",
    "                               symmetric_mean_absolute_percentage_error)\n",
    "df = pd.DataFrame(cv_errors)\n",
    "df.columns = horizons\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../../results/model_selection/stage1/Trust-comb_smape.csv\n"
     ]
    }
   ],
   "source": [
    "#output sMAPE results to file\n",
    "metric = 'smape'\n",
    "print(f'{TOP_LEVEL}/{STAGE}/{REGION}-{METHOD}_{metric}.csv')\n",
    "df.to_csv(f'{TOP_LEVEL}/{STAGE}/{REGION}-{METHOD}_{metric}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>7</th>\n",
       "      <th>14</th>\n",
       "      <th>21</th>\n",
       "      <th>28</th>\n",
       "      <th>35</th>\n",
       "      <th>42</th>\n",
       "      <th>49</th>\n",
       "      <th>56</th>\n",
       "      <th>63</th>\n",
       "      <th>70</th>\n",
       "      <th>77</th>\n",
       "      <th>84</th>\n",
       "      <th>365</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>116.026222</td>\n",
       "      <td>124.735794</td>\n",
       "      <td>130.024436</td>\n",
       "      <td>133.744223</td>\n",
       "      <td>138.366575</td>\n",
       "      <td>142.342950</td>\n",
       "      <td>145.735258</td>\n",
       "      <td>148.688279</td>\n",
       "      <td>152.245203</td>\n",
       "      <td>154.950908</td>\n",
       "      <td>157.568809</td>\n",
       "      <td>160.322071</td>\n",
       "      <td>173.027134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>42.166203</td>\n",
       "      <td>42.198608</td>\n",
       "      <td>39.495637</td>\n",
       "      <td>36.559464</td>\n",
       "      <td>37.218912</td>\n",
       "      <td>37.958186</td>\n",
       "      <td>36.034207</td>\n",
       "      <td>34.945287</td>\n",
       "      <td>33.765041</td>\n",
       "      <td>31.578792</td>\n",
       "      <td>28.025813</td>\n",
       "      <td>27.079460</td>\n",
       "      <td>39.831774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>46.618512</td>\n",
       "      <td>72.843238</td>\n",
       "      <td>76.824240</td>\n",
       "      <td>91.125853</td>\n",
       "      <td>94.781962</td>\n",
       "      <td>96.130241</td>\n",
       "      <td>97.633007</td>\n",
       "      <td>106.669956</td>\n",
       "      <td>108.718770</td>\n",
       "      <td>104.615344</td>\n",
       "      <td>107.175477</td>\n",
       "      <td>111.059828</td>\n",
       "      <td>128.098587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>85.289554</td>\n",
       "      <td>100.615785</td>\n",
       "      <td>106.938011</td>\n",
       "      <td>110.388183</td>\n",
       "      <td>108.328504</td>\n",
       "      <td>115.894182</td>\n",
       "      <td>114.093023</td>\n",
       "      <td>116.842668</td>\n",
       "      <td>119.950491</td>\n",
       "      <td>134.037732</td>\n",
       "      <td>141.630531</td>\n",
       "      <td>145.646319</td>\n",
       "      <td>137.413187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>112.886163</td>\n",
       "      <td>116.403959</td>\n",
       "      <td>120.862203</td>\n",
       "      <td>121.641565</td>\n",
       "      <td>129.204218</td>\n",
       "      <td>133.055584</td>\n",
       "      <td>136.099152</td>\n",
       "      <td>141.010069</td>\n",
       "      <td>153.949997</td>\n",
       "      <td>154.526077</td>\n",
       "      <td>154.339609</td>\n",
       "      <td>156.354794</td>\n",
       "      <td>166.053986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>139.239252</td>\n",
       "      <td>137.553280</td>\n",
       "      <td>145.318354</td>\n",
       "      <td>143.450635</td>\n",
       "      <td>151.259772</td>\n",
       "      <td>163.334095</td>\n",
       "      <td>169.530709</td>\n",
       "      <td>165.021614</td>\n",
       "      <td>167.329742</td>\n",
       "      <td>169.726308</td>\n",
       "      <td>176.439025</td>\n",
       "      <td>182.600954</td>\n",
       "      <td>193.672368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>259.088004</td>\n",
       "      <td>250.807024</td>\n",
       "      <td>238.009558</td>\n",
       "      <td>224.213457</td>\n",
       "      <td>227.454278</td>\n",
       "      <td>245.971990</td>\n",
       "      <td>230.194700</td>\n",
       "      <td>228.163770</td>\n",
       "      <td>225.293410</td>\n",
       "      <td>220.559616</td>\n",
       "      <td>214.135401</td>\n",
       "      <td>220.543392</td>\n",
       "      <td>275.251753</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              7           14          21          28          35          42   \\\n",
       "count   27.000000   27.000000   27.000000   27.000000   27.000000   27.000000   \n",
       "mean   116.026222  124.735794  130.024436  133.744223  138.366575  142.342950   \n",
       "std     42.166203   42.198608   39.495637   36.559464   37.218912   37.958186   \n",
       "min     46.618512   72.843238   76.824240   91.125853   94.781962   96.130241   \n",
       "25%     85.289554  100.615785  106.938011  110.388183  108.328504  115.894182   \n",
       "50%    112.886163  116.403959  120.862203  121.641565  129.204218  133.055584   \n",
       "75%    139.239252  137.553280  145.318354  143.450635  151.259772  163.334095   \n",
       "max    259.088004  250.807024  238.009558  224.213457  227.454278  245.971990   \n",
       "\n",
       "              49          56          63          70          77          84   \\\n",
       "count   27.000000   27.000000   27.000000   27.000000   27.000000   27.000000   \n",
       "mean   145.735258  148.688279  152.245203  154.950908  157.568809  160.322071   \n",
       "std     36.034207   34.945287   33.765041   31.578792   28.025813   27.079460   \n",
       "min     97.633007  106.669956  108.718770  104.615344  107.175477  111.059828   \n",
       "25%    114.093023  116.842668  119.950491  134.037732  141.630531  145.646319   \n",
       "50%    136.099152  141.010069  153.949997  154.526077  154.339609  156.354794   \n",
       "75%    169.530709  165.021614  167.329742  169.726308  176.439025  182.600954   \n",
       "max    230.194700  228.163770  225.293410  220.559616  214.135401  220.543392   \n",
       "\n",
       "              365  \n",
       "count   27.000000  \n",
       "mean   173.027134  \n",
       "std     39.831774  \n",
       "min    128.098587  \n",
       "25%    137.413187  \n",
       "50%    166.053986  \n",
       "75%    193.672368  \n",
       "max    275.251753  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#CV point predictions rmse\n",
    "cv_errors = forecast_errors_cv(cv_preds, cv_test, root_mean_squared_error)\n",
    "df = pd.DataFrame(cv_errors)\n",
    "df.columns = horizons\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../../results/model_selection/stage1/Trust-comb_rmse.csv\n"
     ]
    }
   ],
   "source": [
    "#output rmse\n",
    "metric = 'rmse'\n",
    "print(f'{TOP_LEVEL}/{STAGE}/{REGION}-{METHOD}_{metric}.csv')\n",
    "df.to_csv(f'{TOP_LEVEL}/{STAGE}/{REGION}-{METHOD}_{metric}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>7</th>\n",
       "      <th>14</th>\n",
       "      <th>21</th>\n",
       "      <th>28</th>\n",
       "      <th>35</th>\n",
       "      <th>42</th>\n",
       "      <th>49</th>\n",
       "      <th>56</th>\n",
       "      <th>63</th>\n",
       "      <th>70</th>\n",
       "      <th>77</th>\n",
       "      <th>84</th>\n",
       "      <th>365</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.122101</td>\n",
       "      <td>1.188923</td>\n",
       "      <td>1.216383</td>\n",
       "      <td>1.240823</td>\n",
       "      <td>1.280217</td>\n",
       "      <td>1.313480</td>\n",
       "      <td>1.336937</td>\n",
       "      <td>1.360145</td>\n",
       "      <td>1.394237</td>\n",
       "      <td>1.417573</td>\n",
       "      <td>1.440587</td>\n",
       "      <td>1.467164</td>\n",
       "      <td>1.652850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.450470</td>\n",
       "      <td>0.438261</td>\n",
       "      <td>0.410775</td>\n",
       "      <td>0.365715</td>\n",
       "      <td>0.371286</td>\n",
       "      <td>0.376176</td>\n",
       "      <td>0.356261</td>\n",
       "      <td>0.341648</td>\n",
       "      <td>0.335756</td>\n",
       "      <td>0.315250</td>\n",
       "      <td>0.287835</td>\n",
       "      <td>0.294382</td>\n",
       "      <td>0.469694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.477044</td>\n",
       "      <td>0.654026</td>\n",
       "      <td>0.705157</td>\n",
       "      <td>0.799043</td>\n",
       "      <td>0.877631</td>\n",
       "      <td>0.917456</td>\n",
       "      <td>0.926158</td>\n",
       "      <td>0.979531</td>\n",
       "      <td>1.002695</td>\n",
       "      <td>1.011391</td>\n",
       "      <td>1.009348</td>\n",
       "      <td>1.034784</td>\n",
       "      <td>1.182472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.845205</td>\n",
       "      <td>0.959518</td>\n",
       "      <td>0.982888</td>\n",
       "      <td>1.019405</td>\n",
       "      <td>1.016767</td>\n",
       "      <td>1.096668</td>\n",
       "      <td>1.052117</td>\n",
       "      <td>1.072047</td>\n",
       "      <td>1.119535</td>\n",
       "      <td>1.193911</td>\n",
       "      <td>1.234461</td>\n",
       "      <td>1.252797</td>\n",
       "      <td>1.236548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.074557</td>\n",
       "      <td>1.109406</td>\n",
       "      <td>1.127614</td>\n",
       "      <td>1.136101</td>\n",
       "      <td>1.151357</td>\n",
       "      <td>1.171257</td>\n",
       "      <td>1.277179</td>\n",
       "      <td>1.317182</td>\n",
       "      <td>1.323615</td>\n",
       "      <td>1.341009</td>\n",
       "      <td>1.452884</td>\n",
       "      <td>1.469867</td>\n",
       "      <td>1.530274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.296387</td>\n",
       "      <td>1.267473</td>\n",
       "      <td>1.291686</td>\n",
       "      <td>1.256845</td>\n",
       "      <td>1.411692</td>\n",
       "      <td>1.500616</td>\n",
       "      <td>1.515765</td>\n",
       "      <td>1.519339</td>\n",
       "      <td>1.546098</td>\n",
       "      <td>1.631714</td>\n",
       "      <td>1.628111</td>\n",
       "      <td>1.619943</td>\n",
       "      <td>1.910424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2.728739</td>\n",
       "      <td>2.627448</td>\n",
       "      <td>2.601284</td>\n",
       "      <td>2.454834</td>\n",
       "      <td>2.519974</td>\n",
       "      <td>2.502644</td>\n",
       "      <td>2.472801</td>\n",
       "      <td>2.399179</td>\n",
       "      <td>2.471969</td>\n",
       "      <td>2.403369</td>\n",
       "      <td>2.316685</td>\n",
       "      <td>2.402194</td>\n",
       "      <td>3.019100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             7          14         21         28         35         42   \\\n",
       "count  27.000000  27.000000  27.000000  27.000000  27.000000  27.000000   \n",
       "mean    1.122101   1.188923   1.216383   1.240823   1.280217   1.313480   \n",
       "std     0.450470   0.438261   0.410775   0.365715   0.371286   0.376176   \n",
       "min     0.477044   0.654026   0.705157   0.799043   0.877631   0.917456   \n",
       "25%     0.845205   0.959518   0.982888   1.019405   1.016767   1.096668   \n",
       "50%     1.074557   1.109406   1.127614   1.136101   1.151357   1.171257   \n",
       "75%     1.296387   1.267473   1.291686   1.256845   1.411692   1.500616   \n",
       "max     2.728739   2.627448   2.601284   2.454834   2.519974   2.502644   \n",
       "\n",
       "             49         56         63         70         77         84   \\\n",
       "count  27.000000  27.000000  27.000000  27.000000  27.000000  27.000000   \n",
       "mean    1.336937   1.360145   1.394237   1.417573   1.440587   1.467164   \n",
       "std     0.356261   0.341648   0.335756   0.315250   0.287835   0.294382   \n",
       "min     0.926158   0.979531   1.002695   1.011391   1.009348   1.034784   \n",
       "25%     1.052117   1.072047   1.119535   1.193911   1.234461   1.252797   \n",
       "50%     1.277179   1.317182   1.323615   1.341009   1.452884   1.469867   \n",
       "75%     1.515765   1.519339   1.546098   1.631714   1.628111   1.619943   \n",
       "max     2.472801   2.399179   2.471969   2.403369   2.316685   2.402194   \n",
       "\n",
       "             365  \n",
       "count  27.000000  \n",
       "mean    1.652850  \n",
       "std     0.469694  \n",
       "min     1.182472  \n",
       "25%     1.236548  \n",
       "50%     1.530274  \n",
       "75%     1.910424  \n",
       "max     3.019100  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#mase\n",
    "cv_errors = forecast_errors_cv_scaled(cv_preds, cv_test, train[REGION])\n",
    "df = pd.DataFrame(cv_errors)\n",
    "df.columns = horizons\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../../results/model_selection/stage1/Trust-comb_mase.csv\n"
     ]
    }
   ],
   "source": [
    "#output rmse\n",
    "metric = 'mase'\n",
    "print(f'{TOP_LEVEL}/{STAGE}/{REGION}-{METHOD}_{metric}.csv')\n",
    "df.to_csv(f'{TOP_LEVEL}/{STAGE}/{REGION}-{METHOD}_{metric}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>7</th>\n",
       "      <th>14</th>\n",
       "      <th>21</th>\n",
       "      <th>28</th>\n",
       "      <th>35</th>\n",
       "      <th>42</th>\n",
       "      <th>49</th>\n",
       "      <th>56</th>\n",
       "      <th>63</th>\n",
       "      <th>70</th>\n",
       "      <th>77</th>\n",
       "      <th>84</th>\n",
       "      <th>365</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.841270</td>\n",
       "      <td>0.910053</td>\n",
       "      <td>0.936508</td>\n",
       "      <td>0.947090</td>\n",
       "      <td>0.954497</td>\n",
       "      <td>0.961199</td>\n",
       "      <td>0.965986</td>\n",
       "      <td>0.967593</td>\n",
       "      <td>0.970018</td>\n",
       "      <td>0.973016</td>\n",
       "      <td>0.974988</td>\n",
       "      <td>0.977072</td>\n",
       "      <td>0.994622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.139146</td>\n",
       "      <td>0.096225</td>\n",
       "      <td>0.064702</td>\n",
       "      <td>0.054588</td>\n",
       "      <td>0.047717</td>\n",
       "      <td>0.039703</td>\n",
       "      <td>0.033961</td>\n",
       "      <td>0.031338</td>\n",
       "      <td>0.027959</td>\n",
       "      <td>0.025163</td>\n",
       "      <td>0.022473</td>\n",
       "      <td>0.020600</td>\n",
       "      <td>0.004835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>0.828571</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.877551</td>\n",
       "      <td>0.892857</td>\n",
       "      <td>0.904762</td>\n",
       "      <td>0.914286</td>\n",
       "      <td>0.922078</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>0.983562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.904762</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>0.942857</td>\n",
       "      <td>0.952381</td>\n",
       "      <td>0.959184</td>\n",
       "      <td>0.955357</td>\n",
       "      <td>0.952381</td>\n",
       "      <td>0.957143</td>\n",
       "      <td>0.961039</td>\n",
       "      <td>0.964286</td>\n",
       "      <td>0.991781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>0.952381</td>\n",
       "      <td>0.964286</td>\n",
       "      <td>0.971429</td>\n",
       "      <td>0.976190</td>\n",
       "      <td>0.979592</td>\n",
       "      <td>0.982143</td>\n",
       "      <td>0.968254</td>\n",
       "      <td>0.971429</td>\n",
       "      <td>0.974026</td>\n",
       "      <td>0.976190</td>\n",
       "      <td>0.994521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.991071</td>\n",
       "      <td>0.992063</td>\n",
       "      <td>0.992857</td>\n",
       "      <td>0.987013</td>\n",
       "      <td>0.988095</td>\n",
       "      <td>0.997260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             7          14         21         28         35         42   \\\n",
       "count  27.000000  27.000000  27.000000  27.000000  27.000000  27.000000   \n",
       "mean    0.841270   0.910053   0.936508   0.947090   0.954497   0.961199   \n",
       "std     0.139146   0.096225   0.064702   0.054588   0.047717   0.039703   \n",
       "min     0.571429   0.571429   0.714286   0.785714   0.828571   0.857143   \n",
       "25%     0.714286   0.857143   0.904762   0.928571   0.942857   0.952381   \n",
       "50%     0.857143   0.928571   0.952381   0.964286   0.971429   0.976190   \n",
       "75%     1.000000   1.000000   1.000000   1.000000   1.000000   1.000000   \n",
       "max     1.000000   1.000000   1.000000   1.000000   1.000000   1.000000   \n",
       "\n",
       "             49         56         63         70         77         84   \\\n",
       "count  27.000000  27.000000  27.000000  27.000000  27.000000  27.000000   \n",
       "mean    0.965986   0.967593   0.970018   0.973016   0.974988   0.977072   \n",
       "std     0.033961   0.031338   0.027959   0.025163   0.022473   0.020600   \n",
       "min     0.877551   0.892857   0.904762   0.914286   0.922078   0.928571   \n",
       "25%     0.959184   0.955357   0.952381   0.957143   0.961039   0.964286   \n",
       "50%     0.979592   0.982143   0.968254   0.971429   0.974026   0.976190   \n",
       "75%     1.000000   0.991071   0.992063   0.992857   0.987013   0.988095   \n",
       "max     1.000000   1.000000   1.000000   1.000000   1.000000   1.000000   \n",
       "\n",
       "             365  \n",
       "count  27.000000  \n",
       "mean    0.994622  \n",
       "std     0.004835  \n",
       "min     0.983562  \n",
       "25%     0.991781  \n",
       "50%     0.994521  \n",
       "75%     0.997260  \n",
       "max     1.000000  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#80% PIs\n",
    "cv_coverage = prediction_int_coverage_cv(cv_test, cv_intervals)\n",
    "df = pd.DataFrame(cv_coverage)\n",
    "df.columns = horizons\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../../results/model_selection/stage1/Trust-comb_coverage_80.csv\n"
     ]
    }
   ],
   "source": [
    "#output 80% PI coverage\n",
    "metric = 'coverage_80'\n",
    "print(f'{TOP_LEVEL}/{STAGE}/{REGION}-{METHOD}_{metric}.csv')\n",
    "df.to_csv(f'{TOP_LEVEL}/{STAGE}/{REGION}-{METHOD}_{metric}.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rerun for 95% PI coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "split => 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, done.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "horizons = [7, 14, 21, 28, 35, 42, 49, 56, 63, 70, 77, 84, 365]\n",
    "model = get_comb()\n",
    "\n",
    "results = time_series_cv(model, train[REGION], val[REGION], horizons, \n",
    "                         alpha=0.05, step=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>7</th>\n",
       "      <th>14</th>\n",
       "      <th>21</th>\n",
       "      <th>28</th>\n",
       "      <th>35</th>\n",
       "      <th>42</th>\n",
       "      <th>49</th>\n",
       "      <th>56</th>\n",
       "      <th>63</th>\n",
       "      <th>70</th>\n",
       "      <th>77</th>\n",
       "      <th>84</th>\n",
       "      <th>365</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.957672</td>\n",
       "      <td>0.973545</td>\n",
       "      <td>0.982363</td>\n",
       "      <td>0.985450</td>\n",
       "      <td>0.987302</td>\n",
       "      <td>0.989418</td>\n",
       "      <td>0.990930</td>\n",
       "      <td>0.991402</td>\n",
       "      <td>0.992357</td>\n",
       "      <td>0.993122</td>\n",
       "      <td>0.993747</td>\n",
       "      <td>0.994268</td>\n",
       "      <td>0.998681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.077387</td>\n",
       "      <td>0.052966</td>\n",
       "      <td>0.035311</td>\n",
       "      <td>0.026688</td>\n",
       "      <td>0.021459</td>\n",
       "      <td>0.017883</td>\n",
       "      <td>0.015328</td>\n",
       "      <td>0.014329</td>\n",
       "      <td>0.012737</td>\n",
       "      <td>0.011463</td>\n",
       "      <td>0.010421</td>\n",
       "      <td>0.009553</td>\n",
       "      <td>0.002198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.892857</td>\n",
       "      <td>0.914286</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>0.938776</td>\n",
       "      <td>0.946429</td>\n",
       "      <td>0.952381</td>\n",
       "      <td>0.957143</td>\n",
       "      <td>0.961039</td>\n",
       "      <td>0.964286</td>\n",
       "      <td>0.991781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.928571</td>\n",
       "      <td>0.964286</td>\n",
       "      <td>0.976190</td>\n",
       "      <td>0.964286</td>\n",
       "      <td>0.971429</td>\n",
       "      <td>0.976190</td>\n",
       "      <td>0.979592</td>\n",
       "      <td>0.982143</td>\n",
       "      <td>0.984127</td>\n",
       "      <td>0.985714</td>\n",
       "      <td>0.987013</td>\n",
       "      <td>0.988095</td>\n",
       "      <td>0.997260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             7          14         21         28         35         42   \\\n",
       "count  27.000000  27.000000  27.000000  27.000000  27.000000  27.000000   \n",
       "mean    0.957672   0.973545   0.982363   0.985450   0.987302   0.989418   \n",
       "std     0.077387   0.052966   0.035311   0.026688   0.021459   0.017883   \n",
       "min     0.714286   0.785714   0.857143   0.892857   0.914286   0.928571   \n",
       "25%     0.928571   0.964286   0.976190   0.964286   0.971429   0.976190   \n",
       "50%     1.000000   1.000000   1.000000   1.000000   1.000000   1.000000   \n",
       "75%     1.000000   1.000000   1.000000   1.000000   1.000000   1.000000   \n",
       "max     1.000000   1.000000   1.000000   1.000000   1.000000   1.000000   \n",
       "\n",
       "             49         56         63         70         77         84   \\\n",
       "count  27.000000  27.000000  27.000000  27.000000  27.000000  27.000000   \n",
       "mean    0.990930   0.991402   0.992357   0.993122   0.993747   0.994268   \n",
       "std     0.015328   0.014329   0.012737   0.011463   0.010421   0.009553   \n",
       "min     0.938776   0.946429   0.952381   0.957143   0.961039   0.964286   \n",
       "25%     0.979592   0.982143   0.984127   0.985714   0.987013   0.988095   \n",
       "50%     1.000000   1.000000   1.000000   1.000000   1.000000   1.000000   \n",
       "75%     1.000000   1.000000   1.000000   1.000000   1.000000   1.000000   \n",
       "max     1.000000   1.000000   1.000000   1.000000   1.000000   1.000000   \n",
       "\n",
       "             365  \n",
       "count  27.000000  \n",
       "mean    0.998681  \n",
       "std     0.002198  \n",
       "min     0.991781  \n",
       "25%     0.997260  \n",
       "50%     1.000000  \n",
       "75%     1.000000  \n",
       "max     1.000000  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#95% PIs\n",
    "cv_preds, cv_test, cv_intervals = results\n",
    "cv_coverage = prediction_int_coverage_cv(cv_test, cv_intervals)\n",
    "df = pd.DataFrame(cv_coverage)\n",
    "df.columns = horizons\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../../results/model_selection/stage1/Trust-comb_coverage_95.csv\n"
     ]
    }
   ],
   "source": [
    "#output 95% PI coverage\n",
    "metric = 'coverage_95'\n",
    "print(f'{TOP_LEVEL}/{STAGE}/{REGION}-{METHOD}_{metric}.csv')\n",
    "df.to_csv(f'{TOP_LEVEL}/{STAGE}/{REGION}-{METHOD}_{metric}.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ambo",
   "language": "python",
   "name": "ambo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
