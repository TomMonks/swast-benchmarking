{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross validation: An Ensemble of Facebook's Prophet, Regression with ARIMA errors and Holt-Winters\n",
    "---\n",
    "\n",
    "An unweighted ensemble of \n",
    "\n",
    "* Facebook Prophet with New Year holiday;\n",
    "* Regression with new year holiday and (auto) ARIMA errors; and \n",
    "* Holt-Winters Exponential Smoothing (statespace formulation)\n",
    "\n",
    "The ARIMA process modelled is a (1,1,3)(1,0,1,7).  This was determined by auto_arima from the `pmdarima` package.\n",
    "\n",
    "Note that HW provides now way to easily incorporate special events.\n",
    "\n",
    "This notebook conducts cross validation of the method using a rolling forecast origin method.\n",
    "\n",
    "> The Reg with ARIMA error model is based on SWAST's data.  It may be prudent for each trust to fit their own automatic ARIMA model.\n",
    "\n",
    "**The notebook outputs:**\n",
    "* MASE, RMSE and MAPE at 7 day intervals from 7 to 84 days.\n",
    "* 80 and 95% prediction intervals between 7 and 84 days.\n",
    "\n",
    "These are saved into the folder `results/model_selection/stage1/`\n",
    "\n",
    "---\n",
    "\n",
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#error measures\n",
    "from forecast_tools.metrics import (mean_absolute_scaled_error, \n",
    "                                    root_mean_squared_error,\n",
    "                                    symmetric_mean_absolute_percentage_error)\n",
    "\n",
    "#models\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.tsa.statespace.exponential_smoothing import ExponentialSmoothing\n",
    "from fbprophet import Prophet\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.11.0\n",
      "1.5.2\n"
     ]
    }
   ],
   "source": [
    "import statsmodels as sm\n",
    "import pmdarima \n",
    "\n",
    "print(sm.__version__)\n",
    "print(pmdarima.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to select exceptionally busy days as covariates.\n",
    "from amb_forecast.feature_engineering import (regular_busy_calender_days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#custom ensemble class\n",
    "from amb_forecast.ensemble import (Ensemble, UnweightedVote)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Input\n",
    "\n",
    "The constants `TOP_LEVEL`, `STAGE`, `REGION`,`TRUST` and `METHOD` are used to control data selection and the directory for outputting results.  \n",
    "\n",
    "> Output file is `f'{TOP_LEVEL}/{STAGE}/{REGION}-{METHOD}_{metric}.csv'.csv`.  where metric will be smape, rmse, mase, coverage_80 and coverage_95. Note: `REGION`: is also used to select the correct data from the input dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOP_LEVEL = '../../../results/model_selection'\n",
    "STAGE = 'stage1'\n",
    "REGION = 'Trust'\n",
    "METHOD = 'fbp-arima-hw'\n",
    "\n",
    "FILE_NAME = 'Daily_Responses_5_Years_2019_full.csv'\n",
    "\n",
    "#split training and test data.\n",
    "TEST_SPLIT_DATE = '2019-01-01'\n",
    "\n",
    "#second subdivide: train and val\n",
    "VAL_SPLIT_DATE = '2017-07-01'\n",
    "\n",
    "#discard data after 2020 due to coronavirus\n",
    "#this is the subject of a seperate study.\n",
    "DISCARD_DATE = '2020-01-01'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in path\n",
    "path = f'../../../data/{FILE_NAME}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process_daily_data(path, index_col, by_col, \n",
    "                           values, dayfirst=False):\n",
    "    '''\n",
    "    Daily data is stored in long format.  Read in \n",
    "    and pivot to wide format so that there is a single \n",
    "    colmumn for each regions time series.\n",
    "    '''\n",
    "    df = pd.read_csv(path, index_col=index_col, parse_dates=True, \n",
    "                     dayfirst=dayfirst)\n",
    "    df.columns = map(str.lower, df.columns)\n",
    "    df.index.rename(str(df.index.name).lower(), inplace=True)\n",
    "    \n",
    "    clean_table = pd.pivot_table(df, values=values.lower(), \n",
    "                                 index=[index_col.lower()],\n",
    "                                 columns=[by_col.lower()], aggfunc=np.sum)\n",
    "    \n",
    "    clean_table.index.freq = 'D'\n",
    "    \n",
    "    return clean_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>ora</th>\n",
       "      <th>BNSSG</th>\n",
       "      <th>Cornwall</th>\n",
       "      <th>Devon</th>\n",
       "      <th>Dorset</th>\n",
       "      <th>Gloucestershire</th>\n",
       "      <th>OOA</th>\n",
       "      <th>Somerset</th>\n",
       "      <th>Trust</th>\n",
       "      <th>Wiltshire</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>actual_dt</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2013-12-30</th>\n",
       "      <td>415.0</td>\n",
       "      <td>220.0</td>\n",
       "      <td>502.0</td>\n",
       "      <td>336.0</td>\n",
       "      <td>129.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>183.0</td>\n",
       "      <td>2042.0</td>\n",
       "      <td>255.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-12-31</th>\n",
       "      <td>420.0</td>\n",
       "      <td>236.0</td>\n",
       "      <td>468.0</td>\n",
       "      <td>302.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>180.0</td>\n",
       "      <td>1996.0</td>\n",
       "      <td>260.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-01-01</th>\n",
       "      <td>549.0</td>\n",
       "      <td>341.0</td>\n",
       "      <td>566.0</td>\n",
       "      <td>392.0</td>\n",
       "      <td>157.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>213.0</td>\n",
       "      <td>2570.0</td>\n",
       "      <td>351.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-01-02</th>\n",
       "      <td>450.0</td>\n",
       "      <td>218.0</td>\n",
       "      <td>499.0</td>\n",
       "      <td>301.0</td>\n",
       "      <td>115.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>167.0</td>\n",
       "      <td>2013.0</td>\n",
       "      <td>258.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-01-03</th>\n",
       "      <td>419.0</td>\n",
       "      <td>229.0</td>\n",
       "      <td>503.0</td>\n",
       "      <td>304.0</td>\n",
       "      <td>135.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>195.0</td>\n",
       "      <td>2056.0</td>\n",
       "      <td>269.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "ora         BNSSG  Cornwall  Devon  Dorset  Gloucestershire  OOA  Somerset  \\\n",
       "actual_dt                                                                    \n",
       "2013-12-30  415.0     220.0  502.0   336.0            129.0  NaN     183.0   \n",
       "2013-12-31  420.0     236.0  468.0   302.0            128.0  NaN     180.0   \n",
       "2014-01-01  549.0     341.0  566.0   392.0            157.0  NaN     213.0   \n",
       "2014-01-02  450.0     218.0  499.0   301.0            115.0  NaN     167.0   \n",
       "2014-01-03  419.0     229.0  503.0   304.0            135.0  NaN     195.0   \n",
       "\n",
       "ora          Trust  Wiltshire  \n",
       "actual_dt                      \n",
       "2013-12-30  2042.0      255.0  \n",
       "2013-12-31  1996.0      260.0  \n",
       "2014-01-01  2570.0      351.0  \n",
       "2014-01-02  2013.0      258.0  \n",
       "2014-01-03  2056.0      269.0  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean = pre_process_daily_data(path, 'Actual_dt', 'ORA', 'Actual_Value', \n",
    "                               dayfirst=False)\n",
    "clean.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ts_train_test_split(data, split_date):\n",
    "    '''\n",
    "    Split time series into training and test data\n",
    "    \n",
    "    Parameters:\n",
    "    -------\n",
    "    data - pd.DataFrame - time series data.  Index expected as datatimeindex\n",
    "    split_date - the date on which to split the time series\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple (len=2) \n",
    "    0. pandas.DataFrame - training dataset\n",
    "    1. pandas.DataFrame - test dataset\n",
    "    '''\n",
    "    train = data.loc[data.index < split_date]\n",
    "    test = data.loc[data.index >= split_date]\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = ts_train_test_split(clean, split_date=TEST_SPLIT_DATE)\n",
    "\n",
    "#exclude data after 2020 due to coronavirus.\n",
    "test, discard = ts_train_test_split(test, split_date=DISCARD_DATE)\n",
    "\n",
    "#train split into train and validation\n",
    "train, val = ts_train_test_split(train, split_date=VAL_SPLIT_DATE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1279, 9)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#amount of training data\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(549, 9)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#amount of validation data\n",
    "val.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New years day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "exceptional = regular_busy_calender_days(train[REGION], quantile=0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_year = pd.DataFrame({\n",
    "                         'holiday': 'new_year',\n",
    "                         'ds': pd.date_range(start=exceptional[0], \n",
    "                                             periods=20, \n",
    "                                             freq='YS')\n",
    "                        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>holiday</th>\n",
       "      <th>ds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>new_year</td>\n",
       "      <td>2013-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>new_year</td>\n",
       "      <td>2014-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>new_year</td>\n",
       "      <td>2015-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>new_year</td>\n",
       "      <td>2016-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>new_year</td>\n",
       "      <td>2017-01-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    holiday         ds\n",
       "0  new_year 2013-01-01\n",
       "1  new_year 2014-01-01\n",
       "2  new_year 2015-01-01\n",
       "3  new_year 2016-01-01\n",
       "4  new_year 2017-01-01"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_year.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wrapper classes for Prophet, statsmodels ARIMA and statsmodels Exponential Smoothing.\n",
    "\n",
    "Adapter/wrapper classes to enable usage within `Ensemble` class and work with cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FbProphetWrapper(object):\n",
    "    '''\n",
    "    Facade for FBProphet object - so that it can be\n",
    "    used within Ensemble with methods from other packages\n",
    "\n",
    "    '''\n",
    "    def __init__(self, training_index, holidays=None, interval_width=0.8,\n",
    "                 mcmc_samples=0, changepoint_prior_scale=0.05):\n",
    "        self._training_index = training_index\n",
    "        self._holidays = holidays\n",
    "        self._interval_width = interval_width\n",
    "        self._mcmc_samples = mcmc_samples\n",
    "        self._cp_prior_scale = changepoint_prior_scale\n",
    "\n",
    "    def _get_resids(self):\n",
    "        return self._train - self._forecast['yhat'][:-self._h]\n",
    "\n",
    "    def _get_preds(self):\n",
    "        return self._forecast['yhat'][:-self._h].to_numpy()\n",
    "\n",
    "    def fit(self, train):\n",
    "        \n",
    "        self._model = Prophet(holidays=self._holidays, \n",
    "                              interval_width=self._interval_width,\n",
    "                              mcmc_samples=self._mcmc_samples,\n",
    "                              changepoint_prior_scale=self._cp_prior_scale,\n",
    "                              daily_seasonality=False)\n",
    "        \n",
    "        \n",
    "        self._model.fit(self._pre_process_training(train))\n",
    "        self._t = len(train)\n",
    "        self._train = train\n",
    "        self.predict(len(train))\n",
    "\n",
    "    def _pre_process_training(self, train):\n",
    "\n",
    "        if len(train.shape) > 1:\n",
    "            y_train = train[:, 0]\n",
    "        else:\n",
    "            y_train = train\n",
    "\n",
    "        y_train = np.asarray(y_train)\n",
    "            \n",
    "        #extend the training index\n",
    "        if len(y_train) > len(self._training_index):\n",
    "            self._training_index = pd.date_range(start=self._training_index[0], \n",
    "                                                 periods=len(y_train),\n",
    "                                                 freq=self._training_index.freq)\n",
    "        \n",
    "        \n",
    "        prophet_train = pd.DataFrame(self._training_index)\n",
    "        prophet_train['y'] = y_train\n",
    "        prophet_train.columns = ['ds', 'y']\n",
    "        \n",
    "        return prophet_train\n",
    "\n",
    "    def predict(self, h, return_conf_int=False, alpha=0.2):\n",
    "        '''\n",
    "        forecast h steps ahead.\n",
    "        \n",
    "        Params:\n",
    "        ------\n",
    "        h: int\n",
    "            h-step forecast\n",
    "        \n",
    "        return_conf_int: bool, optional (default=False)\n",
    "            return 1 - alpha PI\n",
    "        \n",
    "        alpha: float, optional (default=0.2)\n",
    "            return 1 - alpha PI\n",
    "                       \n",
    "        Returns:\n",
    "        -------\n",
    "        np.array\n",
    "            If return_conf_int = False returns preds only\n",
    "            \n",
    "        np.array, np.array\n",
    "            If return_conf_int = True returns tuple of preds, pred_ints\n",
    "        '''\n",
    "        if isinstance(h, (np.ndarray, pd.DataFrame)):\n",
    "            h = len(h)\n",
    "        \n",
    "        self._h = h\n",
    "        future = self._model.make_future_dataframe(periods=h)\n",
    "        self._forecast = self._model.predict(future)\n",
    "\n",
    "        if return_conf_int:\n",
    "            return (self._forecast['yhat'][-h:].to_numpy(), \n",
    "                    self._forecast[['yhat_lower', 'yhat_upper']][-h:].to_numpy())\n",
    "        else:\n",
    "            return self._forecast['yhat'][-h:].to_numpy()\n",
    "            \n",
    "\n",
    "    fittedvalues = property(_get_preds)\n",
    "    resid = property(_get_resids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ARIMAWrapper(object):\n",
    "    '''\n",
    "    Facade for statsmodels.statespace\n",
    "    '''\n",
    "    def __init__(self, order, seasonal_order, training_index, holidays=None):\n",
    "        self._order = order\n",
    "        self._seasonal_order = seasonal_order\n",
    "        self._training_index = training_index\n",
    "        self._holidays = holidays\n",
    "\n",
    "    def _get_resids(self):\n",
    "        return self._fitted.resid\n",
    "\n",
    "    def _get_preds(self):\n",
    "        return self._fitted.fittedvalues\n",
    "    \n",
    "    def _encode_holidays(self, holidays, idx):\n",
    "        dummy = idx.isin(holidays).astype(int)\n",
    "        dummy = pd.DataFrame(dummy)\n",
    "        dummy.columns = ['holiday']\n",
    "        dummy.index = idx\n",
    "        return dummy\n",
    "\n",
    "    def fit(self, y_train):\n",
    "        \n",
    "        #extend training index\n",
    "        if len(y_train) > len(self._training_index):\n",
    "\n",
    "            self._training_index = pd.date_range(start=self._training_index[0], \n",
    "                                                 periods=len(y_train),\n",
    "                                                 freq=self._training_index.freq)\n",
    "            \n",
    "        holiday_train = None\n",
    "        if not self._holidays is None:\n",
    "            holiday_train = self._encode_holidays(self._holidays, \n",
    "                                                  self._training_index)\n",
    "    \n",
    "        \n",
    "        self._model = ARIMA(endog=y_train,\n",
    "                            exog=holiday_train,\n",
    "                            order=self._order, \n",
    "                            seasonal_order=self._seasonal_order)#,\n",
    "                            #enforce_stationarity=False)\n",
    "        \n",
    "        self._fitted = self._model.fit()\n",
    "        self._t = len(train)\n",
    "        \n",
    "    \n",
    "    def predict(self, horizon, return_conf_int=False, alpha=0.2):\n",
    "        '''\n",
    "        forecast h steps ahead.\n",
    "        \n",
    "        Params:\n",
    "        ------\n",
    "        h: int\n",
    "            h-step forecast\n",
    "        \n",
    "        return_conf_int: bool, optional (default=False)\n",
    "            return 1 - alpha PI\n",
    "        \n",
    "        alpha: float, optional (default=0.2)\n",
    "            return 1 - alpha PI\n",
    "                       \n",
    "        Returns:\n",
    "        -------\n",
    "        np.array\n",
    "            If return_conf_int = False returns preds only\n",
    "            \n",
    "        np.array, np.array\n",
    "            If return_conf_int = True returns tuple of preds, pred_ints\n",
    "        '''\n",
    "        \n",
    "        #+1 to date range then trim off the first value\n",
    "\n",
    "        f_idx = pd.date_range(start=self._training_index[-1], \n",
    "                              periods=horizon+1,\n",
    "                              freq=self._training_index.freq)[1:]\n",
    "        \n",
    "        #encode holidays if included.\n",
    "        exog_holiday = None\n",
    "        if not self._holidays is None:\n",
    "            exog_holiday = self._encode_holidays(self._holidays, f_idx)\n",
    "        \n",
    "    \n",
    "        forecast = self._fitted.get_forecast(horizon, exog=exog_holiday)\n",
    "        mean_forecast = forecast.summary_frame()['mean'].to_numpy()\n",
    "        \n",
    "        if return_conf_int:\n",
    "            df = forecast.summary_frame(alpha=alpha)\n",
    "            pi = df[['mean_ci_lower', 'mean_ci_upper']].to_numpy()\n",
    "            return mean_forecast, pi\n",
    "            \n",
    "        \n",
    "        else:\n",
    "            return mean_forecast\n",
    "\n",
    "    fittedvalues = property(_get_preds)\n",
    "    resid = property(_get_resids)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExponentialSmoothingWrapper:\n",
    "    '''\n",
    "    Facade for statsmodels exponential smoothing models.  This wrapper\n",
    "    provides a common interface for all models and allow interop with\n",
    "    the custom time series cross validation code.\n",
    "    '''\n",
    "    def __init__(self, trend=False, damped_trend=False, seasonal=None):\n",
    "        self._trend = trend\n",
    "        self._seasonal= seasonal\n",
    "        self._damped_trend = damped_trend\n",
    "\n",
    "    def _get_resids(self):\n",
    "        return self._fitted.resid\n",
    "\n",
    "    def _get_preds(self):\n",
    "        return self._fitted.fittedvalues\n",
    "\n",
    "    def fit(self, train):\n",
    "        '''\n",
    "        Fit the model\n",
    "        \n",
    "        Parameters:\n",
    "        train: array-like\n",
    "            time series to fit.\n",
    "        '''\n",
    "        self._model = ExponentialSmoothing(endog=train,\n",
    "                                          trend=self._trend, \n",
    "                                          damped_trend=self._damped_trend,\n",
    "                                          seasonal=self._seasonal)\n",
    "        self._fitted = self._model.fit()\n",
    "        self._t = len(train)\n",
    "    \n",
    "    def predict(self, horizon, return_conf_int=False, alpha=0.2):\n",
    "        '''\n",
    "        Forecast the time series from the final point in the fitted series.\n",
    "        \n",
    "        Parameters:\n",
    "        ----------\n",
    "        \n",
    "        horizon: int\n",
    "            steps ahead to forecast \n",
    "            \n",
    "        return_conf_int: bool, optional (default=False)\n",
    "            Return prediction interval?  \n",
    "            \n",
    "        alpha: float\n",
    "            Used if return_conf_int=True. 100(1-alpha) interval.\n",
    "        '''\n",
    "        \n",
    "        forecast = self._fitted.get_forecast(horizon)\n",
    "        \n",
    "        mean_forecast = forecast.summary_frame()['mean'].to_numpy()\n",
    "        \n",
    "        if return_conf_int:\n",
    "            df = forecast.summary_frame(alpha=alpha)\n",
    "            pi = df[['mean_ci_lower', 'mean_ci_upper']].to_numpy()\n",
    "            return mean_forecast, pi        \n",
    "        else:\n",
    "            return mean_forecast\n",
    "\n",
    "    fittedvalues = property(_get_preds)\n",
    "    resid = property(_get_resids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example of fitting the ensemble\n",
    "1. Regression with New Year Holiday and Auto ARIMA errors\n",
    "2. FBProphet with new years day holiday.\n",
    "3. Holt-Winters Exponential Smoothing\n",
    "\n",
    "The code below demonstrates how to fit the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1 = ARIMAWrapper(order=(1,1,3), seasonal_order=(1,0,1,7), \n",
    "                       training_index=train.index,\n",
    "                       holidays=new_year['ds'].tolist())\n",
    "\n",
    "model_2 = FbProphetWrapper(training_index=train.index, \n",
    "                           holidays=new_year)\n",
    "\n",
    "model_3 = ExponentialSmoothingWrapper(trend=True, damped_trend=True, \n",
    "                                      seasonal=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimators = {'arima': model_1, 'fbp': model_2, 'hw': model_3}\n",
    "ens = Ensemble(estimators, UnweightedVote())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit to training data in chosen region\n",
    "ens.fit(train[REGION])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict 7 days ahead\n",
    "H = 7\n",
    "ens_preds = ens.predict(horizon=H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2275.15048284, 2259.43983053, 2157.54865967, 2088.51607366,\n",
       "       2081.27992798, 2093.40228599, 2137.83112478])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#view predictions\n",
    "ens_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with prediction intervals\n",
    "ens_preds, pi = ens.predict(horizon=H, return_conf_int=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2275.15048284, 2259.43983053, 2157.54865967, 2088.51607366,\n",
       "       2081.27992798, 2093.40228599, 2137.83112478])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ens_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2177.77495698, 2373.50993744],\n",
       "       [2159.5652313 , 2362.34867649],\n",
       "       [2053.35824724, 2264.04003533],\n",
       "       [1980.218854  , 2195.88773709],\n",
       "       [1972.47693801, 2189.49170262],\n",
       "       [1982.14001586, 2203.39939369],\n",
       "       [2023.84366814, 2254.55811812]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross validation functions\n",
    "\n",
    "`time_series_cv` implements rolling forecast origin cross validation for time series.  \n",
    "It does not calculate forecast error, but instead returns the predictions, pred intervals and actuals in an array that can be passed to any forecast error function. (this is for efficiency and allows additional metrics to be calculated if needed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_series_cv(model, train, val, horizons, alpha=0.2, step=1):\n",
    "    '''\n",
    "    Time series cross validation across multiple horizons for a single model.\n",
    "\n",
    "    Incrementally adds additional training data to the model and tests\n",
    "    across a provided list of forecast horizons. Note that function tests a\n",
    "    model only against complete validation sets.  E.g. if horizon = 15 and \n",
    "    len(val) = 12 then no testing is done.  In the case of multiple horizons\n",
    "    e.g. [7, 14, 28] then the function will use the maximum forecast horizon\n",
    "    to calculate the number of iterations i.e if len(val) = 365 and step = 1\n",
    "    then no. iterations = len(val) - max(horizon) = 365 - 28 = 337.\n",
    "    \n",
    "    Parameters:\n",
    "    --------\n",
    "    model - forecasting model\n",
    "\n",
    "    train - np.array - vector of training data\n",
    "\n",
    "    val - np.array - vector of validation data\n",
    "\n",
    "    horizon - list of ints, forecast horizon e.g. [7, 14, 28] days\n",
    "    \n",
    "    alpha - float, optional (default=0.2)\n",
    "        1 - alpha prediction interval specification\n",
    "\n",
    "    step -- int, optional (default=1)\n",
    "            step taken in cross validation \n",
    "            e.g. 1 in next cross validation training data includes next point \n",
    "            from the validation set.\n",
    "            e.g. 7 in the next cross validation training data includes next 7 points\n",
    "            (default=1)\n",
    "            \n",
    "    Returns:\n",
    "    -------\n",
    "    np.array, np.array, np.array\n",
    "        - cv_preds, cv_test, cv_intervals\n",
    "    '''\n",
    "    \n",
    "    #point forecasts\n",
    "    cv_preds = [] \n",
    "    #ground truth observations\n",
    "    cv_actuals = [] \n",
    "    #prediction intervals\n",
    "    cv_pis = []\n",
    "    \n",
    "    split = 0\n",
    "\n",
    "    print('split => ', end=\"\")\n",
    "    for i in range(0, len(val) - max(horizons) + 1, step):\n",
    "        split += 1\n",
    "        print(f'{split}, ', end=\"\")\n",
    "                \n",
    "        train_cv = np.concatenate([train, val[:i]], axis=0)\n",
    "        model.fit(train_cv)\n",
    "        \n",
    "        #predict the maximum horizon \n",
    "        preds, pis = model.predict(horizon=len(val[i:i+max(horizons)]), \n",
    "                                   return_conf_int=True,\n",
    "                                   alpha=alpha)        \n",
    "        cv_h_preds = []\n",
    "        cv_test = []\n",
    "        cv_h_pis = []\n",
    "        \n",
    "        #sub horizon calculations\n",
    "        for h in horizons:\n",
    "            #store the h-step prediction\n",
    "            cv_h_preds.append(preds[:h])\n",
    "            #store the h-step actual value\n",
    "            cv_test.append(val.iloc[i:i+h])    \n",
    "            cv_h_pis.append(pis[:h])\n",
    "                     \n",
    "        cv_preds.append(cv_h_preds)\n",
    "        cv_actuals.append(cv_test)\n",
    "        cv_pis.append(cv_h_pis)\n",
    "        \n",
    "    print('done.\\n')        \n",
    "    return cv_preds, cv_actuals, cv_pis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom functions for calculating CV scores for point predictions and coverage.\n",
    "\n",
    "These functions have been written to work with the output of `time_series_cv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_cv_error(cv_preds, cv_test, error_func):\n",
    "    '''\n",
    "    Forecast error in the current split\n",
    "    \n",
    "    Params:\n",
    "    -----\n",
    "    cv_preds, np.array\n",
    "        Split predictions\n",
    "        \n",
    "    \n",
    "    cv_test: np.array\n",
    "        acutal ground truth observations\n",
    "        \n",
    "    error_func: object\n",
    "        function with signature (y_true, y_preds)\n",
    "        \n",
    "    Returns:\n",
    "    -------\n",
    "        np.ndarray\n",
    "            cross validation errors for split\n",
    "    '''\n",
    "    n_splits = len(cv_preds)\n",
    "    cv_errors = []\n",
    "    \n",
    "    for split in range(n_splits):\n",
    "        pred_error = error_func(cv_test[split], cv_preds[split])\n",
    "        cv_errors.append(pred_error)\n",
    "        \n",
    "    return np.array(cv_errors)\n",
    "\n",
    "def forecast_errors_cv(cv_preds, cv_test, error_func):\n",
    "    '''\n",
    "    Forecast errors by forecast horizon\n",
    "    \n",
    "    Params:\n",
    "    ------\n",
    "    cv_preds: np.ndarray\n",
    "        Array of arrays.  Each array is of size h representing\n",
    "        the forecast horizon specified.\n",
    "        \n",
    "    cv_test: np.ndarray\n",
    "        Array of arrays.  Each array is of size h representing\n",
    "        the forecast horizon specified.\n",
    "        \n",
    "    error_func: object\n",
    "        function with signature (y_true, y_preds)\n",
    "        \n",
    "    Returns:\n",
    "    -------\n",
    "    np.ndarray\n",
    "        \n",
    "    '''\n",
    "    cv_test = np.array(cv_test)\n",
    "    cv_preds = np.array(cv_preds)\n",
    "    n_horizons = len(cv_test)    \n",
    "    \n",
    "    horizon_errors = []\n",
    "    for h in range(n_horizons):\n",
    "        split_errors = split_cv_error(cv_preds[h], cv_test[h], error_func)\n",
    "        horizon_errors.append(split_errors)\n",
    "\n",
    "    return np.array(horizon_errors)\n",
    "\n",
    "def split_coverage(cv_test, cv_intervals):\n",
    "    n_splits = len(cv_test)\n",
    "    cv_errors = []\n",
    "        \n",
    "    for split in range(n_splits):\n",
    "        val = np.asarray(cv_test[split])\n",
    "        lower = cv_intervals[split].T[0]\n",
    "        upper = cv_intervals[split].T[1]\n",
    "        \n",
    "        coverage = len(np.where((val > lower) & (val < upper))[0])\n",
    "        coverage = coverage / len(val)\n",
    "        \n",
    "        cv_errors.append(coverage)\n",
    "        \n",
    "    return np.array(cv_errors)\n",
    "    \n",
    "    \n",
    "def prediction_int_coverage_cv(cv_test, cv_intervals):\n",
    "    cv_test = np.array(cv_test)\n",
    "    cv_intervals = np.array(cv_intervals)\n",
    "    n_horizons = len(cv_test)    \n",
    "    \n",
    "    horizon_coverage = []\n",
    "    for h in range(n_horizons):\n",
    "        split_coverages = split_coverage(cv_test[h], cv_intervals[h])\n",
    "        horizon_coverage.append(split_coverages)\n",
    "\n",
    "    return np.array(horizon_coverage)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_cv_error_scaled(cv_preds, cv_test, y_train):\n",
    "    n_splits = len(cv_preds)\n",
    "    cv_errors = []\n",
    "    \n",
    "    for split in range(n_splits):\n",
    "        pred_error = mean_absolute_scaled_error(cv_test[split], cv_preds[split], \n",
    "                                                y_train, period=7)\n",
    "        \n",
    "        cv_errors.append(pred_error)\n",
    "        \n",
    "    return np.array(cv_errors)\n",
    "\n",
    "def forecast_errors_cv_scaled(cv_preds, cv_test, y_train):\n",
    "    cv_test = np.array(cv_test)\n",
    "    cv_preds = np.array(cv_preds)\n",
    "    n_horizons = len(cv_test)    \n",
    "    \n",
    "    horizon_errors = []\n",
    "    for h in range(n_horizons):\n",
    "        split_errors = split_cv_error_scaled(cv_preds[h], cv_test[h], y_train)\n",
    "        horizon_errors.append(split_errors)\n",
    "        \n",
    "    return np.array(horizon_errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ensemble(fb_interval=0.8):\n",
    "    '''\n",
    "    Create and return ensemble model\n",
    "    '''\n",
    "    model_1 = ARIMAWrapper(order=(1,1,3), seasonal_order=(1,0,1,7), \n",
    "                           training_index=train.index,\n",
    "                           holidays=new_year['ds'].tolist())\n",
    "\n",
    "    model_2 = FbProphetWrapper(training_index=train.index, \n",
    "                               holidays=new_year)\n",
    "\n",
    "    model_3 = ExponentialSmoothingWrapper(trend=True, damped_trend=True, \n",
    "                                          seasonal=7)\n",
    "    \n",
    "    estimators = {'arima': model_1, 'fbp': model_2, 'hw': model_3}\n",
    "    return Ensemble(estimators, UnweightedVote())\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run cross validation\n",
    "\n",
    "This is run twices once each for 80 and 95% prediction intervals.  The 2nd run is required due to the way Prophet generates prediction intervals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "split => 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, done.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "horizons = [7, 14, 21, 28, 35, 42, 49, 56, 63, 70, 77, 84, 365]\n",
    "model = get_ensemble()\n",
    "\n",
    "results = time_series_cv(model, train[REGION], val[REGION], horizons, \n",
    "                         alpha=0.2, step=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# symmetric MAPE results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>7</th>\n",
       "      <th>14</th>\n",
       "      <th>21</th>\n",
       "      <th>28</th>\n",
       "      <th>35</th>\n",
       "      <th>42</th>\n",
       "      <th>49</th>\n",
       "      <th>56</th>\n",
       "      <th>63</th>\n",
       "      <th>70</th>\n",
       "      <th>77</th>\n",
       "      <th>84</th>\n",
       "      <th>365</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2.846749</td>\n",
       "      <td>3.092036</td>\n",
       "      <td>3.182474</td>\n",
       "      <td>3.240986</td>\n",
       "      <td>3.332646</td>\n",
       "      <td>3.423800</td>\n",
       "      <td>3.494352</td>\n",
       "      <td>3.565179</td>\n",
       "      <td>3.647479</td>\n",
       "      <td>3.698958</td>\n",
       "      <td>3.747922</td>\n",
       "      <td>3.819224</td>\n",
       "      <td>4.754720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.163151</td>\n",
       "      <td>1.165531</td>\n",
       "      <td>0.999269</td>\n",
       "      <td>0.803257</td>\n",
       "      <td>0.749498</td>\n",
       "      <td>0.735622</td>\n",
       "      <td>0.724623</td>\n",
       "      <td>0.677578</td>\n",
       "      <td>0.677669</td>\n",
       "      <td>0.614217</td>\n",
       "      <td>0.563291</td>\n",
       "      <td>0.619997</td>\n",
       "      <td>1.319263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.117553</td>\n",
       "      <td>1.568871</td>\n",
       "      <td>2.031296</td>\n",
       "      <td>2.255147</td>\n",
       "      <td>2.462321</td>\n",
       "      <td>2.390131</td>\n",
       "      <td>2.464135</td>\n",
       "      <td>2.618192</td>\n",
       "      <td>2.736904</td>\n",
       "      <td>2.885545</td>\n",
       "      <td>2.930879</td>\n",
       "      <td>3.109268</td>\n",
       "      <td>3.257040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2.010473</td>\n",
       "      <td>2.301689</td>\n",
       "      <td>2.468120</td>\n",
       "      <td>2.659297</td>\n",
       "      <td>2.766973</td>\n",
       "      <td>2.881795</td>\n",
       "      <td>3.024150</td>\n",
       "      <td>3.075994</td>\n",
       "      <td>3.275051</td>\n",
       "      <td>3.327710</td>\n",
       "      <td>3.288896</td>\n",
       "      <td>3.323892</td>\n",
       "      <td>3.581371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2.498619</td>\n",
       "      <td>2.753276</td>\n",
       "      <td>2.822169</td>\n",
       "      <td>2.994488</td>\n",
       "      <td>3.259336</td>\n",
       "      <td>3.344553</td>\n",
       "      <td>3.472338</td>\n",
       "      <td>3.494913</td>\n",
       "      <td>3.623389</td>\n",
       "      <td>3.541138</td>\n",
       "      <td>3.640023</td>\n",
       "      <td>3.690660</td>\n",
       "      <td>4.754310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3.645799</td>\n",
       "      <td>3.818730</td>\n",
       "      <td>3.515286</td>\n",
       "      <td>3.633556</td>\n",
       "      <td>3.776292</td>\n",
       "      <td>3.832156</td>\n",
       "      <td>3.854289</td>\n",
       "      <td>3.844016</td>\n",
       "      <td>3.834770</td>\n",
       "      <td>4.057190</td>\n",
       "      <td>4.069152</td>\n",
       "      <td>4.101410</td>\n",
       "      <td>5.385557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>5.885374</td>\n",
       "      <td>6.085828</td>\n",
       "      <td>5.951263</td>\n",
       "      <td>5.397920</td>\n",
       "      <td>5.731698</td>\n",
       "      <td>5.805834</td>\n",
       "      <td>5.805863</td>\n",
       "      <td>5.517743</td>\n",
       "      <td>5.796322</td>\n",
       "      <td>5.595239</td>\n",
       "      <td>5.326817</td>\n",
       "      <td>5.705193</td>\n",
       "      <td>9.464877</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             7          14         21         28         35         42   \\\n",
       "count  27.000000  27.000000  27.000000  27.000000  27.000000  27.000000   \n",
       "mean    2.846749   3.092036   3.182474   3.240986   3.332646   3.423800   \n",
       "std     1.163151   1.165531   0.999269   0.803257   0.749498   0.735622   \n",
       "min     1.117553   1.568871   2.031296   2.255147   2.462321   2.390131   \n",
       "25%     2.010473   2.301689   2.468120   2.659297   2.766973   2.881795   \n",
       "50%     2.498619   2.753276   2.822169   2.994488   3.259336   3.344553   \n",
       "75%     3.645799   3.818730   3.515286   3.633556   3.776292   3.832156   \n",
       "max     5.885374   6.085828   5.951263   5.397920   5.731698   5.805834   \n",
       "\n",
       "             49         56         63         70         77         84   \\\n",
       "count  27.000000  27.000000  27.000000  27.000000  27.000000  27.000000   \n",
       "mean    3.494352   3.565179   3.647479   3.698958   3.747922   3.819224   \n",
       "std     0.724623   0.677578   0.677669   0.614217   0.563291   0.619997   \n",
       "min     2.464135   2.618192   2.736904   2.885545   2.930879   3.109268   \n",
       "25%     3.024150   3.075994   3.275051   3.327710   3.288896   3.323892   \n",
       "50%     3.472338   3.494913   3.623389   3.541138   3.640023   3.690660   \n",
       "75%     3.854289   3.844016   3.834770   4.057190   4.069152   4.101410   \n",
       "max     5.805863   5.517743   5.796322   5.595239   5.326817   5.705193   \n",
       "\n",
       "             365  \n",
       "count  27.000000  \n",
       "mean    4.754720  \n",
       "std     1.319263  \n",
       "min     3.257040  \n",
       "25%     3.581371  \n",
       "50%     4.754310  \n",
       "75%     5.385557  \n",
       "max     9.464877  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_preds, cv_test, cv_intervals = results\n",
    "#CV point predictions smape\n",
    "cv_errors = forecast_errors_cv(cv_preds, cv_test, \n",
    "                               symmetric_mean_absolute_percentage_error)\n",
    "df = pd.DataFrame(cv_errors)\n",
    "df.columns = horizons\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../../results/model_selection/stage1/Trust-fbp-arima-hw_smape.csv\n"
     ]
    }
   ],
   "source": [
    "#output sMAPE results to file\n",
    "metric = 'smape'\n",
    "print(f'{TOP_LEVEL}/{STAGE}/{REGION}-{METHOD}_{metric}.csv')\n",
    "df.to_csv(f'{TOP_LEVEL}/{STAGE}/{REGION}-{METHOD}_{metric}.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RMSE results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>7</th>\n",
       "      <th>14</th>\n",
       "      <th>21</th>\n",
       "      <th>28</th>\n",
       "      <th>35</th>\n",
       "      <th>42</th>\n",
       "      <th>49</th>\n",
       "      <th>56</th>\n",
       "      <th>63</th>\n",
       "      <th>70</th>\n",
       "      <th>77</th>\n",
       "      <th>84</th>\n",
       "      <th>365</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>75.587461</td>\n",
       "      <td>83.436523</td>\n",
       "      <td>88.014581</td>\n",
       "      <td>90.990705</td>\n",
       "      <td>94.457224</td>\n",
       "      <td>97.565954</td>\n",
       "      <td>100.102980</td>\n",
       "      <td>102.425431</td>\n",
       "      <td>105.039578</td>\n",
       "      <td>106.965926</td>\n",
       "      <td>108.723014</td>\n",
       "      <td>111.112421</td>\n",
       "      <td>133.053109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>32.435023</td>\n",
       "      <td>34.347057</td>\n",
       "      <td>31.603350</td>\n",
       "      <td>28.153752</td>\n",
       "      <td>26.640948</td>\n",
       "      <td>25.537461</td>\n",
       "      <td>24.727842</td>\n",
       "      <td>23.645624</td>\n",
       "      <td>22.699520</td>\n",
       "      <td>20.362138</td>\n",
       "      <td>18.206387</td>\n",
       "      <td>18.282286</td>\n",
       "      <td>29.931394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>34.088450</td>\n",
       "      <td>41.602797</td>\n",
       "      <td>56.882113</td>\n",
       "      <td>61.578124</td>\n",
       "      <td>68.218256</td>\n",
       "      <td>66.396401</td>\n",
       "      <td>67.429169</td>\n",
       "      <td>72.264433</td>\n",
       "      <td>74.560376</td>\n",
       "      <td>78.014230</td>\n",
       "      <td>79.384545</td>\n",
       "      <td>84.432581</td>\n",
       "      <td>99.971306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>52.404550</td>\n",
       "      <td>61.174895</td>\n",
       "      <td>68.334607</td>\n",
       "      <td>70.889239</td>\n",
       "      <td>74.750661</td>\n",
       "      <td>77.939530</td>\n",
       "      <td>78.826740</td>\n",
       "      <td>81.638667</td>\n",
       "      <td>86.703982</td>\n",
       "      <td>91.908781</td>\n",
       "      <td>95.446425</td>\n",
       "      <td>99.162395</td>\n",
       "      <td>105.692423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>68.193704</td>\n",
       "      <td>75.763037</td>\n",
       "      <td>77.083683</td>\n",
       "      <td>81.510236</td>\n",
       "      <td>83.738065</td>\n",
       "      <td>89.611431</td>\n",
       "      <td>93.119825</td>\n",
       "      <td>99.448376</td>\n",
       "      <td>102.504369</td>\n",
       "      <td>108.059575</td>\n",
       "      <td>110.213369</td>\n",
       "      <td>111.242033</td>\n",
       "      <td>134.235836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>89.151301</td>\n",
       "      <td>96.876296</td>\n",
       "      <td>93.049811</td>\n",
       "      <td>99.103203</td>\n",
       "      <td>106.334244</td>\n",
       "      <td>110.594108</td>\n",
       "      <td>121.285803</td>\n",
       "      <td>121.155951</td>\n",
       "      <td>117.404853</td>\n",
       "      <td>117.936319</td>\n",
       "      <td>117.721667</td>\n",
       "      <td>124.416955</td>\n",
       "      <td>147.414788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>177.606476</td>\n",
       "      <td>191.264458</td>\n",
       "      <td>166.117575</td>\n",
       "      <td>152.960289</td>\n",
       "      <td>154.517114</td>\n",
       "      <td>153.920568</td>\n",
       "      <td>153.359584</td>\n",
       "      <td>152.769632</td>\n",
       "      <td>154.490469</td>\n",
       "      <td>148.168573</td>\n",
       "      <td>143.496467</td>\n",
       "      <td>154.755083</td>\n",
       "      <td>241.230925</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              7           14          21          28          35          42   \\\n",
       "count   27.000000   27.000000   27.000000   27.000000   27.000000   27.000000   \n",
       "mean    75.587461   83.436523   88.014581   90.990705   94.457224   97.565954   \n",
       "std     32.435023   34.347057   31.603350   28.153752   26.640948   25.537461   \n",
       "min     34.088450   41.602797   56.882113   61.578124   68.218256   66.396401   \n",
       "25%     52.404550   61.174895   68.334607   70.889239   74.750661   77.939530   \n",
       "50%     68.193704   75.763037   77.083683   81.510236   83.738065   89.611431   \n",
       "75%     89.151301   96.876296   93.049811   99.103203  106.334244  110.594108   \n",
       "max    177.606476  191.264458  166.117575  152.960289  154.517114  153.920568   \n",
       "\n",
       "              49          56          63          70          77          84   \\\n",
       "count   27.000000   27.000000   27.000000   27.000000   27.000000   27.000000   \n",
       "mean   100.102980  102.425431  105.039578  106.965926  108.723014  111.112421   \n",
       "std     24.727842   23.645624   22.699520   20.362138   18.206387   18.282286   \n",
       "min     67.429169   72.264433   74.560376   78.014230   79.384545   84.432581   \n",
       "25%     78.826740   81.638667   86.703982   91.908781   95.446425   99.162395   \n",
       "50%     93.119825   99.448376  102.504369  108.059575  110.213369  111.242033   \n",
       "75%    121.285803  121.155951  117.404853  117.936319  117.721667  124.416955   \n",
       "max    153.359584  152.769632  154.490469  148.168573  143.496467  154.755083   \n",
       "\n",
       "              365  \n",
       "count   27.000000  \n",
       "mean   133.053109  \n",
       "std     29.931394  \n",
       "min     99.971306  \n",
       "25%    105.692423  \n",
       "50%    134.235836  \n",
       "75%    147.414788  \n",
       "max    241.230925  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#CV point predictions rmse\n",
    "cv_errors = forecast_errors_cv(cv_preds, cv_test, root_mean_squared_error)\n",
    "df = pd.DataFrame(cv_errors)\n",
    "df.columns = horizons\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../../results/model_selection/stage1/Trust-fbp-arima-hw_rmse.csv\n"
     ]
    }
   ],
   "source": [
    "#output RMSE to file\n",
    "metric = 'rmse'\n",
    "print(f'{TOP_LEVEL}/{STAGE}/{REGION}-{METHOD}_{metric}.csv')\n",
    "df.to_csv(f'{TOP_LEVEL}/{STAGE}/{REGION}-{METHOD}_{metric}.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mean Absolute Scaled Error (MASE)\n",
    "\n",
    "Scaled by one-step insample Seasonal Naive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>7</th>\n",
       "      <th>14</th>\n",
       "      <th>21</th>\n",
       "      <th>28</th>\n",
       "      <th>35</th>\n",
       "      <th>42</th>\n",
       "      <th>49</th>\n",
       "      <th>56</th>\n",
       "      <th>63</th>\n",
       "      <th>70</th>\n",
       "      <th>77</th>\n",
       "      <th>84</th>\n",
       "      <th>365</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.769247</td>\n",
       "      <td>0.837194</td>\n",
       "      <td>0.863188</td>\n",
       "      <td>0.880267</td>\n",
       "      <td>0.906248</td>\n",
       "      <td>0.932128</td>\n",
       "      <td>0.952295</td>\n",
       "      <td>0.972063</td>\n",
       "      <td>0.994965</td>\n",
       "      <td>1.009526</td>\n",
       "      <td>1.023438</td>\n",
       "      <td>1.043465</td>\n",
       "      <td>1.291173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.345692</td>\n",
       "      <td>0.348243</td>\n",
       "      <td>0.302069</td>\n",
       "      <td>0.249692</td>\n",
       "      <td>0.234744</td>\n",
       "      <td>0.229569</td>\n",
       "      <td>0.224344</td>\n",
       "      <td>0.210009</td>\n",
       "      <td>0.206947</td>\n",
       "      <td>0.186305</td>\n",
       "      <td>0.167863</td>\n",
       "      <td>0.178943</td>\n",
       "      <td>0.370061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.320987</td>\n",
       "      <td>0.448615</td>\n",
       "      <td>0.570463</td>\n",
       "      <td>0.606842</td>\n",
       "      <td>0.665210</td>\n",
       "      <td>0.641020</td>\n",
       "      <td>0.661823</td>\n",
       "      <td>0.684272</td>\n",
       "      <td>0.736487</td>\n",
       "      <td>0.779424</td>\n",
       "      <td>0.757287</td>\n",
       "      <td>0.809023</td>\n",
       "      <td>0.887580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.515734</td>\n",
       "      <td>0.615694</td>\n",
       "      <td>0.676661</td>\n",
       "      <td>0.709066</td>\n",
       "      <td>0.716072</td>\n",
       "      <td>0.764096</td>\n",
       "      <td>0.783489</td>\n",
       "      <td>0.810081</td>\n",
       "      <td>0.863090</td>\n",
       "      <td>0.882178</td>\n",
       "      <td>0.909459</td>\n",
       "      <td>0.916232</td>\n",
       "      <td>0.973956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.679974</td>\n",
       "      <td>0.736956</td>\n",
       "      <td>0.758836</td>\n",
       "      <td>0.803499</td>\n",
       "      <td>0.869166</td>\n",
       "      <td>0.869367</td>\n",
       "      <td>0.935549</td>\n",
       "      <td>0.962928</td>\n",
       "      <td>0.979883</td>\n",
       "      <td>0.971366</td>\n",
       "      <td>1.004169</td>\n",
       "      <td>1.014351</td>\n",
       "      <td>1.279971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.937593</td>\n",
       "      <td>1.013996</td>\n",
       "      <td>0.928592</td>\n",
       "      <td>0.945984</td>\n",
       "      <td>1.023141</td>\n",
       "      <td>1.083264</td>\n",
       "      <td>1.102934</td>\n",
       "      <td>1.103623</td>\n",
       "      <td>1.056879</td>\n",
       "      <td>1.079097</td>\n",
       "      <td>1.113680</td>\n",
       "      <td>1.129877</td>\n",
       "      <td>1.442016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.818729</td>\n",
       "      <td>1.793468</td>\n",
       "      <td>1.720280</td>\n",
       "      <td>1.555148</td>\n",
       "      <td>1.638749</td>\n",
       "      <td>1.655502</td>\n",
       "      <td>1.650945</td>\n",
       "      <td>1.567235</td>\n",
       "      <td>1.640177</td>\n",
       "      <td>1.584331</td>\n",
       "      <td>1.509627</td>\n",
       "      <td>1.613054</td>\n",
       "      <td>2.657950</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             7          14         21         28         35         42   \\\n",
       "count  27.000000  27.000000  27.000000  27.000000  27.000000  27.000000   \n",
       "mean    0.769247   0.837194   0.863188   0.880267   0.906248   0.932128   \n",
       "std     0.345692   0.348243   0.302069   0.249692   0.234744   0.229569   \n",
       "min     0.320987   0.448615   0.570463   0.606842   0.665210   0.641020   \n",
       "25%     0.515734   0.615694   0.676661   0.709066   0.716072   0.764096   \n",
       "50%     0.679974   0.736956   0.758836   0.803499   0.869166   0.869367   \n",
       "75%     0.937593   1.013996   0.928592   0.945984   1.023141   1.083264   \n",
       "max     1.818729   1.793468   1.720280   1.555148   1.638749   1.655502   \n",
       "\n",
       "             49         56         63         70         77         84   \\\n",
       "count  27.000000  27.000000  27.000000  27.000000  27.000000  27.000000   \n",
       "mean    0.952295   0.972063   0.994965   1.009526   1.023438   1.043465   \n",
       "std     0.224344   0.210009   0.206947   0.186305   0.167863   0.178943   \n",
       "min     0.661823   0.684272   0.736487   0.779424   0.757287   0.809023   \n",
       "25%     0.783489   0.810081   0.863090   0.882178   0.909459   0.916232   \n",
       "50%     0.935549   0.962928   0.979883   0.971366   1.004169   1.014351   \n",
       "75%     1.102934   1.103623   1.056879   1.079097   1.113680   1.129877   \n",
       "max     1.650945   1.567235   1.640177   1.584331   1.509627   1.613054   \n",
       "\n",
       "             365  \n",
       "count  27.000000  \n",
       "mean    1.291173  \n",
       "std     0.370061  \n",
       "min     0.887580  \n",
       "25%     0.973956  \n",
       "50%     1.279971  \n",
       "75%     1.442016  \n",
       "max     2.657950  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#mase\n",
    "cv_errors = forecast_errors_cv_scaled(cv_preds, cv_test, train[REGION])\n",
    "df = pd.DataFrame(cv_errors)\n",
    "df.columns = horizons\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../../results/model_selection/stage1/Trust-fbp-arima-hw_mase.csv\n"
     ]
    }
   ],
   "source": [
    "#output mase to file.\n",
    "metric = 'mase'\n",
    "print(f'{TOP_LEVEL}/{STAGE}/{REGION}-{METHOD}_{metric}.csv')\n",
    "df.to_csv(f'{TOP_LEVEL}/{STAGE}/{REGION}-{METHOD}_{metric}.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 80% Prediction Interval Coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>7</th>\n",
       "      <th>14</th>\n",
       "      <th>21</th>\n",
       "      <th>28</th>\n",
       "      <th>35</th>\n",
       "      <th>42</th>\n",
       "      <th>49</th>\n",
       "      <th>56</th>\n",
       "      <th>63</th>\n",
       "      <th>70</th>\n",
       "      <th>77</th>\n",
       "      <th>84</th>\n",
       "      <th>365</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.820106</td>\n",
       "      <td>0.812169</td>\n",
       "      <td>0.821869</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.834921</td>\n",
       "      <td>0.836861</td>\n",
       "      <td>0.837491</td>\n",
       "      <td>0.843254</td>\n",
       "      <td>0.844209</td>\n",
       "      <td>0.849206</td>\n",
       "      <td>0.853295</td>\n",
       "      <td>0.854497</td>\n",
       "      <td>0.914054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.200441</td>\n",
       "      <td>0.190171</td>\n",
       "      <td>0.157155</td>\n",
       "      <td>0.129150</td>\n",
       "      <td>0.125127</td>\n",
       "      <td>0.122425</td>\n",
       "      <td>0.108209</td>\n",
       "      <td>0.095310</td>\n",
       "      <td>0.093910</td>\n",
       "      <td>0.085286</td>\n",
       "      <td>0.077069</td>\n",
       "      <td>0.081236</td>\n",
       "      <td>0.069672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.380952</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.492063</td>\n",
       "      <td>0.542857</td>\n",
       "      <td>0.584416</td>\n",
       "      <td>0.559524</td>\n",
       "      <td>0.621918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>0.767857</td>\n",
       "      <td>0.742857</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>0.795918</td>\n",
       "      <td>0.812500</td>\n",
       "      <td>0.817460</td>\n",
       "      <td>0.821429</td>\n",
       "      <td>0.837662</td>\n",
       "      <td>0.845238</td>\n",
       "      <td>0.897260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.892857</td>\n",
       "      <td>0.885714</td>\n",
       "      <td>0.880952</td>\n",
       "      <td>0.836735</td>\n",
       "      <td>0.839286</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.870130</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.939726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>0.904762</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>0.914286</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.897959</td>\n",
       "      <td>0.910714</td>\n",
       "      <td>0.896825</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.896104</td>\n",
       "      <td>0.898810</td>\n",
       "      <td>0.956164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.952381</td>\n",
       "      <td>0.959184</td>\n",
       "      <td>0.964286</td>\n",
       "      <td>0.968254</td>\n",
       "      <td>0.971429</td>\n",
       "      <td>0.974026</td>\n",
       "      <td>0.976190</td>\n",
       "      <td>0.969863</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             7          14         21         28         35         42   \\\n",
       "count  27.000000  27.000000  27.000000  27.000000  27.000000  27.000000   \n",
       "mean    0.820106   0.812169   0.821869   0.833333   0.834921   0.836861   \n",
       "std     0.200441   0.190171   0.157155   0.129150   0.125127   0.122425   \n",
       "min     0.428571   0.285714   0.333333   0.428571   0.400000   0.380952   \n",
       "25%     0.714286   0.750000   0.785714   0.767857   0.742857   0.785714   \n",
       "50%     0.857143   0.857143   0.857143   0.892857   0.885714   0.880952   \n",
       "75%     1.000000   0.928571   0.904762   0.928571   0.914286   0.916667   \n",
       "max     1.000000   1.000000   1.000000   1.000000   1.000000   0.952381   \n",
       "\n",
       "             49         56         63         70         77         84   \\\n",
       "count  27.000000  27.000000  27.000000  27.000000  27.000000  27.000000   \n",
       "mean    0.837491   0.843254   0.844209   0.849206   0.853295   0.854497   \n",
       "std     0.108209   0.095310   0.093910   0.085286   0.077069   0.081236   \n",
       "min     0.428571   0.500000   0.492063   0.542857   0.584416   0.559524   \n",
       "25%     0.795918   0.812500   0.817460   0.821429   0.837662   0.845238   \n",
       "50%     0.836735   0.839286   0.857143   0.857143   0.870130   0.857143   \n",
       "75%     0.897959   0.910714   0.896825   0.900000   0.896104   0.898810   \n",
       "max     0.959184   0.964286   0.968254   0.971429   0.974026   0.976190   \n",
       "\n",
       "             365  \n",
       "count  27.000000  \n",
       "mean    0.914054  \n",
       "std     0.069672  \n",
       "min     0.621918  \n",
       "25%     0.897260  \n",
       "50%     0.939726  \n",
       "75%     0.956164  \n",
       "max     0.969863  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#80% PIs\n",
    "cv_coverage = prediction_int_coverage_cv(cv_test, cv_intervals)\n",
    "df = pd.DataFrame(cv_coverage)\n",
    "df.columns = horizons\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../../results/model_selection/stage1/Trust-fbp-arima-hw_coverage_80.csv\n"
     ]
    }
   ],
   "source": [
    "#write 80% coverage to file\n",
    "metric = 'coverage_80'\n",
    "print(f'{TOP_LEVEL}/{STAGE}/{REGION}-{METHOD}_{metric}.csv')\n",
    "df.to_csv(f'{TOP_LEVEL}/{STAGE}/{REGION}-{METHOD}_{metric}.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 95% Prediction Interval Coverage\n",
    "\n",
    "Rerun analysis and obtain 95% Prediction intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "split => 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, done.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "horizons = [7, 14, 21, 28, 35, 42, 49, 56, 63, 70, 77, 84, 365]\n",
    "model = get_ensemble(fb_interval=0.95)\n",
    "\n",
    "results = time_series_cv(model, train[REGION], val[REGION], horizons, \n",
    "                         alpha=0.05, step=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>7</th>\n",
       "      <th>14</th>\n",
       "      <th>21</th>\n",
       "      <th>28</th>\n",
       "      <th>35</th>\n",
       "      <th>42</th>\n",
       "      <th>49</th>\n",
       "      <th>56</th>\n",
       "      <th>63</th>\n",
       "      <th>70</th>\n",
       "      <th>77</th>\n",
       "      <th>84</th>\n",
       "      <th>365</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.936508</td>\n",
       "      <td>0.920635</td>\n",
       "      <td>0.929453</td>\n",
       "      <td>0.937831</td>\n",
       "      <td>0.940741</td>\n",
       "      <td>0.944444</td>\n",
       "      <td>0.945578</td>\n",
       "      <td>0.947090</td>\n",
       "      <td>0.948854</td>\n",
       "      <td>0.950794</td>\n",
       "      <td>0.952381</td>\n",
       "      <td>0.952381</td>\n",
       "      <td>0.976966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.127365</td>\n",
       "      <td>0.137728</td>\n",
       "      <td>0.110001</td>\n",
       "      <td>0.090020</td>\n",
       "      <td>0.078816</td>\n",
       "      <td>0.069259</td>\n",
       "      <td>0.062775</td>\n",
       "      <td>0.056574</td>\n",
       "      <td>0.052522</td>\n",
       "      <td>0.046937</td>\n",
       "      <td>0.042466</td>\n",
       "      <td>0.041503</td>\n",
       "      <td>0.010966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.678571</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.761905</td>\n",
       "      <td>0.775510</td>\n",
       "      <td>0.803571</td>\n",
       "      <td>0.809524</td>\n",
       "      <td>0.828571</td>\n",
       "      <td>0.844156</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.939726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.892857</td>\n",
       "      <td>0.904762</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.892857</td>\n",
       "      <td>0.887755</td>\n",
       "      <td>0.892857</td>\n",
       "      <td>0.904762</td>\n",
       "      <td>0.914286</td>\n",
       "      <td>0.922078</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>0.976712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.964286</td>\n",
       "      <td>0.971429</td>\n",
       "      <td>0.976190</td>\n",
       "      <td>0.979592</td>\n",
       "      <td>0.964286</td>\n",
       "      <td>0.968254</td>\n",
       "      <td>0.971429</td>\n",
       "      <td>0.961039</td>\n",
       "      <td>0.940476</td>\n",
       "      <td>0.980822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.992857</td>\n",
       "      <td>0.993506</td>\n",
       "      <td>0.994048</td>\n",
       "      <td>0.982192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.986301</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             7          14         21         28         35         42   \\\n",
       "count  27.000000  27.000000  27.000000  27.000000  27.000000  27.000000   \n",
       "mean    0.936508   0.920635   0.929453   0.937831   0.940741   0.944444   \n",
       "std     0.127365   0.137728   0.110001   0.090020   0.078816   0.069259   \n",
       "min     0.428571   0.428571   0.571429   0.678571   0.714286   0.761905   \n",
       "25%     0.857143   0.892857   0.904762   0.928571   0.900000   0.892857   \n",
       "50%     1.000000   1.000000   1.000000   0.964286   0.971429   0.976190   \n",
       "75%     1.000000   1.000000   1.000000   1.000000   1.000000   1.000000   \n",
       "max     1.000000   1.000000   1.000000   1.000000   1.000000   1.000000   \n",
       "\n",
       "             49         56         63         70         77         84   \\\n",
       "count  27.000000  27.000000  27.000000  27.000000  27.000000  27.000000   \n",
       "mean    0.945578   0.947090   0.948854   0.950794   0.952381   0.952381   \n",
       "std     0.062775   0.056574   0.052522   0.046937   0.042466   0.041503   \n",
       "min     0.775510   0.803571   0.809524   0.828571   0.844156   0.833333   \n",
       "25%     0.887755   0.892857   0.904762   0.914286   0.922078   0.928571   \n",
       "50%     0.979592   0.964286   0.968254   0.971429   0.961039   0.940476   \n",
       "75%     1.000000   1.000000   1.000000   0.992857   0.993506   0.994048   \n",
       "max     1.000000   1.000000   1.000000   1.000000   1.000000   1.000000   \n",
       "\n",
       "             365  \n",
       "count  27.000000  \n",
       "mean    0.976966  \n",
       "std     0.010966  \n",
       "min     0.939726  \n",
       "25%     0.976712  \n",
       "50%     0.980822  \n",
       "75%     0.982192  \n",
       "max     0.986301  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#95% PIs\n",
    "cv_preds, cv_test, cv_intervals = results\n",
    "cv_coverage = prediction_int_coverage_cv(cv_test, cv_intervals)\n",
    "df = pd.DataFrame(cv_coverage)\n",
    "df.columns = horizons\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../../results/model_selection/stage1/Trust-fbp-arima-hw_coverage_95.csv\n"
     ]
    }
   ],
   "source": [
    "#write 95% coverage to file\n",
    "metric = 'coverage_95'\n",
    "print(f'{TOP_LEVEL}/{STAGE}/{REGION}-{METHOD}_{metric}.csv')\n",
    "df.to_csv(f'{TOP_LEVEL}/{STAGE}/{REGION}-{METHOD}_{metric}.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ambo",
   "language": "python",
   "name": "ambo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
