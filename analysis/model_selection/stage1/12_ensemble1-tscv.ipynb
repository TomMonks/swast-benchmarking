{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross validation: An Ensemble of Regression with ARIMA errors and Holt-Winters\n",
    "---\n",
    "\n",
    "An unweighted ensemble of \n",
    "\n",
    "* Regression with new year holiday and (auto) ARIMA errors; and \n",
    "* Holt-Winters Exponential Smoothing (statespace formulation)\n",
    "\n",
    "The ARIMA process modelled is a (1,1,3)(1,0,1,7).  This was determined by auto_arima from the `pmdarima` package.\n",
    "\n",
    "Note that HW provides no way to easily incorporate special events.\n",
    "\n",
    "This notebook conducts cross validation of the method using a rolling forecast origin method.\n",
    "\n",
    "> The Reg with ARIMA error model is based on SWAST's data.  It may be prudent for each trust to fit their own automatic ARIMA model.\n",
    "\n",
    "**The notebook outputs:**\n",
    "* MASE, RMSE and MAPE at 7 day intervals from 7 to 84 days.\n",
    "* 80 and 95% prediction intervals between 7 and 84 days.\n",
    "\n",
    "These are saved into the folder `results/model_selection/stage1/`\n",
    "\n",
    "---\n",
    "\n",
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#error measures\n",
    "from forecast_tools.metrics import (mean_absolute_scaled_error, \n",
    "                                    root_mean_squared_error,\n",
    "                                    symmetric_mean_absolute_percentage_error)\n",
    "\n",
    "#models\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.tsa.statespace.exponential_smoothing import ExponentialSmoothing\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.11.0\n",
      "1.5.2\n"
     ]
    }
   ],
   "source": [
    "import statsmodels as sm\n",
    "import pmdarima \n",
    "\n",
    "print(sm.__version__)\n",
    "print(pmdarima.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to select exceptionally busy days as covariates.\n",
    "from amb_forecast.feature_engineering import (regular_busy_calender_days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#custom ensemble class\n",
    "from amb_forecast.ensemble import (Ensemble, UnweightedVote)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Input\n",
    "\n",
    "The constants `TOP_LEVEL`, `STAGE`, `REGION`,`TRUST` and `METHOD` are used to control data selection and the directory for outputting results.  \n",
    "\n",
    "> Output file is `f'{TOP_LEVEL}/{STAGE}/{REGION}-{METHOD}_{metric}.csv'.csv`.  where metric will be smape, rmse, mase, coverage_80 and coverage_95. Note: `REGION`: is also used to select the correct data from the input dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOP_LEVEL = '../../../results/model_selection'\n",
    "STAGE = 'stage1'\n",
    "REGION = 'Trust'\n",
    "METHOD = 'arima-hw'\n",
    "\n",
    "FILE_NAME = 'Daily_Responses_5_Years_2019_full.csv'\n",
    "\n",
    "#split training and test data.\n",
    "TEST_SPLIT_DATE = '2019-01-01'\n",
    "\n",
    "#second subdivide: train and val\n",
    "VAL_SPLIT_DATE = '2017-07-01'\n",
    "\n",
    "#discard data after 2020 due to coronavirus\n",
    "#this is the subject of a seperate study.\n",
    "DISCARD_DATE = '2020-01-01'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in path\n",
    "path = f'../../../data/{FILE_NAME}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process_daily_data(path, index_col, by_col, \n",
    "                           values, dayfirst=False):\n",
    "    '''\n",
    "    Daily data is stored in long format.  Read in \n",
    "    and pivot to wide format so that there is a single \n",
    "    colmumn for each regions time series.\n",
    "    '''\n",
    "    df = pd.read_csv(path, index_col=index_col, parse_dates=True, \n",
    "                     dayfirst=dayfirst)\n",
    "    df.columns = map(str.lower, df.columns)\n",
    "    df.index.rename(str(df.index.name).lower(), inplace=True)\n",
    "    \n",
    "    clean_table = pd.pivot_table(df, values=values.lower(), \n",
    "                                 index=[index_col.lower()],\n",
    "                                 columns=[by_col.lower()], aggfunc=np.sum)\n",
    "    \n",
    "    clean_table.index.freq = 'D'\n",
    "    \n",
    "    return clean_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>ora</th>\n",
       "      <th>BNSSG</th>\n",
       "      <th>Cornwall</th>\n",
       "      <th>Devon</th>\n",
       "      <th>Dorset</th>\n",
       "      <th>Gloucestershire</th>\n",
       "      <th>OOA</th>\n",
       "      <th>Somerset</th>\n",
       "      <th>Trust</th>\n",
       "      <th>Wiltshire</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>actual_dt</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2013-12-30</th>\n",
       "      <td>415.0</td>\n",
       "      <td>220.0</td>\n",
       "      <td>502.0</td>\n",
       "      <td>336.0</td>\n",
       "      <td>129.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>183.0</td>\n",
       "      <td>2042.0</td>\n",
       "      <td>255.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-12-31</th>\n",
       "      <td>420.0</td>\n",
       "      <td>236.0</td>\n",
       "      <td>468.0</td>\n",
       "      <td>302.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>180.0</td>\n",
       "      <td>1996.0</td>\n",
       "      <td>260.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-01-01</th>\n",
       "      <td>549.0</td>\n",
       "      <td>341.0</td>\n",
       "      <td>566.0</td>\n",
       "      <td>392.0</td>\n",
       "      <td>157.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>213.0</td>\n",
       "      <td>2570.0</td>\n",
       "      <td>351.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-01-02</th>\n",
       "      <td>450.0</td>\n",
       "      <td>218.0</td>\n",
       "      <td>499.0</td>\n",
       "      <td>301.0</td>\n",
       "      <td>115.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>167.0</td>\n",
       "      <td>2013.0</td>\n",
       "      <td>258.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-01-03</th>\n",
       "      <td>419.0</td>\n",
       "      <td>229.0</td>\n",
       "      <td>503.0</td>\n",
       "      <td>304.0</td>\n",
       "      <td>135.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>195.0</td>\n",
       "      <td>2056.0</td>\n",
       "      <td>269.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "ora         BNSSG  Cornwall  Devon  Dorset  Gloucestershire  OOA  Somerset  \\\n",
       "actual_dt                                                                    \n",
       "2013-12-30  415.0     220.0  502.0   336.0            129.0  NaN     183.0   \n",
       "2013-12-31  420.0     236.0  468.0   302.0            128.0  NaN     180.0   \n",
       "2014-01-01  549.0     341.0  566.0   392.0            157.0  NaN     213.0   \n",
       "2014-01-02  450.0     218.0  499.0   301.0            115.0  NaN     167.0   \n",
       "2014-01-03  419.0     229.0  503.0   304.0            135.0  NaN     195.0   \n",
       "\n",
       "ora          Trust  Wiltshire  \n",
       "actual_dt                      \n",
       "2013-12-30  2042.0      255.0  \n",
       "2013-12-31  1996.0      260.0  \n",
       "2014-01-01  2570.0      351.0  \n",
       "2014-01-02  2013.0      258.0  \n",
       "2014-01-03  2056.0      269.0  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean = pre_process_daily_data(path, 'Actual_dt', 'ORA', 'Actual_Value', \n",
    "                               dayfirst=False)\n",
    "clean.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ts_train_test_split(data, split_date):\n",
    "    '''\n",
    "    Split time series into training and test data\n",
    "    \n",
    "    Parameters:\n",
    "    -------\n",
    "    data - pd.DataFrame - time series data.  Index expected as datatimeindex\n",
    "    split_date - the date on which to split the time series\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple (len=2) \n",
    "    0. pandas.DataFrame - training dataset\n",
    "    1. pandas.DataFrame - test dataset\n",
    "    '''\n",
    "    train = data.loc[data.index < split_date]\n",
    "    test = data.loc[data.index >= split_date]\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = ts_train_test_split(clean, split_date=TEST_SPLIT_DATE)\n",
    "\n",
    "#exclude data after 2020 due to coronavirus.\n",
    "test, discard = ts_train_test_split(test, split_date=DISCARD_DATE)\n",
    "\n",
    "#train split into train and validation\n",
    "train, val = ts_train_test_split(train, split_date=VAL_SPLIT_DATE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1279, 9)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#amount of training data\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(549, 9)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#amount of validation data\n",
    "val.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New years day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "exceptional = regular_busy_calender_days(train[REGION], quantile=0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_year = pd.DataFrame({\n",
    "                         'holiday': 'new_year',\n",
    "                         'ds': pd.date_range(start=exceptional[0], \n",
    "                                             periods=20, \n",
    "                                             freq='YS')\n",
    "                        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>holiday</th>\n",
       "      <th>ds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>new_year</td>\n",
       "      <td>2013-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>new_year</td>\n",
       "      <td>2014-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>new_year</td>\n",
       "      <td>2015-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>new_year</td>\n",
       "      <td>2016-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>new_year</td>\n",
       "      <td>2017-01-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    holiday         ds\n",
       "0  new_year 2013-01-01\n",
       "1  new_year 2014-01-01\n",
       "2  new_year 2015-01-01\n",
       "3  new_year 2016-01-01\n",
       "4  new_year 2017-01-01"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_year.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wrapper classes for statsmodels ARIMA and statsmodels Exponential Smoothing.\n",
    "\n",
    "Adapter/wrapper classes to enable usage within `Ensemble` class and work with cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ARIMAWrapper(object):\n",
    "    '''\n",
    "    Facade for statsmodels.statespace\n",
    "    '''\n",
    "    def __init__(self, order, seasonal_order, training_index, holidays=None):\n",
    "        self._order = order\n",
    "        self._seasonal_order = seasonal_order\n",
    "        self._training_index = training_index\n",
    "        self._holidays = holidays\n",
    "\n",
    "    def _get_resids(self):\n",
    "        return self._fitted.resid\n",
    "\n",
    "    def _get_preds(self):\n",
    "        return self._fitted.fittedvalues\n",
    "    \n",
    "    def _encode_holidays(self, holidays, idx):\n",
    "        dummy = idx.isin(holidays).astype(int)\n",
    "        dummy = pd.DataFrame(dummy)\n",
    "        dummy.columns = ['holiday']\n",
    "        dummy.index = idx\n",
    "        return dummy\n",
    "\n",
    "    def fit(self, y_train):\n",
    "        \n",
    "        #extend training index\n",
    "        if len(y_train) > len(self._training_index):\n",
    "\n",
    "            self._training_index = pd.date_range(start=self._training_index[0], \n",
    "                                                 periods=len(y_train),\n",
    "                                                 freq=self._training_index.freq)\n",
    "            \n",
    "        holiday_train = None\n",
    "        if not self._holidays is None:\n",
    "            holiday_train = self._encode_holidays(self._holidays, \n",
    "                                                  self._training_index)\n",
    "    \n",
    "        \n",
    "        self._model = ARIMA(endog=y_train,\n",
    "                            exog=holiday_train,\n",
    "                            order=self._order, \n",
    "                            seasonal_order=self._seasonal_order)#,\n",
    "                            #enforce_stationarity=False)\n",
    "        \n",
    "        self._fitted = self._model.fit()\n",
    "        self._t = len(train)\n",
    "        \n",
    "    \n",
    "    def predict(self, horizon, return_conf_int=False, alpha=0.2):\n",
    "        '''\n",
    "        forecast h steps ahead.\n",
    "        \n",
    "        Params:\n",
    "        ------\n",
    "        h: int\n",
    "            h-step forecast\n",
    "        \n",
    "        return_conf_int: bool, optional (default=False)\n",
    "            return 1 - alpha PI\n",
    "        \n",
    "        alpha: float, optional (default=0.2)\n",
    "            return 1 - alpha PI\n",
    "                       \n",
    "        Returns:\n",
    "        -------\n",
    "        np.array\n",
    "            If return_conf_int = False returns preds only\n",
    "            \n",
    "        np.array, np.array\n",
    "            If return_conf_int = True returns tuple of preds, pred_ints\n",
    "        '''\n",
    "        \n",
    "        #+1 to date range then trim off the first value\n",
    "\n",
    "        f_idx = pd.date_range(start=self._training_index[-1], \n",
    "                              periods=horizon+1,\n",
    "                              freq=self._training_index.freq)[1:]\n",
    "        \n",
    "        #encode holidays if included.\n",
    "        exog_holiday = None\n",
    "        if not self._holidays is None:\n",
    "            exog_holiday = self._encode_holidays(self._holidays, f_idx)\n",
    "        \n",
    "    \n",
    "        forecast = self._fitted.get_forecast(horizon, exog=exog_holiday)\n",
    "        mean_forecast = forecast.summary_frame()['mean'].to_numpy()\n",
    "        \n",
    "        if return_conf_int:\n",
    "            df = forecast.summary_frame(alpha=alpha)\n",
    "            pi = df[['mean_ci_lower', 'mean_ci_upper']].to_numpy()\n",
    "            return mean_forecast, pi\n",
    "            \n",
    "        \n",
    "        else:\n",
    "            return mean_forecast\n",
    "\n",
    "    fittedvalues = property(_get_preds)\n",
    "    resid = property(_get_resids)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExponentialSmoothingWrapper:\n",
    "    '''\n",
    "    Facade for statsmodels exponential smoothing models.  This wrapper\n",
    "    provides a common interface for all models and allow interop with\n",
    "    the custom time series cross validation code.\n",
    "    '''\n",
    "    def __init__(self, trend=False, damped_trend=False, seasonal=None):\n",
    "        self._trend = trend\n",
    "        self._seasonal= seasonal\n",
    "        self._damped_trend = damped_trend\n",
    "\n",
    "    def _get_resids(self):\n",
    "        return self._fitted.resid\n",
    "\n",
    "    def _get_preds(self):\n",
    "        return self._fitted.fittedvalues\n",
    "\n",
    "    def fit(self, train):\n",
    "        '''\n",
    "        Fit the model\n",
    "        \n",
    "        Parameters:\n",
    "        train: array-like\n",
    "            time series to fit.\n",
    "        '''\n",
    "        self._model = ExponentialSmoothing(endog=train,\n",
    "                                          trend=self._trend, \n",
    "                                          damped_trend=self._damped_trend,\n",
    "                                          seasonal=self._seasonal)\n",
    "        self._fitted = self._model.fit()\n",
    "        self._t = len(train)\n",
    "    \n",
    "    def predict(self, horizon, return_conf_int=False, alpha=0.2):\n",
    "        '''\n",
    "        Forecast the time series from the final point in the fitted series.\n",
    "        \n",
    "        Parameters:\n",
    "        ----------\n",
    "        \n",
    "        horizon: int\n",
    "            steps ahead to forecast \n",
    "            \n",
    "        return_conf_int: bool, optional (default=False)\n",
    "            Return prediction interval?  \n",
    "            \n",
    "        alpha: float\n",
    "            Used if return_conf_int=True. 100(1-alpha) interval.\n",
    "        '''\n",
    "        \n",
    "        forecast = self._fitted.get_forecast(horizon)\n",
    "        \n",
    "        mean_forecast = forecast.summary_frame()['mean'].to_numpy()\n",
    "        \n",
    "        if return_conf_int:\n",
    "            df = forecast.summary_frame(alpha=alpha)\n",
    "            pi = df[['mean_ci_lower', 'mean_ci_upper']].to_numpy()\n",
    "            return mean_forecast, pi        \n",
    "        else:\n",
    "            return mean_forecast\n",
    "\n",
    "    fittedvalues = property(_get_preds)\n",
    "    resid = property(_get_resids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example of fitting the ensemble\n",
    "1. Regression with New Year Holiday and Auto ARIMA errors\n",
    "2. Holt-Winters Exponential Smoothing\n",
    "\n",
    "The code below demonstrates how to fit the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1 = ARIMAWrapper(order=(1,1,3), seasonal_order=(1,0,1,7), \n",
    "                       training_index=train.index,\n",
    "                       holidays=new_year['ds'].tolist())\n",
    "\n",
    "model_2 = ExponentialSmoothingWrapper(trend=True, damped_trend=True, \n",
    "                                      seasonal=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimators = {'arima': model_1, 'hw': model_2}\n",
    "ens = Ensemble(estimators, UnweightedVote())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit to training data in chosen region\n",
    "ens.fit(train[REGION])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict 7 days ahead\n",
    "H = 7\n",
    "ens_preds = ens.predict(horizon=H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2261.24638683, 2250.59085702, 2148.79400083, 2083.72132816,\n",
       "       2074.14597754, 2081.28015005, 2122.24263417])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#view predictions\n",
    "ens_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with prediction intervals\n",
    "ens_preds, pi = ens.predict(horizon=H, return_conf_int=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2261.24638683, 2250.59085702, 2148.79400083, 2083.72132816,\n",
       "       2074.14597754, 2081.28015005, 2122.24263417])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ens_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2165.3015595 , 2357.19121417],\n",
       "       [2147.72563186, 2353.45608217],\n",
       "       [2041.94187011, 2255.64613154],\n",
       "       [1973.26005144, 2194.18260487],\n",
       "       [1960.38309659, 2187.9088585 ],\n",
       "       [1964.46960816, 2198.09069193],\n",
       "       [2002.59663442, 2241.88863393]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross validation functions\n",
    "\n",
    "`time_series_cv` implements rolling forecast origin cross validation for time series.  \n",
    "It does not calculate forecast error, but instead returns the predictions, pred intervals and actuals in an array that can be passed to any forecast error function. (this is for efficiency and allows additional metrics to be calculated if needed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_series_cv(model, train, val, horizons, alpha=0.2, step=1):\n",
    "    '''\n",
    "    Time series cross validation across multiple horizons for a single model.\n",
    "\n",
    "    Incrementally adds additional training data to the model and tests\n",
    "    across a provided list of forecast horizons. Note that function tests a\n",
    "    model only against complete validation sets.  E.g. if horizon = 15 and \n",
    "    len(val) = 12 then no testing is done.  In the case of multiple horizons\n",
    "    e.g. [7, 14, 28] then the function will use the maximum forecast horizon\n",
    "    to calculate the number of iterations i.e if len(val) = 365 and step = 1\n",
    "    then no. iterations = len(val) - max(horizon) = 365 - 28 = 337.\n",
    "    \n",
    "    Parameters:\n",
    "    --------\n",
    "    model - forecasting model\n",
    "\n",
    "    train - np.array - vector of training data\n",
    "\n",
    "    val - np.array - vector of validation data\n",
    "\n",
    "    horizon - list of ints, forecast horizon e.g. [7, 14, 28] days\n",
    "    \n",
    "    alpha - float, optional (default=0.2)\n",
    "        1 - alpha prediction interval specification\n",
    "\n",
    "    step -- int, optional (default=1)\n",
    "            step taken in cross validation \n",
    "            e.g. 1 in next cross validation training data includes next point \n",
    "            from the validation set.\n",
    "            e.g. 7 in the next cross validation training data includes next 7 points\n",
    "            (default=1)\n",
    "            \n",
    "    Returns:\n",
    "    -------\n",
    "    np.array, np.array, np.array\n",
    "        - cv_preds, cv_test, cv_intervals\n",
    "    '''\n",
    "    \n",
    "    #point forecasts\n",
    "    cv_preds = [] \n",
    "    #ground truth observations\n",
    "    cv_actuals = [] \n",
    "    #prediction intervals\n",
    "    cv_pis = []\n",
    "    \n",
    "    split = 0\n",
    "\n",
    "    print('split => ', end=\"\")\n",
    "    for i in range(0, len(val) - max(horizons) + 1, step):\n",
    "        split += 1\n",
    "        print(f'{split}, ', end=\"\")\n",
    "                \n",
    "        train_cv = np.concatenate([train, val[:i]], axis=0)\n",
    "        model.fit(train_cv)\n",
    "        \n",
    "        #predict the maximum horizon \n",
    "        preds, pis = model.predict(horizon=len(val[i:i+max(horizons)]), \n",
    "                                   return_conf_int=True,\n",
    "                                   alpha=alpha)        \n",
    "        cv_h_preds = []\n",
    "        cv_test = []\n",
    "        cv_h_pis = []\n",
    "        \n",
    "        #sub horizon calculations\n",
    "        for h in horizons:\n",
    "            #store the h-step prediction\n",
    "            cv_h_preds.append(preds[:h])\n",
    "            #store the h-step actual value\n",
    "            cv_test.append(val.iloc[i:i+h])    \n",
    "            cv_h_pis.append(pis[:h])\n",
    "                     \n",
    "        cv_preds.append(cv_h_preds)\n",
    "        cv_actuals.append(cv_test)\n",
    "        cv_pis.append(cv_h_pis)\n",
    "        \n",
    "    print('done.\\n')        \n",
    "    return cv_preds, cv_actuals, cv_pis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom functions for calculating CV scores for point predictions and coverage.\n",
    "\n",
    "These functions have been written to work with the output of `time_series_cv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_cv_error(cv_preds, cv_test, error_func):\n",
    "    '''\n",
    "    Forecast error in the current split\n",
    "    \n",
    "    Params:\n",
    "    -----\n",
    "    cv_preds, np.array\n",
    "        Split predictions\n",
    "        \n",
    "    \n",
    "    cv_test: np.array\n",
    "        acutal ground truth observations\n",
    "        \n",
    "    error_func: object\n",
    "        function with signature (y_true, y_preds)\n",
    "        \n",
    "    Returns:\n",
    "    -------\n",
    "        np.ndarray\n",
    "            cross validation errors for split\n",
    "    '''\n",
    "    n_splits = len(cv_preds)\n",
    "    cv_errors = []\n",
    "    \n",
    "    for split in range(n_splits):\n",
    "        pred_error = error_func(cv_test[split], cv_preds[split])\n",
    "        cv_errors.append(pred_error)\n",
    "        \n",
    "    return np.array(cv_errors)\n",
    "\n",
    "def forecast_errors_cv(cv_preds, cv_test, error_func):\n",
    "    '''\n",
    "    Forecast errors by forecast horizon\n",
    "    \n",
    "    Params:\n",
    "    ------\n",
    "    cv_preds: np.ndarray\n",
    "        Array of arrays.  Each array is of size h representing\n",
    "        the forecast horizon specified.\n",
    "        \n",
    "    cv_test: np.ndarray\n",
    "        Array of arrays.  Each array is of size h representing\n",
    "        the forecast horizon specified.\n",
    "        \n",
    "    error_func: object\n",
    "        function with signature (y_true, y_preds)\n",
    "        \n",
    "    Returns:\n",
    "    -------\n",
    "    np.ndarray\n",
    "        \n",
    "    '''\n",
    "    cv_test = np.array(cv_test)\n",
    "    cv_preds = np.array(cv_preds)\n",
    "    n_horizons = len(cv_test)    \n",
    "    \n",
    "    horizon_errors = []\n",
    "    for h in range(n_horizons):\n",
    "        split_errors = split_cv_error(cv_preds[h], cv_test[h], error_func)\n",
    "        horizon_errors.append(split_errors)\n",
    "\n",
    "    return np.array(horizon_errors)\n",
    "\n",
    "def split_coverage(cv_test, cv_intervals):\n",
    "    n_splits = len(cv_test)\n",
    "    cv_errors = []\n",
    "        \n",
    "    for split in range(n_splits):\n",
    "        val = np.asarray(cv_test[split])\n",
    "        lower = cv_intervals[split].T[0]\n",
    "        upper = cv_intervals[split].T[1]\n",
    "        \n",
    "        coverage = len(np.where((val > lower) & (val < upper))[0])\n",
    "        coverage = coverage / len(val)\n",
    "        \n",
    "        cv_errors.append(coverage)\n",
    "        \n",
    "    return np.array(cv_errors)\n",
    "    \n",
    "    \n",
    "def prediction_int_coverage_cv(cv_test, cv_intervals):\n",
    "    cv_test = np.array(cv_test)\n",
    "    cv_intervals = np.array(cv_intervals)\n",
    "    n_horizons = len(cv_test)    \n",
    "    \n",
    "    horizon_coverage = []\n",
    "    for h in range(n_horizons):\n",
    "        split_coverages = split_coverage(cv_test[h], cv_intervals[h])\n",
    "        horizon_coverage.append(split_coverages)\n",
    "\n",
    "    return np.array(horizon_coverage)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_cv_error_scaled(cv_preds, cv_test, y_train):\n",
    "    n_splits = len(cv_preds)\n",
    "    cv_errors = []\n",
    "    \n",
    "    for split in range(n_splits):\n",
    "        pred_error = mean_absolute_scaled_error(cv_test[split], cv_preds[split], \n",
    "                                                y_train, period=7)\n",
    "        \n",
    "        cv_errors.append(pred_error)\n",
    "        \n",
    "    return np.array(cv_errors)\n",
    "\n",
    "def forecast_errors_cv_scaled(cv_preds, cv_test, y_train):\n",
    "    cv_test = np.array(cv_test)\n",
    "    cv_preds = np.array(cv_preds)\n",
    "    n_horizons = len(cv_test)    \n",
    "    \n",
    "    horizon_errors = []\n",
    "    for h in range(n_horizons):\n",
    "        split_errors = split_cv_error_scaled(cv_preds[h], cv_test[h], y_train)\n",
    "        horizon_errors.append(split_errors)\n",
    "        \n",
    "    return np.array(horizon_errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ensemble(fb_interval=0.8):\n",
    "    '''\n",
    "    Create and return ensemble model\n",
    "    '''\n",
    "    model_1 = ARIMAWrapper(order=(1,1,3), seasonal_order=(1,0,1,7), \n",
    "                           training_index=train.index,\n",
    "                           holidays=new_year['ds'].tolist())\n",
    "\n",
    "    model_2 = ExponentialSmoothingWrapper(trend=True, damped_trend=True, \n",
    "                                          seasonal=7)\n",
    "    \n",
    "    estimators = {'arima': model_1, 'hw': model_2}\n",
    "    return Ensemble(estimators, UnweightedVote())\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run cross validation\n",
    "\n",
    "This is run twices once each for 80 and 95% prediction intervals.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "split => 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, done.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "horizons = [7, 14, 21, 28, 35, 42, 49, 56, 63, 70, 77, 84, 365]\n",
    "model = get_ensemble()\n",
    "\n",
    "results = time_series_cv(model, train[REGION], val[REGION], horizons, \n",
    "                         alpha=0.2, step=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# symmetric MAPE results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>7</th>\n",
       "      <th>14</th>\n",
       "      <th>21</th>\n",
       "      <th>28</th>\n",
       "      <th>35</th>\n",
       "      <th>42</th>\n",
       "      <th>49</th>\n",
       "      <th>56</th>\n",
       "      <th>63</th>\n",
       "      <th>70</th>\n",
       "      <th>77</th>\n",
       "      <th>84</th>\n",
       "      <th>365</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2.841716</td>\n",
       "      <td>3.193180</td>\n",
       "      <td>3.346955</td>\n",
       "      <td>3.462023</td>\n",
       "      <td>3.610849</td>\n",
       "      <td>3.748776</td>\n",
       "      <td>3.847361</td>\n",
       "      <td>3.938023</td>\n",
       "      <td>4.043647</td>\n",
       "      <td>4.110324</td>\n",
       "      <td>4.170022</td>\n",
       "      <td>4.251676</td>\n",
       "      <td>5.108226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.092489</td>\n",
       "      <td>1.188468</td>\n",
       "      <td>1.147059</td>\n",
       "      <td>1.060012</td>\n",
       "      <td>1.100819</td>\n",
       "      <td>1.127005</td>\n",
       "      <td>1.140626</td>\n",
       "      <td>1.108010</td>\n",
       "      <td>1.106966</td>\n",
       "      <td>1.010946</td>\n",
       "      <td>0.938292</td>\n",
       "      <td>0.974988</td>\n",
       "      <td>1.652565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.498201</td>\n",
       "      <td>1.735182</td>\n",
       "      <td>1.875330</td>\n",
       "      <td>2.141260</td>\n",
       "      <td>2.214448</td>\n",
       "      <td>2.357114</td>\n",
       "      <td>2.374147</td>\n",
       "      <td>2.493206</td>\n",
       "      <td>2.690794</td>\n",
       "      <td>2.889782</td>\n",
       "      <td>2.943369</td>\n",
       "      <td>3.072235</td>\n",
       "      <td>3.488983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.960857</td>\n",
       "      <td>2.513679</td>\n",
       "      <td>2.694034</td>\n",
       "      <td>2.819100</td>\n",
       "      <td>2.837185</td>\n",
       "      <td>2.979749</td>\n",
       "      <td>3.078592</td>\n",
       "      <td>3.247460</td>\n",
       "      <td>3.409912</td>\n",
       "      <td>3.420824</td>\n",
       "      <td>3.468915</td>\n",
       "      <td>3.627075</td>\n",
       "      <td>3.777043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2.444450</td>\n",
       "      <td>2.891852</td>\n",
       "      <td>3.125479</td>\n",
       "      <td>3.256980</td>\n",
       "      <td>3.367021</td>\n",
       "      <td>3.324309</td>\n",
       "      <td>3.626755</td>\n",
       "      <td>3.710027</td>\n",
       "      <td>3.851025</td>\n",
       "      <td>3.991769</td>\n",
       "      <td>4.051374</td>\n",
       "      <td>4.029532</td>\n",
       "      <td>5.049119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3.639793</td>\n",
       "      <td>3.777411</td>\n",
       "      <td>3.725987</td>\n",
       "      <td>3.709215</td>\n",
       "      <td>4.211614</td>\n",
       "      <td>4.456798</td>\n",
       "      <td>4.493621</td>\n",
       "      <td>4.384990</td>\n",
       "      <td>4.424759</td>\n",
       "      <td>4.308067</td>\n",
       "      <td>4.683766</td>\n",
       "      <td>4.773524</td>\n",
       "      <td>5.863021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>5.634527</td>\n",
       "      <td>6.860175</td>\n",
       "      <td>7.351093</td>\n",
       "      <td>7.107138</td>\n",
       "      <td>7.532948</td>\n",
       "      <td>7.603609</td>\n",
       "      <td>7.607221</td>\n",
       "      <td>7.323897</td>\n",
       "      <td>7.679503</td>\n",
       "      <td>7.395063</td>\n",
       "      <td>7.156615</td>\n",
       "      <td>7.537536</td>\n",
       "      <td>11.351217</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             7          14         21         28         35         42   \\\n",
       "count  27.000000  27.000000  27.000000  27.000000  27.000000  27.000000   \n",
       "mean    2.841716   3.193180   3.346955   3.462023   3.610849   3.748776   \n",
       "std     1.092489   1.188468   1.147059   1.060012   1.100819   1.127005   \n",
       "min     1.498201   1.735182   1.875330   2.141260   2.214448   2.357114   \n",
       "25%     1.960857   2.513679   2.694034   2.819100   2.837185   2.979749   \n",
       "50%     2.444450   2.891852   3.125479   3.256980   3.367021   3.324309   \n",
       "75%     3.639793   3.777411   3.725987   3.709215   4.211614   4.456798   \n",
       "max     5.634527   6.860175   7.351093   7.107138   7.532948   7.603609   \n",
       "\n",
       "             49         56         63         70         77         84   \\\n",
       "count  27.000000  27.000000  27.000000  27.000000  27.000000  27.000000   \n",
       "mean    3.847361   3.938023   4.043647   4.110324   4.170022   4.251676   \n",
       "std     1.140626   1.108010   1.106966   1.010946   0.938292   0.974988   \n",
       "min     2.374147   2.493206   2.690794   2.889782   2.943369   3.072235   \n",
       "25%     3.078592   3.247460   3.409912   3.420824   3.468915   3.627075   \n",
       "50%     3.626755   3.710027   3.851025   3.991769   4.051374   4.029532   \n",
       "75%     4.493621   4.384990   4.424759   4.308067   4.683766   4.773524   \n",
       "max     7.607221   7.323897   7.679503   7.395063   7.156615   7.537536   \n",
       "\n",
       "             365  \n",
       "count  27.000000  \n",
       "mean    5.108226  \n",
       "std     1.652565  \n",
       "min     3.488983  \n",
       "25%     3.777043  \n",
       "50%     5.049119  \n",
       "75%     5.863021  \n",
       "max    11.351217  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_preds, cv_test, cv_intervals = results\n",
    "#CV point predictions smape\n",
    "cv_errors = forecast_errors_cv(cv_preds, cv_test, \n",
    "                               symmetric_mean_absolute_percentage_error)\n",
    "df = pd.DataFrame(cv_errors)\n",
    "df.columns = horizons\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../../results/model_selection/stage1/Trust-arima-hw_smape.csv\n"
     ]
    }
   ],
   "source": [
    "#output sMAPE results to file\n",
    "metric = 'smape'\n",
    "print(f'{TOP_LEVEL}/{STAGE}/{REGION}-{METHOD}_{metric}.csv')\n",
    "df.to_csv(f'{TOP_LEVEL}/{STAGE}/{REGION}-{METHOD}_{metric}.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RMSE results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>7</th>\n",
       "      <th>14</th>\n",
       "      <th>21</th>\n",
       "      <th>28</th>\n",
       "      <th>35</th>\n",
       "      <th>42</th>\n",
       "      <th>49</th>\n",
       "      <th>56</th>\n",
       "      <th>63</th>\n",
       "      <th>70</th>\n",
       "      <th>77</th>\n",
       "      <th>84</th>\n",
       "      <th>365</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>74.988263</td>\n",
       "      <td>85.534422</td>\n",
       "      <td>91.410668</td>\n",
       "      <td>95.445099</td>\n",
       "      <td>100.067545</td>\n",
       "      <td>104.244589</td>\n",
       "      <td>107.463300</td>\n",
       "      <td>110.352160</td>\n",
       "      <td>113.669126</td>\n",
       "      <td>116.195505</td>\n",
       "      <td>118.397852</td>\n",
       "      <td>121.176639</td>\n",
       "      <td>142.623856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>28.434099</td>\n",
       "      <td>33.140740</td>\n",
       "      <td>33.058368</td>\n",
       "      <td>31.452604</td>\n",
       "      <td>32.111216</td>\n",
       "      <td>32.239890</td>\n",
       "      <td>32.536045</td>\n",
       "      <td>32.215778</td>\n",
       "      <td>31.899325</td>\n",
       "      <td>29.545289</td>\n",
       "      <td>27.485643</td>\n",
       "      <td>27.830162</td>\n",
       "      <td>37.574007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>46.797529</td>\n",
       "      <td>44.698958</td>\n",
       "      <td>49.339830</td>\n",
       "      <td>59.359345</td>\n",
       "      <td>61.101241</td>\n",
       "      <td>65.834922</td>\n",
       "      <td>66.083921</td>\n",
       "      <td>68.545492</td>\n",
       "      <td>74.561462</td>\n",
       "      <td>79.081271</td>\n",
       "      <td>80.194306</td>\n",
       "      <td>82.736560</td>\n",
       "      <td>106.438723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>53.318795</td>\n",
       "      <td>65.185142</td>\n",
       "      <td>73.550448</td>\n",
       "      <td>79.750552</td>\n",
       "      <td>78.776548</td>\n",
       "      <td>79.661766</td>\n",
       "      <td>82.186630</td>\n",
       "      <td>88.268085</td>\n",
       "      <td>93.351392</td>\n",
       "      <td>91.701330</td>\n",
       "      <td>94.584957</td>\n",
       "      <td>100.402162</td>\n",
       "      <td>113.759928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>64.816353</td>\n",
       "      <td>80.111237</td>\n",
       "      <td>80.686719</td>\n",
       "      <td>84.297659</td>\n",
       "      <td>89.619708</td>\n",
       "      <td>91.289293</td>\n",
       "      <td>96.552269</td>\n",
       "      <td>99.144146</td>\n",
       "      <td>104.186922</td>\n",
       "      <td>111.671527</td>\n",
       "      <td>117.501815</td>\n",
       "      <td>116.134107</td>\n",
       "      <td>140.977670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>87.712965</td>\n",
       "      <td>95.468675</td>\n",
       "      <td>94.728276</td>\n",
       "      <td>94.631052</td>\n",
       "      <td>108.123192</td>\n",
       "      <td>123.185961</td>\n",
       "      <td>130.726748</td>\n",
       "      <td>129.125872</td>\n",
       "      <td>127.556611</td>\n",
       "      <td>127.305474</td>\n",
       "      <td>130.351363</td>\n",
       "      <td>138.110855</td>\n",
       "      <td>159.353892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>156.956142</td>\n",
       "      <td>183.112775</td>\n",
       "      <td>190.129699</td>\n",
       "      <td>182.786288</td>\n",
       "      <td>191.532560</td>\n",
       "      <td>191.928067</td>\n",
       "      <td>191.950803</td>\n",
       "      <td>186.175786</td>\n",
       "      <td>194.533736</td>\n",
       "      <td>189.502333</td>\n",
       "      <td>184.414480</td>\n",
       "      <td>195.756517</td>\n",
       "      <td>286.424396</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              7           14          21          28          35          42   \\\n",
       "count   27.000000   27.000000   27.000000   27.000000   27.000000   27.000000   \n",
       "mean    74.988263   85.534422   91.410668   95.445099  100.067545  104.244589   \n",
       "std     28.434099   33.140740   33.058368   31.452604   32.111216   32.239890   \n",
       "min     46.797529   44.698958   49.339830   59.359345   61.101241   65.834922   \n",
       "25%     53.318795   65.185142   73.550448   79.750552   78.776548   79.661766   \n",
       "50%     64.816353   80.111237   80.686719   84.297659   89.619708   91.289293   \n",
       "75%     87.712965   95.468675   94.728276   94.631052  108.123192  123.185961   \n",
       "max    156.956142  183.112775  190.129699  182.786288  191.532560  191.928067   \n",
       "\n",
       "              49          56          63          70          77          84   \\\n",
       "count   27.000000   27.000000   27.000000   27.000000   27.000000   27.000000   \n",
       "mean   107.463300  110.352160  113.669126  116.195505  118.397852  121.176639   \n",
       "std     32.536045   32.215778   31.899325   29.545289   27.485643   27.830162   \n",
       "min     66.083921   68.545492   74.561462   79.081271   80.194306   82.736560   \n",
       "25%     82.186630   88.268085   93.351392   91.701330   94.584957  100.402162   \n",
       "50%     96.552269   99.144146  104.186922  111.671527  117.501815  116.134107   \n",
       "75%    130.726748  129.125872  127.556611  127.305474  130.351363  138.110855   \n",
       "max    191.950803  186.175786  194.533736  189.502333  184.414480  195.756517   \n",
       "\n",
       "              365  \n",
       "count   27.000000  \n",
       "mean   142.623856  \n",
       "std     37.574007  \n",
       "min    106.438723  \n",
       "25%    113.759928  \n",
       "50%    140.977670  \n",
       "75%    159.353892  \n",
       "max    286.424396  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#CV point predictions rmse\n",
    "cv_errors = forecast_errors_cv(cv_preds, cv_test, root_mean_squared_error)\n",
    "df = pd.DataFrame(cv_errors)\n",
    "df.columns = horizons\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../../results/model_selection/stage1/Trust-arima-hw_rmse.csv\n"
     ]
    }
   ],
   "source": [
    "#output RMSE to file\n",
    "metric = 'rmse'\n",
    "print(f'{TOP_LEVEL}/{STAGE}/{REGION}-{METHOD}_{metric}.csv')\n",
    "df.to_csv(f'{TOP_LEVEL}/{STAGE}/{REGION}-{METHOD}_{metric}.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mean Absolute Scaled Error (MASE)\n",
    "\n",
    "Scaled by one-step insample Seasonal Naive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>7</th>\n",
       "      <th>14</th>\n",
       "      <th>21</th>\n",
       "      <th>28</th>\n",
       "      <th>35</th>\n",
       "      <th>42</th>\n",
       "      <th>49</th>\n",
       "      <th>56</th>\n",
       "      <th>63</th>\n",
       "      <th>70</th>\n",
       "      <th>77</th>\n",
       "      <th>84</th>\n",
       "      <th>365</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.767222</td>\n",
       "      <td>0.865183</td>\n",
       "      <td>0.909189</td>\n",
       "      <td>0.942009</td>\n",
       "      <td>0.983500</td>\n",
       "      <td>1.022064</td>\n",
       "      <td>1.049843</td>\n",
       "      <td>1.074886</td>\n",
       "      <td>1.104023</td>\n",
       "      <td>1.122465</td>\n",
       "      <td>1.139132</td>\n",
       "      <td>1.161788</td>\n",
       "      <td>1.387167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.325144</td>\n",
       "      <td>0.359754</td>\n",
       "      <td>0.350113</td>\n",
       "      <td>0.328241</td>\n",
       "      <td>0.338548</td>\n",
       "      <td>0.344636</td>\n",
       "      <td>0.346127</td>\n",
       "      <td>0.335549</td>\n",
       "      <td>0.333356</td>\n",
       "      <td>0.304900</td>\n",
       "      <td>0.282118</td>\n",
       "      <td>0.288518</td>\n",
       "      <td>0.468986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.408913</td>\n",
       "      <td>0.457874</td>\n",
       "      <td>0.503834</td>\n",
       "      <td>0.576618</td>\n",
       "      <td>0.599030</td>\n",
       "      <td>0.632419</td>\n",
       "      <td>0.638874</td>\n",
       "      <td>0.668845</td>\n",
       "      <td>0.725274</td>\n",
       "      <td>0.779323</td>\n",
       "      <td>0.760998</td>\n",
       "      <td>0.798394</td>\n",
       "      <td>0.949530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.542749</td>\n",
       "      <td>0.648681</td>\n",
       "      <td>0.711453</td>\n",
       "      <td>0.733531</td>\n",
       "      <td>0.740555</td>\n",
       "      <td>0.775785</td>\n",
       "      <td>0.807740</td>\n",
       "      <td>0.865963</td>\n",
       "      <td>0.909666</td>\n",
       "      <td>0.885829</td>\n",
       "      <td>0.920054</td>\n",
       "      <td>0.976661</td>\n",
       "      <td>1.025336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.652615</td>\n",
       "      <td>0.754847</td>\n",
       "      <td>0.812946</td>\n",
       "      <td>0.875076</td>\n",
       "      <td>0.874533</td>\n",
       "      <td>0.876260</td>\n",
       "      <td>0.959082</td>\n",
       "      <td>0.973048</td>\n",
       "      <td>1.041685</td>\n",
       "      <td>1.079482</td>\n",
       "      <td>1.117149</td>\n",
       "      <td>1.080794</td>\n",
       "      <td>1.371037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.944688</td>\n",
       "      <td>0.985114</td>\n",
       "      <td>0.992906</td>\n",
       "      <td>0.980608</td>\n",
       "      <td>1.129034</td>\n",
       "      <td>1.226786</td>\n",
       "      <td>1.246211</td>\n",
       "      <td>1.206528</td>\n",
       "      <td>1.228042</td>\n",
       "      <td>1.206092</td>\n",
       "      <td>1.293715</td>\n",
       "      <td>1.305888</td>\n",
       "      <td>1.570518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.659366</td>\n",
       "      <td>2.001984</td>\n",
       "      <td>2.118957</td>\n",
       "      <td>2.045908</td>\n",
       "      <td>2.158184</td>\n",
       "      <td>2.175243</td>\n",
       "      <td>2.172599</td>\n",
       "      <td>2.090264</td>\n",
       "      <td>2.186301</td>\n",
       "      <td>2.105705</td>\n",
       "      <td>2.040416</td>\n",
       "      <td>2.145312</td>\n",
       "      <td>3.218237</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             7          14         21         28         35         42   \\\n",
       "count  27.000000  27.000000  27.000000  27.000000  27.000000  27.000000   \n",
       "mean    0.767222   0.865183   0.909189   0.942009   0.983500   1.022064   \n",
       "std     0.325144   0.359754   0.350113   0.328241   0.338548   0.344636   \n",
       "min     0.408913   0.457874   0.503834   0.576618   0.599030   0.632419   \n",
       "25%     0.542749   0.648681   0.711453   0.733531   0.740555   0.775785   \n",
       "50%     0.652615   0.754847   0.812946   0.875076   0.874533   0.876260   \n",
       "75%     0.944688   0.985114   0.992906   0.980608   1.129034   1.226786   \n",
       "max     1.659366   2.001984   2.118957   2.045908   2.158184   2.175243   \n",
       "\n",
       "             49         56         63         70         77         84   \\\n",
       "count  27.000000  27.000000  27.000000  27.000000  27.000000  27.000000   \n",
       "mean    1.049843   1.074886   1.104023   1.122465   1.139132   1.161788   \n",
       "std     0.346127   0.335549   0.333356   0.304900   0.282118   0.288518   \n",
       "min     0.638874   0.668845   0.725274   0.779323   0.760998   0.798394   \n",
       "25%     0.807740   0.865963   0.909666   0.885829   0.920054   0.976661   \n",
       "50%     0.959082   0.973048   1.041685   1.079482   1.117149   1.080794   \n",
       "75%     1.246211   1.206528   1.228042   1.206092   1.293715   1.305888   \n",
       "max     2.172599   2.090264   2.186301   2.105705   2.040416   2.145312   \n",
       "\n",
       "             365  \n",
       "count  27.000000  \n",
       "mean    1.387167  \n",
       "std     0.468986  \n",
       "min     0.949530  \n",
       "25%     1.025336  \n",
       "50%     1.371037  \n",
       "75%     1.570518  \n",
       "max     3.218237  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#mase\n",
    "cv_errors = forecast_errors_cv_scaled(cv_preds, cv_test, train[REGION])\n",
    "df = pd.DataFrame(cv_errors)\n",
    "df.columns = horizons\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../../results/model_selection/stage1/Trust-arima-hw_mase.csv\n"
     ]
    }
   ],
   "source": [
    "#output mase to file.\n",
    "metric = 'mase'\n",
    "print(f'{TOP_LEVEL}/{STAGE}/{REGION}-{METHOD}_{metric}.csv')\n",
    "df.to_csv(f'{TOP_LEVEL}/{STAGE}/{REGION}-{METHOD}_{metric}.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 80% Prediction Interval Coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>7</th>\n",
       "      <th>14</th>\n",
       "      <th>21</th>\n",
       "      <th>28</th>\n",
       "      <th>35</th>\n",
       "      <th>42</th>\n",
       "      <th>49</th>\n",
       "      <th>56</th>\n",
       "      <th>63</th>\n",
       "      <th>70</th>\n",
       "      <th>77</th>\n",
       "      <th>84</th>\n",
       "      <th>365</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.825397</td>\n",
       "      <td>0.820106</td>\n",
       "      <td>0.828924</td>\n",
       "      <td>0.838624</td>\n",
       "      <td>0.840212</td>\n",
       "      <td>0.841270</td>\n",
       "      <td>0.846561</td>\n",
       "      <td>0.854497</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.860847</td>\n",
       "      <td>0.863877</td>\n",
       "      <td>0.865079</td>\n",
       "      <td>0.935667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.207146</td>\n",
       "      <td>0.191428</td>\n",
       "      <td>0.165622</td>\n",
       "      <td>0.139862</td>\n",
       "      <td>0.140029</td>\n",
       "      <td>0.137094</td>\n",
       "      <td>0.129737</td>\n",
       "      <td>0.119246</td>\n",
       "      <td>0.122478</td>\n",
       "      <td>0.108406</td>\n",
       "      <td>0.097610</td>\n",
       "      <td>0.099766</td>\n",
       "      <td>0.057996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.190476</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.257143</td>\n",
       "      <td>0.261905</td>\n",
       "      <td>0.306122</td>\n",
       "      <td>0.392857</td>\n",
       "      <td>0.365079</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.480519</td>\n",
       "      <td>0.464286</td>\n",
       "      <td>0.671233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>0.821429</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>0.797619</td>\n",
       "      <td>0.795918</td>\n",
       "      <td>0.803571</td>\n",
       "      <td>0.825397</td>\n",
       "      <td>0.828571</td>\n",
       "      <td>0.831169</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.936986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.892857</td>\n",
       "      <td>0.885714</td>\n",
       "      <td>0.880952</td>\n",
       "      <td>0.897959</td>\n",
       "      <td>0.892857</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.885714</td>\n",
       "      <td>0.870130</td>\n",
       "      <td>0.869048</td>\n",
       "      <td>0.945205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>0.910714</td>\n",
       "      <td>0.914286</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>0.921429</td>\n",
       "      <td>0.922078</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>0.963014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.971429</td>\n",
       "      <td>0.976190</td>\n",
       "      <td>0.979592</td>\n",
       "      <td>0.982143</td>\n",
       "      <td>0.984127</td>\n",
       "      <td>0.985714</td>\n",
       "      <td>0.987013</td>\n",
       "      <td>0.976190</td>\n",
       "      <td>0.969863</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             7          14         21         28         35         42   \\\n",
       "count  27.000000  27.000000  27.000000  27.000000  27.000000  27.000000   \n",
       "mean    0.825397   0.820106   0.828924   0.838624   0.840212   0.841270   \n",
       "std     0.207146   0.191428   0.165622   0.139862   0.140029   0.137094   \n",
       "min     0.142857   0.142857   0.190476   0.285714   0.257143   0.261905   \n",
       "25%     0.714286   0.750000   0.785714   0.821429   0.785714   0.797619   \n",
       "50%     0.857143   0.857143   0.857143   0.892857   0.885714   0.880952   \n",
       "75%     1.000000   0.928571   0.928571   0.910714   0.914286   0.928571   \n",
       "max     1.000000   1.000000   1.000000   1.000000   0.971429   0.976190   \n",
       "\n",
       "             49         56         63         70         77         84   \\\n",
       "count  27.000000  27.000000  27.000000  27.000000  27.000000  27.000000   \n",
       "mean    0.846561   0.854497   0.857143   0.860847   0.863877   0.865079   \n",
       "std     0.129737   0.119246   0.122478   0.108406   0.097610   0.099766   \n",
       "min     0.306122   0.392857   0.365079   0.428571   0.480519   0.464286   \n",
       "25%     0.795918   0.803571   0.825397   0.828571   0.831169   0.833333   \n",
       "50%     0.897959   0.892857   0.888889   0.885714   0.870130   0.869048   \n",
       "75%     0.928571   0.928571   0.928571   0.921429   0.922078   0.928571   \n",
       "max     0.979592   0.982143   0.984127   0.985714   0.987013   0.976190   \n",
       "\n",
       "             365  \n",
       "count  27.000000  \n",
       "mean    0.935667  \n",
       "std     0.057996  \n",
       "min     0.671233  \n",
       "25%     0.936986  \n",
       "50%     0.945205  \n",
       "75%     0.963014  \n",
       "max     0.969863  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#80% PIs\n",
    "cv_coverage = prediction_int_coverage_cv(cv_test, cv_intervals)\n",
    "df = pd.DataFrame(cv_coverage)\n",
    "df.columns = horizons\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../../results/model_selection/stage1/Trust-arima-hw_coverage_80.csv\n"
     ]
    }
   ],
   "source": [
    "#write 80% coverage to file\n",
    "metric = 'coverage_80'\n",
    "print(f'{TOP_LEVEL}/{STAGE}/{REGION}-{METHOD}_{metric}.csv')\n",
    "df.to_csv(f'{TOP_LEVEL}/{STAGE}/{REGION}-{METHOD}_{metric}.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 95% Prediction Interval Coverage\n",
    "\n",
    "Rerun analysis and obtain 95% Prediction intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "split => 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, done.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "horizons = [7, 14, 21, 28, 35, 42, 49, 56, 63, 70, 77, 84, 365]\n",
    "model = get_ensemble(fb_interval=0.95)\n",
    "\n",
    "results = time_series_cv(model, train[REGION], val[REGION], horizons, \n",
    "                         alpha=0.05, step=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>7</th>\n",
       "      <th>14</th>\n",
       "      <th>21</th>\n",
       "      <th>28</th>\n",
       "      <th>35</th>\n",
       "      <th>42</th>\n",
       "      <th>49</th>\n",
       "      <th>56</th>\n",
       "      <th>63</th>\n",
       "      <th>70</th>\n",
       "      <th>77</th>\n",
       "      <th>84</th>\n",
       "      <th>365</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.968254</td>\n",
       "      <td>0.952381</td>\n",
       "      <td>0.950617</td>\n",
       "      <td>0.953704</td>\n",
       "      <td>0.953439</td>\n",
       "      <td>0.955908</td>\n",
       "      <td>0.956160</td>\n",
       "      <td>0.957011</td>\n",
       "      <td>0.957672</td>\n",
       "      <td>0.959259</td>\n",
       "      <td>0.960558</td>\n",
       "      <td>0.959877</td>\n",
       "      <td>0.982750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.072339</td>\n",
       "      <td>0.106684</td>\n",
       "      <td>0.107686</td>\n",
       "      <td>0.090141</td>\n",
       "      <td>0.079696</td>\n",
       "      <td>0.070105</td>\n",
       "      <td>0.063964</td>\n",
       "      <td>0.057813</td>\n",
       "      <td>0.054276</td>\n",
       "      <td>0.049281</td>\n",
       "      <td>0.044478</td>\n",
       "      <td>0.042560</td>\n",
       "      <td>0.008283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.678571</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.761905</td>\n",
       "      <td>0.775510</td>\n",
       "      <td>0.803571</td>\n",
       "      <td>0.809524</td>\n",
       "      <td>0.828571</td>\n",
       "      <td>0.844156</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.950685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>0.952381</td>\n",
       "      <td>0.964286</td>\n",
       "      <td>0.957143</td>\n",
       "      <td>0.952381</td>\n",
       "      <td>0.908163</td>\n",
       "      <td>0.892857</td>\n",
       "      <td>0.904762</td>\n",
       "      <td>0.914286</td>\n",
       "      <td>0.922078</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>0.980822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.982143</td>\n",
       "      <td>0.984127</td>\n",
       "      <td>0.985714</td>\n",
       "      <td>0.987013</td>\n",
       "      <td>0.976190</td>\n",
       "      <td>0.983562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.986301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.994521</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             7          14         21         28         35         42   \\\n",
       "count  27.000000  27.000000  27.000000  27.000000  27.000000  27.000000   \n",
       "mean    0.968254   0.952381   0.950617   0.953704   0.953439   0.955908   \n",
       "std     0.072339   0.106684   0.107686   0.090141   0.079696   0.070105   \n",
       "min     0.714286   0.571429   0.571429   0.678571   0.714286   0.761905   \n",
       "25%     1.000000   0.928571   0.952381   0.964286   0.957143   0.952381   \n",
       "50%     1.000000   1.000000   1.000000   1.000000   1.000000   1.000000   \n",
       "75%     1.000000   1.000000   1.000000   1.000000   1.000000   1.000000   \n",
       "max     1.000000   1.000000   1.000000   1.000000   1.000000   1.000000   \n",
       "\n",
       "             49         56         63         70         77         84   \\\n",
       "count  27.000000  27.000000  27.000000  27.000000  27.000000  27.000000   \n",
       "mean    0.956160   0.957011   0.957672   0.959259   0.960558   0.959877   \n",
       "std     0.063964   0.057813   0.054276   0.049281   0.044478   0.042560   \n",
       "min     0.775510   0.803571   0.809524   0.828571   0.844156   0.833333   \n",
       "25%     0.908163   0.892857   0.904762   0.914286   0.922078   0.928571   \n",
       "50%     1.000000   0.982143   0.984127   0.985714   0.987013   0.976190   \n",
       "75%     1.000000   1.000000   1.000000   1.000000   1.000000   1.000000   \n",
       "max     1.000000   1.000000   1.000000   1.000000   1.000000   1.000000   \n",
       "\n",
       "             365  \n",
       "count  27.000000  \n",
       "mean    0.982750  \n",
       "std     0.008283  \n",
       "min     0.950685  \n",
       "25%     0.980822  \n",
       "50%     0.983562  \n",
       "75%     0.986301  \n",
       "max     0.994521  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#95% PIs\n",
    "cv_preds, cv_test, cv_intervals = results\n",
    "cv_coverage = prediction_int_coverage_cv(cv_test, cv_intervals)\n",
    "df = pd.DataFrame(cv_coverage)\n",
    "df.columns = horizons\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../../results/model_selection/stage1/Trust-arima-hw_coverage_95.csv\n"
     ]
    }
   ],
   "source": [
    "#write 95% coverage to file\n",
    "metric = 'coverage_95'\n",
    "print(f'{TOP_LEVEL}/{STAGE}/{REGION}-{METHOD}_{metric}.csv')\n",
    "df.to_csv(f'{TOP_LEVEL}/{STAGE}/{REGION}-{METHOD}_{metric}.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ambo",
   "language": "python",
   "name": "ambo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
