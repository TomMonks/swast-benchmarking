{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross validation: An Ensemble of Facebook's Prophet and Regression with ARIMA errors\n",
    "---\n",
    "\n",
    "An unweighted ensemble of Facebook Prophet with New Year holiday and Regression with new year holiday and (auto) ARIMA errors.  The ARIMA process modelled is a (1,1,3)(1,0,1,7).  This was determined by auto_arima from the `pmdarima` package.\n",
    "\n",
    "This notebook conducts cross validation of the method using a rolling forecast origin method.\n",
    "\n",
    "> The Reg with ARIMA error model is based on SWAST's data.  It may be prudent for each trust to fit their own automatic ARIMA model.\n",
    "\n",
    "**The notebook outputs:**\n",
    "* MASE, RMSE and MAPE at 7 day intervals from 7 to 84 days.\n",
    "* 80 and 95% prediction intervals between 7 and 84 days.\n",
    "\n",
    "These are saved into the folder `results/model_selection/stage1/`\n",
    "\n",
    "---\n",
    "\n",
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#error measures\n",
    "from forecast_tools.metrics import (mean_absolute_scaled_error, \n",
    "                                    root_mean_squared_error,\n",
    "                                    symmetric_mean_absolute_percentage_error)\n",
    "\n",
    "#models\n",
    "from pmdarima import auto_arima\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from fbprophet import Prophet\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.11.0\n",
      "1.5.2\n"
     ]
    }
   ],
   "source": [
    "import statsmodels as sm\n",
    "import pmdarima \n",
    "\n",
    "print(sm.__version__)\n",
    "print(pmdarima.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to select exceptionally busy days as covariates.\n",
    "from amb_forecast.feature_engineering import (regular_busy_calender_days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#custom ensemble class\n",
    "from amb_forecast.ensemble import (Ensemble, UnweightedVote)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Input\n",
    "\n",
    "The constants `TOP_LEVEL`, `STAGE`, `REGION`,`TRUST` and `METHOD` are used to control data selection and the directory for outputting results.  \n",
    "\n",
    "> Output file is `f'{TOP_LEVEL}/{STAGE}/{REGION}-{METHOD}_{metric}.csv'.csv`.  where metric will be smape, rmse, mase, coverage_80 and coverage_95. Note: `REGION`: is also used to select the correct data from the input dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOP_LEVEL = '../../../results/model_selection'\n",
    "STAGE = 'stage1'\n",
    "REGION = 'Trust'\n",
    "METHOD = 'fbp-arima'\n",
    "\n",
    "FILE_NAME = 'Daily_Responses_5_Years_2019_full.csv'\n",
    "\n",
    "#split training and test data.\n",
    "TEST_SPLIT_DATE = '2019-01-01'\n",
    "\n",
    "#second subdivide: train and val\n",
    "VAL_SPLIT_DATE = '2017-07-01'\n",
    "\n",
    "#discard data after 2020 due to coronavirus\n",
    "#this is the subject of a seperate study.\n",
    "DISCARD_DATE = '2020-01-01'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in path\n",
    "path = f'../../../data/{FILE_NAME}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process_daily_data(path, index_col, by_col, \n",
    "                           values, dayfirst=False):\n",
    "    '''\n",
    "    Daily data is stored in long format.  Read in \n",
    "    and pivot to wide format so that there is a single \n",
    "    colmumn for each regions time series.\n",
    "    '''\n",
    "    df = pd.read_csv(path, index_col=index_col, parse_dates=True, \n",
    "                     dayfirst=dayfirst)\n",
    "    df.columns = map(str.lower, df.columns)\n",
    "    df.index.rename(str(df.index.name).lower(), inplace=True)\n",
    "    \n",
    "    clean_table = pd.pivot_table(df, values=values.lower(), \n",
    "                                 index=[index_col.lower()],\n",
    "                                 columns=[by_col.lower()], aggfunc=np.sum)\n",
    "    \n",
    "    clean_table.index.freq = 'D'\n",
    "    \n",
    "    return clean_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>ora</th>\n",
       "      <th>BNSSG</th>\n",
       "      <th>Cornwall</th>\n",
       "      <th>Devon</th>\n",
       "      <th>Dorset</th>\n",
       "      <th>Gloucestershire</th>\n",
       "      <th>OOA</th>\n",
       "      <th>Somerset</th>\n",
       "      <th>Trust</th>\n",
       "      <th>Wiltshire</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>actual_dt</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2013-12-30</th>\n",
       "      <td>415.0</td>\n",
       "      <td>220.0</td>\n",
       "      <td>502.0</td>\n",
       "      <td>336.0</td>\n",
       "      <td>129.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>183.0</td>\n",
       "      <td>2042.0</td>\n",
       "      <td>255.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-12-31</th>\n",
       "      <td>420.0</td>\n",
       "      <td>236.0</td>\n",
       "      <td>468.0</td>\n",
       "      <td>302.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>180.0</td>\n",
       "      <td>1996.0</td>\n",
       "      <td>260.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-01-01</th>\n",
       "      <td>549.0</td>\n",
       "      <td>341.0</td>\n",
       "      <td>566.0</td>\n",
       "      <td>392.0</td>\n",
       "      <td>157.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>213.0</td>\n",
       "      <td>2570.0</td>\n",
       "      <td>351.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-01-02</th>\n",
       "      <td>450.0</td>\n",
       "      <td>218.0</td>\n",
       "      <td>499.0</td>\n",
       "      <td>301.0</td>\n",
       "      <td>115.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>167.0</td>\n",
       "      <td>2013.0</td>\n",
       "      <td>258.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-01-03</th>\n",
       "      <td>419.0</td>\n",
       "      <td>229.0</td>\n",
       "      <td>503.0</td>\n",
       "      <td>304.0</td>\n",
       "      <td>135.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>195.0</td>\n",
       "      <td>2056.0</td>\n",
       "      <td>269.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "ora         BNSSG  Cornwall  Devon  Dorset  Gloucestershire  OOA  Somerset  \\\n",
       "actual_dt                                                                    \n",
       "2013-12-30  415.0     220.0  502.0   336.0            129.0  NaN     183.0   \n",
       "2013-12-31  420.0     236.0  468.0   302.0            128.0  NaN     180.0   \n",
       "2014-01-01  549.0     341.0  566.0   392.0            157.0  NaN     213.0   \n",
       "2014-01-02  450.0     218.0  499.0   301.0            115.0  NaN     167.0   \n",
       "2014-01-03  419.0     229.0  503.0   304.0            135.0  NaN     195.0   \n",
       "\n",
       "ora          Trust  Wiltshire  \n",
       "actual_dt                      \n",
       "2013-12-30  2042.0      255.0  \n",
       "2013-12-31  1996.0      260.0  \n",
       "2014-01-01  2570.0      351.0  \n",
       "2014-01-02  2013.0      258.0  \n",
       "2014-01-03  2056.0      269.0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean = pre_process_daily_data(path, 'Actual_dt', 'ORA', 'Actual_Value', \n",
    "                               dayfirst=False)\n",
    "clean.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ts_train_test_split(data, split_date):\n",
    "    '''\n",
    "    Split time series into training and test data\n",
    "    \n",
    "    Parameters:\n",
    "    -------\n",
    "    data - pd.DataFrame - time series data.  Index expected as datatimeindex\n",
    "    split_date - the date on which to split the time series\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple (len=2) \n",
    "    0. pandas.DataFrame - training dataset\n",
    "    1. pandas.DataFrame - test dataset\n",
    "    '''\n",
    "    train = data.loc[data.index < split_date]\n",
    "    test = data.loc[data.index >= split_date]\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = ts_train_test_split(clean, split_date=TEST_SPLIT_DATE)\n",
    "\n",
    "#exclude data after 2020 due to coronavirus.\n",
    "test, discard = ts_train_test_split(test, split_date=DISCARD_DATE)\n",
    "\n",
    "#train split into train and validation\n",
    "train, val = ts_train_test_split(train, split_date=VAL_SPLIT_DATE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1279, 9)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#amount of training data\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(549, 9)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#amount of validation data\n",
    "val.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New years day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "exceptional = regular_busy_calender_days(train[REGION], quantile=0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_year = pd.DataFrame({\n",
    "                         'holiday': 'new_year',\n",
    "                         'ds': pd.date_range(start=exceptional[0], \n",
    "                                             periods=20, \n",
    "                                             freq='YS')\n",
    "                        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>holiday</th>\n",
       "      <th>ds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>new_year</td>\n",
       "      <td>2013-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>new_year</td>\n",
       "      <td>2014-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>new_year</td>\n",
       "      <td>2015-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>new_year</td>\n",
       "      <td>2016-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>new_year</td>\n",
       "      <td>2017-01-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    holiday         ds\n",
       "0  new_year 2013-01-01\n",
       "1  new_year 2014-01-01\n",
       "2  new_year 2015-01-01\n",
       "3  new_year 2016-01-01\n",
       "4  new_year 2017-01-01"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_year.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wrapper classes for Prophet and statsmodels ARIMA\n",
    "\n",
    "Adapter/wrapper classes to enable usage within `Ensemble` class and work with cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FbProphetWrapper(object):\n",
    "    '''\n",
    "    Facade for FBProphet object - so that it can be\n",
    "    used within Ensemble with methods from other packages\n",
    "\n",
    "    '''\n",
    "    def __init__(self, training_index, holidays=None, interval_width=0.8,\n",
    "                 mcmc_samples=0, changepoint_prior_scale=0.05):\n",
    "        self._training_index = training_index\n",
    "        self._holidays = holidays\n",
    "        self._interval_width = interval_width\n",
    "        self._mcmc_samples = mcmc_samples\n",
    "        self._cp_prior_scale = changepoint_prior_scale\n",
    "\n",
    "    def _get_resids(self):\n",
    "        return self._train - self._forecast['yhat'][:-self._h]\n",
    "\n",
    "    def _get_preds(self):\n",
    "        return self._forecast['yhat'][:-self._h].to_numpy()\n",
    "\n",
    "    def fit(self, train):\n",
    "        \n",
    "        self._model = Prophet(holidays=self._holidays, \n",
    "                              interval_width=self._interval_width,\n",
    "                              mcmc_samples=self._mcmc_samples,\n",
    "                              changepoint_prior_scale=self._cp_prior_scale,\n",
    "                              daily_seasonality=False)\n",
    "        \n",
    "        \n",
    "        self._model.fit(self._pre_process_training(train))\n",
    "        self._t = len(train)\n",
    "        self._train = train\n",
    "        self.predict(len(train))\n",
    "\n",
    "    def _pre_process_training(self, train):\n",
    "\n",
    "        if len(train.shape) > 1:\n",
    "            y_train = train[:, 0]\n",
    "        else:\n",
    "            y_train = train\n",
    "\n",
    "        y_train = np.asarray(y_train)\n",
    "            \n",
    "        #extend the training index\n",
    "        if len(y_train) > len(self._training_index):\n",
    "            self._training_index = pd.date_range(start=self._training_index[0], \n",
    "                                                 periods=len(y_train),\n",
    "                                                 freq=self._training_index.freq)\n",
    "        \n",
    "        \n",
    "        prophet_train = pd.DataFrame(self._training_index)\n",
    "        prophet_train['y'] = y_train\n",
    "        prophet_train.columns = ['ds', 'y']\n",
    "        \n",
    "        return prophet_train\n",
    "\n",
    "    def predict(self, h, return_conf_int=False, alpha=0.2):\n",
    "        '''\n",
    "        forecast h steps ahead.\n",
    "        \n",
    "        Params:\n",
    "        ------\n",
    "        h: int\n",
    "            h-step forecast\n",
    "        \n",
    "        return_conf_int: bool, optional (default=False)\n",
    "            return 1 - alpha PI\n",
    "        \n",
    "        alpha: float, optional (default=0.2)\n",
    "            return 1 - alpha PI\n",
    "                       \n",
    "        Returns:\n",
    "        -------\n",
    "        np.array\n",
    "            If return_conf_int = False returns preds only\n",
    "            \n",
    "        np.array, np.array\n",
    "            If return_conf_int = True returns tuple of preds, pred_ints\n",
    "        '''\n",
    "        if isinstance(h, (np.ndarray, pd.DataFrame)):\n",
    "            h = len(h)\n",
    "        \n",
    "        self._h = h\n",
    "        future = self._model.make_future_dataframe(periods=h)\n",
    "        self._forecast = self._model.predict(future)\n",
    "\n",
    "        if return_conf_int:\n",
    "            return (self._forecast['yhat'][-h:].to_numpy(), \n",
    "                    self._forecast[['yhat_lower', 'yhat_upper']][-h:].to_numpy())\n",
    "        else:\n",
    "            return self._forecast['yhat'][-h:].to_numpy()\n",
    "            \n",
    "\n",
    "    fittedvalues = property(_get_preds)\n",
    "    resid = property(_get_resids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StateSpaceARIMA(object):\n",
    "    '''\n",
    "    Facade for statsmodels.statespace\n",
    "    '''\n",
    "    def __init__(self, order, seasonal_order, training_index, holidays=None):\n",
    "        self._order = order\n",
    "        self._seasonal_order = seasonal_order\n",
    "        self._training_index = training_index\n",
    "        self._holidays = holidays\n",
    "\n",
    "    def _get_resids(self):\n",
    "        return self._fitted.resid\n",
    "\n",
    "    def _get_preds(self):\n",
    "        return self._fitted.fittedvalues\n",
    "    \n",
    "    def _encode_holidays(self, holidays, idx):\n",
    "        dummy = idx.isin(holidays).astype(int)\n",
    "        dummy = pd.DataFrame(dummy)\n",
    "        dummy.columns = ['holiday']\n",
    "        dummy.index = idx\n",
    "        return dummy\n",
    "\n",
    "    def fit(self, y_train):\n",
    "        \n",
    "        #extend training index\n",
    "        if len(y_train) > len(self._training_index):\n",
    "\n",
    "            self._training_index = pd.date_range(start=self._training_index[0], \n",
    "                                                 periods=len(y_train),\n",
    "                                                 freq=self._training_index.freq)\n",
    "            \n",
    "        holiday_train = None\n",
    "        if not self._holidays is None:\n",
    "            holiday_train = self._encode_holidays(self._holidays, \n",
    "                                                  self._training_index)\n",
    "    \n",
    "        \n",
    "        self._model = ARIMA(endog=y_train,\n",
    "                            exog=holiday_train,\n",
    "                            order=self._order, \n",
    "                            seasonal_order=self._seasonal_order)#,\n",
    "                            #enforce_stationarity=False)\n",
    "        \n",
    "        self._fitted = self._model.fit()\n",
    "        self._t = len(train)\n",
    "        \n",
    "    \n",
    "    def predict(self, horizon, return_conf_int=False, alpha=0.2):\n",
    "        '''\n",
    "        forecast h steps ahead.\n",
    "        \n",
    "        Params:\n",
    "        ------\n",
    "        h: int\n",
    "            h-step forecast\n",
    "        \n",
    "        return_conf_int: bool, optional (default=False)\n",
    "            return 1 - alpha PI\n",
    "        \n",
    "        alpha: float, optional (default=0.2)\n",
    "            return 1 - alpha PI\n",
    "                       \n",
    "        Returns:\n",
    "        -------\n",
    "        np.array\n",
    "            If return_conf_int = False returns preds only\n",
    "            \n",
    "        np.array, np.array\n",
    "            If return_conf_int = True returns tuple of preds, pred_ints\n",
    "        '''\n",
    "        \n",
    "        #+1 to date range then trim off the first value\n",
    "\n",
    "        f_idx = pd.date_range(start=self._training_index[-1], \n",
    "                              periods=horizon+1,\n",
    "                              freq=self._training_index.freq)[1:]\n",
    "        \n",
    "        #encode holidays if included.\n",
    "        exog_holiday = None\n",
    "        if not self._holidays is None:\n",
    "            exog_holiday = self._encode_holidays(self._holidays, f_idx)\n",
    "        \n",
    "    \n",
    "        forecast = self._fitted.get_forecast(horizon, exog=exog_holiday)\n",
    "        mean_forecast = forecast.summary_frame()['mean'].to_numpy()\n",
    "        \n",
    "        if return_conf_int:\n",
    "            df = forecast.summary_frame(alpha=alpha)\n",
    "            pi = df[['mean_ci_lower', 'mean_ci_upper']].to_numpy()\n",
    "            return mean_forecast, pi\n",
    "            \n",
    "        \n",
    "        else:\n",
    "            return mean_forecast\n",
    "\n",
    "    fittedvalues = property(_get_preds)\n",
    "    resid = property(_get_resids)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example of fitting the ensemble\n",
    "1. Regression with New Year Holiday and Auto ARIMA errors\n",
    "2. FBProphet with new years day holiday.\n",
    "\n",
    "The code below demonstrates how to fit the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1 = StateSpaceARIMA(order=(1,1,3), seasonal_order=(1,0,1,7), \n",
    "                          training_index=train.index,\n",
    "                          holidays=new_year['ds'].tolist())\n",
    "\n",
    "model_2 = FbProphetWrapper(training_index=train.index, \n",
    "                           holidays=new_year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimators = {'arima': model_1, 'fbp': model_2}\n",
    "ens = Ensemble(estimators, UnweightedVote())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit to training data in chosen region\n",
    "ens.fit(train[REGION])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict 7 days ahead\n",
    "H = 7\n",
    "ens_preds = ens.predict(horizon=H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2290.40779838, 2263.27772752, 2164.9636996 , 2090.13940692,\n",
       "       2085.71037951, 2103.43703937, 2150.59791813])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#view predictions\n",
    "ens_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with prediction intervals\n",
    "ens_preds, pi = ens.predict(horizon=H, return_conf_int=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2290.40779838, 2263.27772752, 2164.9636996 , 2090.13940692,\n",
       "       2085.71037951, 2103.43703937, 2150.59791813])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ens_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2192.85853961, 2384.46925144],\n",
       "       [2166.41304197, 2363.10771756],\n",
       "       [2067.46956434, 2266.16699598],\n",
       "       [1987.13386077, 2193.06241133],\n",
       "       [1982.74950146, 2191.56068841],\n",
       "       [1997.99161196, 2206.46546462],\n",
       "       [2045.93987287, 2251.79056048]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross validation functions\n",
    "\n",
    "`time_series_cv` implements rolling forecast origin cross validation for time series.  \n",
    "It does not calculate forecast error, but instead returns the predictions, pred intervals and actuals in an array that can be passed to any forecast error function. (this is for efficiency and allows additional metrics to be calculated if needed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_series_cv(model, train, val, horizons, alpha=0.2, step=1):\n",
    "    '''\n",
    "    Time series cross validation across multiple horizons for a single model.\n",
    "\n",
    "    Incrementally adds additional training data to the model and tests\n",
    "    across a provided list of forecast horizons. Note that function tests a\n",
    "    model only against complete validation sets.  E.g. if horizon = 15 and \n",
    "    len(val) = 12 then no testing is done.  In the case of multiple horizons\n",
    "    e.g. [7, 14, 28] then the function will use the maximum forecast horizon\n",
    "    to calculate the number of iterations i.e if len(val) = 365 and step = 1\n",
    "    then no. iterations = len(val) - max(horizon) = 365 - 28 = 337.\n",
    "    \n",
    "    Parameters:\n",
    "    --------\n",
    "    model - forecasting model\n",
    "\n",
    "    train - np.array - vector of training data\n",
    "\n",
    "    val - np.array - vector of validation data\n",
    "\n",
    "    horizon - list of ints, forecast horizon e.g. [7, 14, 28] days\n",
    "    \n",
    "    alpha - float, optional (default=0.2)\n",
    "        1 - alpha prediction interval specification\n",
    "\n",
    "    step -- int, optional (default=1)\n",
    "            step taken in cross validation \n",
    "            e.g. 1 in next cross validation training data includes next point \n",
    "            from the validation set.\n",
    "            e.g. 7 in the next cross validation training data includes next 7 points\n",
    "            (default=1)\n",
    "            \n",
    "    Returns:\n",
    "    -------\n",
    "    np.array, np.array, np.array\n",
    "        - cv_preds, cv_test, cv_intervals\n",
    "    '''\n",
    "    \n",
    "    #point forecasts\n",
    "    cv_preds = [] \n",
    "    #ground truth observations\n",
    "    cv_actuals = [] \n",
    "    #prediction intervals\n",
    "    cv_pis = []\n",
    "    \n",
    "    split = 0\n",
    "\n",
    "    print('split => ', end=\"\")\n",
    "    for i in range(0, len(val) - max(horizons) + 1, step):\n",
    "        split += 1\n",
    "        print(f'{split}, ', end=\"\")\n",
    "                \n",
    "        train_cv = np.concatenate([train, val[:i]], axis=0)\n",
    "        model.fit(train_cv)\n",
    "        \n",
    "        #predict the maximum horizon \n",
    "        preds, pis = model.predict(horizon=len(val[i:i+max(horizons)]), \n",
    "                                   return_conf_int=True,\n",
    "                                   alpha=alpha)        \n",
    "        cv_h_preds = []\n",
    "        cv_test = []\n",
    "        cv_h_pis = []\n",
    "        \n",
    "        #sub horizon calculations\n",
    "        for h in horizons:\n",
    "            #store the h-step prediction\n",
    "            cv_h_preds.append(preds[:h])\n",
    "            #store the h-step actual value\n",
    "            cv_test.append(val.iloc[i:i+h])    \n",
    "            cv_h_pis.append(pis[:h])\n",
    "                     \n",
    "        cv_preds.append(cv_h_preds)\n",
    "        cv_actuals.append(cv_test)\n",
    "        cv_pis.append(cv_h_pis)\n",
    "        \n",
    "    print('done.\\n')        \n",
    "    return cv_preds, cv_actuals, cv_pis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom functions for calculating CV scores for point predictions and coverage.\n",
    "\n",
    "These functions have been written to work with the output of `time_series_cv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_cv_error(cv_preds, cv_test, error_func):\n",
    "    '''\n",
    "    Forecast error in the current split\n",
    "    \n",
    "    Params:\n",
    "    -----\n",
    "    cv_preds, np.array\n",
    "        Split predictions\n",
    "        \n",
    "    \n",
    "    cv_test: np.array\n",
    "        acutal ground truth observations\n",
    "        \n",
    "    error_func: object\n",
    "        function with signature (y_true, y_preds)\n",
    "        \n",
    "    Returns:\n",
    "    -------\n",
    "        np.ndarray\n",
    "            cross validation errors for split\n",
    "    '''\n",
    "    n_splits = len(cv_preds)\n",
    "    cv_errors = []\n",
    "    \n",
    "    for split in range(n_splits):\n",
    "        pred_error = error_func(cv_test[split], cv_preds[split])\n",
    "        cv_errors.append(pred_error)\n",
    "        \n",
    "    return np.array(cv_errors)\n",
    "\n",
    "def forecast_errors_cv(cv_preds, cv_test, error_func):\n",
    "    '''\n",
    "    Forecast errors by forecast horizon\n",
    "    \n",
    "    Params:\n",
    "    ------\n",
    "    cv_preds: np.ndarray\n",
    "        Array of arrays.  Each array is of size h representing\n",
    "        the forecast horizon specified.\n",
    "        \n",
    "    cv_test: np.ndarray\n",
    "        Array of arrays.  Each array is of size h representing\n",
    "        the forecast horizon specified.\n",
    "        \n",
    "    error_func: object\n",
    "        function with signature (y_true, y_preds)\n",
    "        \n",
    "    Returns:\n",
    "    -------\n",
    "    np.ndarray\n",
    "        \n",
    "    '''\n",
    "    cv_test = np.array(cv_test)\n",
    "    cv_preds = np.array(cv_preds)\n",
    "    n_horizons = len(cv_test)    \n",
    "    \n",
    "    horizon_errors = []\n",
    "    for h in range(n_horizons):\n",
    "        split_errors = split_cv_error(cv_preds[h], cv_test[h], error_func)\n",
    "        horizon_errors.append(split_errors)\n",
    "\n",
    "    return np.array(horizon_errors)\n",
    "\n",
    "def split_coverage(cv_test, cv_intervals):\n",
    "    n_splits = len(cv_test)\n",
    "    cv_errors = []\n",
    "        \n",
    "    for split in range(n_splits):\n",
    "        val = np.asarray(cv_test[split])\n",
    "        lower = cv_intervals[split].T[0]\n",
    "        upper = cv_intervals[split].T[1]\n",
    "        \n",
    "        coverage = len(np.where((val > lower) & (val < upper))[0])\n",
    "        coverage = coverage / len(val)\n",
    "        \n",
    "        cv_errors.append(coverage)\n",
    "        \n",
    "    return np.array(cv_errors)\n",
    "    \n",
    "    \n",
    "def prediction_int_coverage_cv(cv_test, cv_intervals):\n",
    "    cv_test = np.array(cv_test)\n",
    "    cv_intervals = np.array(cv_intervals)\n",
    "    n_horizons = len(cv_test)    \n",
    "    \n",
    "    horizon_coverage = []\n",
    "    for h in range(n_horizons):\n",
    "        split_coverages = split_coverage(cv_test[h], cv_intervals[h])\n",
    "        horizon_coverage.append(split_coverages)\n",
    "\n",
    "    return np.array(horizon_coverage)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_cv_error_scaled(cv_preds, cv_test, y_train):\n",
    "    n_splits = len(cv_preds)\n",
    "    cv_errors = []\n",
    "    \n",
    "    for split in range(n_splits):\n",
    "        pred_error = mean_absolute_scaled_error(cv_test[split], cv_preds[split], \n",
    "                                                y_train, period=7)\n",
    "        \n",
    "        cv_errors.append(pred_error)\n",
    "        \n",
    "    return np.array(cv_errors)\n",
    "\n",
    "def forecast_errors_cv_scaled(cv_preds, cv_test, y_train):\n",
    "    cv_test = np.array(cv_test)\n",
    "    cv_preds = np.array(cv_preds)\n",
    "    n_horizons = len(cv_test)    \n",
    "    \n",
    "    horizon_errors = []\n",
    "    for h in range(n_horizons):\n",
    "        split_errors = split_cv_error_scaled(cv_preds[h], cv_test[h], y_train)\n",
    "        horizon_errors.append(split_errors)\n",
    "        \n",
    "    return np.array(horizon_errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ensemble(meta_learner=None, fb_interval=0.8):\n",
    "    '''\n",
    "    Create ensemble model\n",
    "    '''\n",
    "    if meta_learner is None:\n",
    "        meta_learner = UnweightedVote()\n",
    "        \n",
    "    model_1 = StateSpaceARIMA(order=(1,1,3), seasonal_order=(1,0,1,7), \n",
    "                          training_index=train.index,\n",
    "                          holidays=new_year['ds'].tolist())\n",
    "        \n",
    "    model_2 = FbProphetWrapper(training_index=train.index, \n",
    "                           holidays=new_year, interval_width=fb_interval)\n",
    "    \n",
    "    estimators = {'arima': model_1, 'fbp': model_2}\n",
    "    return Ensemble(estimators, UnweightedVote())\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run cross validation\n",
    "\n",
    "This is run twices once each for 80 and 95% prediction intervals.  The 2nd run is required due to the way Prophet generates prediction intervals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "split => 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, done.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "horizons = [7, 14, 21, 28, 35, 42, 49, 56, 63, 70, 77, 84, 365]\n",
    "model = get_ensemble()\n",
    "\n",
    "results = time_series_cv(model, train[REGION], val[REGION], horizons, \n",
    "                         alpha=0.2, step=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# symmetric MAPE results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>7</th>\n",
       "      <th>14</th>\n",
       "      <th>21</th>\n",
       "      <th>28</th>\n",
       "      <th>35</th>\n",
       "      <th>42</th>\n",
       "      <th>49</th>\n",
       "      <th>56</th>\n",
       "      <th>63</th>\n",
       "      <th>70</th>\n",
       "      <th>77</th>\n",
       "      <th>84</th>\n",
       "      <th>365</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2.888748</td>\n",
       "      <td>3.069071</td>\n",
       "      <td>3.142560</td>\n",
       "      <td>3.181019</td>\n",
       "      <td>3.243412</td>\n",
       "      <td>3.307821</td>\n",
       "      <td>3.363752</td>\n",
       "      <td>3.419441</td>\n",
       "      <td>3.480343</td>\n",
       "      <td>3.522194</td>\n",
       "      <td>3.563835</td>\n",
       "      <td>3.628557</td>\n",
       "      <td>4.623541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.154105</td>\n",
       "      <td>1.078228</td>\n",
       "      <td>0.854233</td>\n",
       "      <td>0.628019</td>\n",
       "      <td>0.546131</td>\n",
       "      <td>0.517146</td>\n",
       "      <td>0.488710</td>\n",
       "      <td>0.440075</td>\n",
       "      <td>0.420162</td>\n",
       "      <td>0.391239</td>\n",
       "      <td>0.393037</td>\n",
       "      <td>0.456654</td>\n",
       "      <td>1.178746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.140628</td>\n",
       "      <td>1.545419</td>\n",
       "      <td>2.008209</td>\n",
       "      <td>2.256095</td>\n",
       "      <td>2.472786</td>\n",
       "      <td>2.592505</td>\n",
       "      <td>2.569449</td>\n",
       "      <td>2.657952</td>\n",
       "      <td>2.781615</td>\n",
       "      <td>2.987487</td>\n",
       "      <td>2.949513</td>\n",
       "      <td>2.996825</td>\n",
       "      <td>3.301535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.940003</td>\n",
       "      <td>2.344569</td>\n",
       "      <td>2.478941</td>\n",
       "      <td>2.669560</td>\n",
       "      <td>2.780674</td>\n",
       "      <td>2.965035</td>\n",
       "      <td>3.008267</td>\n",
       "      <td>3.111241</td>\n",
       "      <td>3.252026</td>\n",
       "      <td>3.207608</td>\n",
       "      <td>3.217273</td>\n",
       "      <td>3.211276</td>\n",
       "      <td>3.685045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2.542677</td>\n",
       "      <td>2.782540</td>\n",
       "      <td>2.902718</td>\n",
       "      <td>3.019332</td>\n",
       "      <td>3.047905</td>\n",
       "      <td>3.221637</td>\n",
       "      <td>3.363829</td>\n",
       "      <td>3.410275</td>\n",
       "      <td>3.329360</td>\n",
       "      <td>3.411234</td>\n",
       "      <td>3.493673</td>\n",
       "      <td>3.659688</td>\n",
       "      <td>4.564660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3.678402</td>\n",
       "      <td>3.799912</td>\n",
       "      <td>3.474135</td>\n",
       "      <td>3.672439</td>\n",
       "      <td>3.771205</td>\n",
       "      <td>3.690691</td>\n",
       "      <td>3.701352</td>\n",
       "      <td>3.677538</td>\n",
       "      <td>3.761878</td>\n",
       "      <td>3.875395</td>\n",
       "      <td>3.882196</td>\n",
       "      <td>3.908651</td>\n",
       "      <td>4.901565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>5.458276</td>\n",
       "      <td>5.681857</td>\n",
       "      <td>4.935363</td>\n",
       "      <td>4.519707</td>\n",
       "      <td>4.289216</td>\n",
       "      <td>4.355061</td>\n",
       "      <td>4.352985</td>\n",
       "      <td>4.357056</td>\n",
       "      <td>4.377883</td>\n",
       "      <td>4.263073</td>\n",
       "      <td>4.380400</td>\n",
       "      <td>4.525349</td>\n",
       "      <td>8.945251</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             7          14         21         28         35         42   \\\n",
       "count  27.000000  27.000000  27.000000  27.000000  27.000000  27.000000   \n",
       "mean    2.888748   3.069071   3.142560   3.181019   3.243412   3.307821   \n",
       "std     1.154105   1.078228   0.854233   0.628019   0.546131   0.517146   \n",
       "min     1.140628   1.545419   2.008209   2.256095   2.472786   2.592505   \n",
       "25%     1.940003   2.344569   2.478941   2.669560   2.780674   2.965035   \n",
       "50%     2.542677   2.782540   2.902718   3.019332   3.047905   3.221637   \n",
       "75%     3.678402   3.799912   3.474135   3.672439   3.771205   3.690691   \n",
       "max     5.458276   5.681857   4.935363   4.519707   4.289216   4.355061   \n",
       "\n",
       "             49         56         63         70         77         84   \\\n",
       "count  27.000000  27.000000  27.000000  27.000000  27.000000  27.000000   \n",
       "mean    3.363752   3.419441   3.480343   3.522194   3.563835   3.628557   \n",
       "std     0.488710   0.440075   0.420162   0.391239   0.393037   0.456654   \n",
       "min     2.569449   2.657952   2.781615   2.987487   2.949513   2.996825   \n",
       "25%     3.008267   3.111241   3.252026   3.207608   3.217273   3.211276   \n",
       "50%     3.363829   3.410275   3.329360   3.411234   3.493673   3.659688   \n",
       "75%     3.701352   3.677538   3.761878   3.875395   3.882196   3.908651   \n",
       "max     4.352985   4.357056   4.377883   4.263073   4.380400   4.525349   \n",
       "\n",
       "             365  \n",
       "count  27.000000  \n",
       "mean    4.623541  \n",
       "std     1.178746  \n",
       "min     3.301535  \n",
       "25%     3.685045  \n",
       "50%     4.564660  \n",
       "75%     4.901565  \n",
       "max     8.945251  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_preds, cv_test, cv_intervals = results\n",
    "cv_errors = forecast_errors_cv(cv_preds, cv_test, \n",
    "                               symmetric_mean_absolute_percentage_error)\n",
    "df = pd.DataFrame(cv_errors)\n",
    "df.columns = horizons\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../../results/model_selection/temp/Trust-fbp-arima_smape.csv\n"
     ]
    }
   ],
   "source": [
    "#output sMAPE results to file\n",
    "metric = 'smape'\n",
    "print(f'{TOP_LEVEL}/{STAGE}/{REGION}-{METHOD}_{metric}.csv')\n",
    "df.to_csv(f'{TOP_LEVEL}/{STAGE}/{REGION}-{METHOD}_{metric}.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RMSE results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>7</th>\n",
       "      <th>14</th>\n",
       "      <th>21</th>\n",
       "      <th>28</th>\n",
       "      <th>35</th>\n",
       "      <th>42</th>\n",
       "      <th>49</th>\n",
       "      <th>56</th>\n",
       "      <th>63</th>\n",
       "      <th>70</th>\n",
       "      <th>77</th>\n",
       "      <th>84</th>\n",
       "      <th>365</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>76.322438</td>\n",
       "      <td>82.961792</td>\n",
       "      <td>87.044831</td>\n",
       "      <td>89.581903</td>\n",
       "      <td>92.174299</td>\n",
       "      <td>94.456973</td>\n",
       "      <td>96.489349</td>\n",
       "      <td>98.388413</td>\n",
       "      <td>100.425831</td>\n",
       "      <td>101.982125</td>\n",
       "      <td>103.442803</td>\n",
       "      <td>105.590240</td>\n",
       "      <td>129.442085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>31.861061</td>\n",
       "      <td>31.404482</td>\n",
       "      <td>27.738723</td>\n",
       "      <td>24.119081</td>\n",
       "      <td>22.037625</td>\n",
       "      <td>20.510388</td>\n",
       "      <td>19.224001</td>\n",
       "      <td>17.644049</td>\n",
       "      <td>15.985038</td>\n",
       "      <td>13.918642</td>\n",
       "      <td>12.484009</td>\n",
       "      <td>12.640840</td>\n",
       "      <td>27.763381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>33.998821</td>\n",
       "      <td>42.558074</td>\n",
       "      <td>55.175184</td>\n",
       "      <td>60.135793</td>\n",
       "      <td>66.126028</td>\n",
       "      <td>70.221792</td>\n",
       "      <td>71.485611</td>\n",
       "      <td>73.468009</td>\n",
       "      <td>74.251548</td>\n",
       "      <td>79.835318</td>\n",
       "      <td>79.328734</td>\n",
       "      <td>84.795769</td>\n",
       "      <td>99.052419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>52.443934</td>\n",
       "      <td>61.348069</td>\n",
       "      <td>69.145384</td>\n",
       "      <td>72.332606</td>\n",
       "      <td>75.969840</td>\n",
       "      <td>77.221799</td>\n",
       "      <td>79.501797</td>\n",
       "      <td>83.323549</td>\n",
       "      <td>87.298849</td>\n",
       "      <td>90.590713</td>\n",
       "      <td>93.818806</td>\n",
       "      <td>98.453261</td>\n",
       "      <td>107.556015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>70.422038</td>\n",
       "      <td>75.456617</td>\n",
       "      <td>79.419141</td>\n",
       "      <td>82.929364</td>\n",
       "      <td>83.921992</td>\n",
       "      <td>86.082014</td>\n",
       "      <td>98.161349</td>\n",
       "      <td>100.391861</td>\n",
       "      <td>101.743252</td>\n",
       "      <td>104.113577</td>\n",
       "      <td>103.966390</td>\n",
       "      <td>105.484323</td>\n",
       "      <td>128.492144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>90.927909</td>\n",
       "      <td>93.388844</td>\n",
       "      <td>91.505616</td>\n",
       "      <td>101.538721</td>\n",
       "      <td>105.289861</td>\n",
       "      <td>109.104326</td>\n",
       "      <td>114.036727</td>\n",
       "      <td>111.945535</td>\n",
       "      <td>108.595558</td>\n",
       "      <td>110.009469</td>\n",
       "      <td>111.805180</td>\n",
       "      <td>115.665794</td>\n",
       "      <td>135.595638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>167.012731</td>\n",
       "      <td>180.920355</td>\n",
       "      <td>158.385140</td>\n",
       "      <td>142.843182</td>\n",
       "      <td>137.490799</td>\n",
       "      <td>134.245967</td>\n",
       "      <td>131.369623</td>\n",
       "      <td>137.700572</td>\n",
       "      <td>133.089819</td>\n",
       "      <td>127.728633</td>\n",
       "      <td>124.505023</td>\n",
       "      <td>133.017921</td>\n",
       "      <td>233.149446</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              7           14          21          28          35          42   \\\n",
       "count   27.000000   27.000000   27.000000   27.000000   27.000000   27.000000   \n",
       "mean    76.322438   82.961792   87.044831   89.581903   92.174299   94.456973   \n",
       "std     31.861061   31.404482   27.738723   24.119081   22.037625   20.510388   \n",
       "min     33.998821   42.558074   55.175184   60.135793   66.126028   70.221792   \n",
       "25%     52.443934   61.348069   69.145384   72.332606   75.969840   77.221799   \n",
       "50%     70.422038   75.456617   79.419141   82.929364   83.921992   86.082014   \n",
       "75%     90.927909   93.388844   91.505616  101.538721  105.289861  109.104326   \n",
       "max    167.012731  180.920355  158.385140  142.843182  137.490799  134.245967   \n",
       "\n",
       "              49          56          63          70          77          84   \\\n",
       "count   27.000000   27.000000   27.000000   27.000000   27.000000   27.000000   \n",
       "mean    96.489349   98.388413  100.425831  101.982125  103.442803  105.590240   \n",
       "std     19.224001   17.644049   15.985038   13.918642   12.484009   12.640840   \n",
       "min     71.485611   73.468009   74.251548   79.835318   79.328734   84.795769   \n",
       "25%     79.501797   83.323549   87.298849   90.590713   93.818806   98.453261   \n",
       "50%     98.161349  100.391861  101.743252  104.113577  103.966390  105.484323   \n",
       "75%    114.036727  111.945535  108.595558  110.009469  111.805180  115.665794   \n",
       "max    131.369623  137.700572  133.089819  127.728633  124.505023  133.017921   \n",
       "\n",
       "              365  \n",
       "count   27.000000  \n",
       "mean   129.442085  \n",
       "std     27.763381  \n",
       "min     99.052419  \n",
       "25%    107.556015  \n",
       "50%    128.492144  \n",
       "75%    135.595638  \n",
       "max    233.149446  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_preds, cv_test, cv_intervals = results\n",
    "cv_errors = forecast_errors_cv(cv_preds, cv_test, root_mean_squared_error)\n",
    "df = pd.DataFrame(cv_errors)\n",
    "df.columns = horizons\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../../results/model_selection/temp/Trust-fbp-arima_rmse.csv\n"
     ]
    }
   ],
   "source": [
    "#output RMSE to file\n",
    "metric = 'rmse'\n",
    "print(f'{TOP_LEVEL}/{STAGE}/{REGION}-{METHOD}_{metric}.csv')\n",
    "df.to_csv(f'{TOP_LEVEL}/{STAGE}/{REGION}-{METHOD}_{metric}.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mean Absolute Scaled Error (MASE)\n",
    "\n",
    "Scaled by one-step insample Seasonal Naive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>7</th>\n",
       "      <th>14</th>\n",
       "      <th>21</th>\n",
       "      <th>28</th>\n",
       "      <th>35</th>\n",
       "      <th>42</th>\n",
       "      <th>49</th>\n",
       "      <th>56</th>\n",
       "      <th>63</th>\n",
       "      <th>70</th>\n",
       "      <th>77</th>\n",
       "      <th>84</th>\n",
       "      <th>365</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.779735</td>\n",
       "      <td>0.829899</td>\n",
       "      <td>0.850839</td>\n",
       "      <td>0.862184</td>\n",
       "      <td>0.880112</td>\n",
       "      <td>0.898571</td>\n",
       "      <td>0.914623</td>\n",
       "      <td>0.930381</td>\n",
       "      <td>0.947574</td>\n",
       "      <td>0.959693</td>\n",
       "      <td>0.971735</td>\n",
       "      <td>0.990095</td>\n",
       "      <td>1.256874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.338296</td>\n",
       "      <td>0.319189</td>\n",
       "      <td>0.258226</td>\n",
       "      <td>0.197235</td>\n",
       "      <td>0.174982</td>\n",
       "      <td>0.165175</td>\n",
       "      <td>0.155135</td>\n",
       "      <td>0.139770</td>\n",
       "      <td>0.130419</td>\n",
       "      <td>0.117955</td>\n",
       "      <td>0.112327</td>\n",
       "      <td>0.126001</td>\n",
       "      <td>0.334113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.327196</td>\n",
       "      <td>0.443398</td>\n",
       "      <td>0.571509</td>\n",
       "      <td>0.636428</td>\n",
       "      <td>0.691173</td>\n",
       "      <td>0.664678</td>\n",
       "      <td>0.657033</td>\n",
       "      <td>0.679980</td>\n",
       "      <td>0.730807</td>\n",
       "      <td>0.768810</td>\n",
       "      <td>0.761729</td>\n",
       "      <td>0.818899</td>\n",
       "      <td>0.899054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.531322</td>\n",
       "      <td>0.627220</td>\n",
       "      <td>0.673955</td>\n",
       "      <td>0.719479</td>\n",
       "      <td>0.738609</td>\n",
       "      <td>0.768642</td>\n",
       "      <td>0.798454</td>\n",
       "      <td>0.817743</td>\n",
       "      <td>0.860626</td>\n",
       "      <td>0.877523</td>\n",
       "      <td>0.890573</td>\n",
       "      <td>0.886508</td>\n",
       "      <td>1.002215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.689467</td>\n",
       "      <td>0.732499</td>\n",
       "      <td>0.773683</td>\n",
       "      <td>0.786387</td>\n",
       "      <td>0.833724</td>\n",
       "      <td>0.834633</td>\n",
       "      <td>0.931070</td>\n",
       "      <td>0.950313</td>\n",
       "      <td>0.945777</td>\n",
       "      <td>0.935142</td>\n",
       "      <td>0.962758</td>\n",
       "      <td>1.004145</td>\n",
       "      <td>1.238640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.962099</td>\n",
       "      <td>0.999292</td>\n",
       "      <td>0.921281</td>\n",
       "      <td>0.961827</td>\n",
       "      <td>1.019570</td>\n",
       "      <td>1.064656</td>\n",
       "      <td>1.051246</td>\n",
       "      <td>1.018840</td>\n",
       "      <td>1.019162</td>\n",
       "      <td>1.029213</td>\n",
       "      <td>1.067611</td>\n",
       "      <td>1.043209</td>\n",
       "      <td>1.329126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.692838</td>\n",
       "      <td>1.725855</td>\n",
       "      <td>1.465348</td>\n",
       "      <td>1.324131</td>\n",
       "      <td>1.226330</td>\n",
       "      <td>1.240197</td>\n",
       "      <td>1.234702</td>\n",
       "      <td>1.248874</td>\n",
       "      <td>1.234477</td>\n",
       "      <td>1.203657</td>\n",
       "      <td>1.160548</td>\n",
       "      <td>1.243871</td>\n",
       "      <td>2.510996</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             7          14         21         28         35         42   \\\n",
       "count  27.000000  27.000000  27.000000  27.000000  27.000000  27.000000   \n",
       "mean    0.779735   0.829899   0.850839   0.862184   0.880112   0.898571   \n",
       "std     0.338296   0.319189   0.258226   0.197235   0.174982   0.165175   \n",
       "min     0.327196   0.443398   0.571509   0.636428   0.691173   0.664678   \n",
       "25%     0.531322   0.627220   0.673955   0.719479   0.738609   0.768642   \n",
       "50%     0.689467   0.732499   0.773683   0.786387   0.833724   0.834633   \n",
       "75%     0.962099   0.999292   0.921281   0.961827   1.019570   1.064656   \n",
       "max     1.692838   1.725855   1.465348   1.324131   1.226330   1.240197   \n",
       "\n",
       "             49         56         63         70         77         84   \\\n",
       "count  27.000000  27.000000  27.000000  27.000000  27.000000  27.000000   \n",
       "mean    0.914623   0.930381   0.947574   0.959693   0.971735   0.990095   \n",
       "std     0.155135   0.139770   0.130419   0.117955   0.112327   0.126001   \n",
       "min     0.657033   0.679980   0.730807   0.768810   0.761729   0.818899   \n",
       "25%     0.798454   0.817743   0.860626   0.877523   0.890573   0.886508   \n",
       "50%     0.931070   0.950313   0.945777   0.935142   0.962758   1.004145   \n",
       "75%     1.051246   1.018840   1.019162   1.029213   1.067611   1.043209   \n",
       "max     1.234702   1.248874   1.234477   1.203657   1.160548   1.243871   \n",
       "\n",
       "             365  \n",
       "count  27.000000  \n",
       "mean    1.256874  \n",
       "std     0.334113  \n",
       "min     0.899054  \n",
       "25%     1.002215  \n",
       "50%     1.238640  \n",
       "75%     1.329126  \n",
       "max     2.510996  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_errors = forecast_errors_cv_scaled(cv_preds, cv_test, train[REGION])\n",
    "df = pd.DataFrame(cv_errors)\n",
    "df.columns = horizons\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../../results/model_selection/temp/Trust-fbp-arima_mase.csv\n"
     ]
    }
   ],
   "source": [
    "#output mase to file.\n",
    "metric = 'mase'\n",
    "print(f'{TOP_LEVEL}/{STAGE}/{REGION}-{METHOD}_{metric}.csv')\n",
    "df.to_csv(f'{TOP_LEVEL}/{STAGE}/{REGION}-{METHOD}_{metric}.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 80% Prediction Interval Coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>7</th>\n",
       "      <th>14</th>\n",
       "      <th>21</th>\n",
       "      <th>28</th>\n",
       "      <th>35</th>\n",
       "      <th>42</th>\n",
       "      <th>49</th>\n",
       "      <th>56</th>\n",
       "      <th>63</th>\n",
       "      <th>70</th>\n",
       "      <th>77</th>\n",
       "      <th>84</th>\n",
       "      <th>365</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.788360</td>\n",
       "      <td>0.775132</td>\n",
       "      <td>0.776014</td>\n",
       "      <td>0.781746</td>\n",
       "      <td>0.774603</td>\n",
       "      <td>0.768078</td>\n",
       "      <td>0.764928</td>\n",
       "      <td>0.761905</td>\n",
       "      <td>0.753674</td>\n",
       "      <td>0.747090</td>\n",
       "      <td>0.740741</td>\n",
       "      <td>0.731922</td>\n",
       "      <td>0.718214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.214726</td>\n",
       "      <td>0.181248</td>\n",
       "      <td>0.141517</td>\n",
       "      <td>0.109109</td>\n",
       "      <td>0.101583</td>\n",
       "      <td>0.097409</td>\n",
       "      <td>0.090275</td>\n",
       "      <td>0.079552</td>\n",
       "      <td>0.077808</td>\n",
       "      <td>0.070300</td>\n",
       "      <td>0.073154</td>\n",
       "      <td>0.085424</td>\n",
       "      <td>0.125958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.357143</td>\n",
       "      <td>0.476190</td>\n",
       "      <td>0.607143</td>\n",
       "      <td>0.542857</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.530612</td>\n",
       "      <td>0.589286</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.585714</td>\n",
       "      <td>0.558442</td>\n",
       "      <td>0.535714</td>\n",
       "      <td>0.293151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.642857</td>\n",
       "      <td>0.642857</td>\n",
       "      <td>0.690476</td>\n",
       "      <td>0.696429</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.738095</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.696429</td>\n",
       "      <td>0.706349</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.681818</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.657534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.809524</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>0.775510</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.757143</td>\n",
       "      <td>0.753247</td>\n",
       "      <td>0.761905</td>\n",
       "      <td>0.679452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.892857</td>\n",
       "      <td>0.880952</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.821429</td>\n",
       "      <td>0.816327</td>\n",
       "      <td>0.803571</td>\n",
       "      <td>0.809524</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.792208</td>\n",
       "      <td>0.809524</td>\n",
       "      <td>0.830137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.964286</td>\n",
       "      <td>0.914286</td>\n",
       "      <td>0.904762</td>\n",
       "      <td>0.918367</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>0.920635</td>\n",
       "      <td>0.842857</td>\n",
       "      <td>0.844156</td>\n",
       "      <td>0.845238</td>\n",
       "      <td>0.879452</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             7          14         21         28         35         42   \\\n",
       "count  27.000000  27.000000  27.000000  27.000000  27.000000  27.000000   \n",
       "mean    0.788360   0.775132   0.776014   0.781746   0.774603   0.768078   \n",
       "std     0.214726   0.181248   0.141517   0.109109   0.101583   0.097409   \n",
       "min     0.285714   0.357143   0.476190   0.607143   0.542857   0.500000   \n",
       "25%     0.642857   0.642857   0.690476   0.696429   0.714286   0.738095   \n",
       "50%     0.857143   0.857143   0.809524   0.785714   0.800000   0.785714   \n",
       "75%     1.000000   0.892857   0.880952   0.857143   0.857143   0.821429   \n",
       "max     1.000000   1.000000   1.000000   0.964286   0.914286   0.904762   \n",
       "\n",
       "             49         56         63         70         77         84   \\\n",
       "count  27.000000  27.000000  27.000000  27.000000  27.000000  27.000000   \n",
       "mean    0.764928   0.761905   0.753674   0.747090   0.740741   0.731922   \n",
       "std     0.090275   0.079552   0.077808   0.070300   0.073154   0.085424   \n",
       "min     0.530612   0.589286   0.555556   0.585714   0.558442   0.535714   \n",
       "25%     0.714286   0.696429   0.706349   0.714286   0.681818   0.666667   \n",
       "50%     0.775510   0.785714   0.777778   0.757143   0.753247   0.761905   \n",
       "75%     0.816327   0.803571   0.809524   0.800000   0.792208   0.809524   \n",
       "max     0.918367   0.928571   0.920635   0.842857   0.844156   0.845238   \n",
       "\n",
       "             365  \n",
       "count  27.000000  \n",
       "mean    0.718214  \n",
       "std     0.125958  \n",
       "min     0.293151  \n",
       "25%     0.657534  \n",
       "50%     0.679452  \n",
       "75%     0.830137  \n",
       "max     0.879452  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#80% PIs\n",
    "cv_coverage = prediction_int_coverage_cv(cv_test, cv_intervals)\n",
    "df = pd.DataFrame(cv_coverage)\n",
    "df.columns = horizons\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../../results/model_selection/temp/Trust-fbp-arima_coverage_80.csv\n"
     ]
    }
   ],
   "source": [
    "#write 80% coverage to file\n",
    "metric = 'coverage_80'\n",
    "print(f'{TOP_LEVEL}/{STAGE}/{REGION}-{METHOD}_{metric}.csv')\n",
    "df.to_csv(f'{TOP_LEVEL}/{STAGE}/{REGION}-{METHOD}_{metric}.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 95% Prediction Interval Coverage\n",
    "\n",
    "Rerun analysis and obtain 95% Prediction intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "split => 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, done.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "horizons = [7, 14, 21, 28, 35, 42, 49, 56, 63, 70, 77, 84, 365]\n",
    "model = get_ensemble(fb_interval=0.95)\n",
    "\n",
    "results = time_series_cv(model, train[REGION], val[REGION], horizons, \n",
    "                         alpha=0.05, step=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>7</th>\n",
       "      <th>14</th>\n",
       "      <th>21</th>\n",
       "      <th>28</th>\n",
       "      <th>35</th>\n",
       "      <th>42</th>\n",
       "      <th>49</th>\n",
       "      <th>56</th>\n",
       "      <th>63</th>\n",
       "      <th>70</th>\n",
       "      <th>77</th>\n",
       "      <th>84</th>\n",
       "      <th>365</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.936508</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>0.924162</td>\n",
       "      <td>0.921958</td>\n",
       "      <td>0.920635</td>\n",
       "      <td>0.919753</td>\n",
       "      <td>0.919123</td>\n",
       "      <td>0.917328</td>\n",
       "      <td>0.916520</td>\n",
       "      <td>0.916402</td>\n",
       "      <td>0.916306</td>\n",
       "      <td>0.914462</td>\n",
       "      <td>0.915068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.127365</td>\n",
       "      <td>0.112066</td>\n",
       "      <td>0.096384</td>\n",
       "      <td>0.082896</td>\n",
       "      <td>0.073201</td>\n",
       "      <td>0.064414</td>\n",
       "      <td>0.056171</td>\n",
       "      <td>0.048310</td>\n",
       "      <td>0.041902</td>\n",
       "      <td>0.035814</td>\n",
       "      <td>0.032484</td>\n",
       "      <td>0.029906</td>\n",
       "      <td>0.047148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.836735</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.869048</td>\n",
       "      <td>0.747945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.892857</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.892857</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.896104</td>\n",
       "      <td>0.898810</td>\n",
       "      <td>0.894521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.952381</td>\n",
       "      <td>0.964286</td>\n",
       "      <td>0.942857</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>0.918367</td>\n",
       "      <td>0.892857</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.917808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.976190</td>\n",
       "      <td>0.959184</td>\n",
       "      <td>0.964286</td>\n",
       "      <td>0.960317</td>\n",
       "      <td>0.957143</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.954795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.971429</td>\n",
       "      <td>0.974026</td>\n",
       "      <td>0.976190</td>\n",
       "      <td>0.972603</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             7          14         21         28         35         42   \\\n",
       "count  27.000000  27.000000  27.000000  27.000000  27.000000  27.000000   \n",
       "mean    0.936508   0.928571   0.924162   0.921958   0.920635   0.919753   \n",
       "std     0.127365   0.112066   0.096384   0.082896   0.073201   0.064414   \n",
       "min     0.428571   0.571429   0.714286   0.750000   0.800000   0.833333   \n",
       "25%     0.857143   0.892857   0.857143   0.892857   0.857143   0.857143   \n",
       "50%     1.000000   1.000000   0.952381   0.964286   0.942857   0.928571   \n",
       "75%     1.000000   1.000000   1.000000   1.000000   1.000000   0.976190   \n",
       "max     1.000000   1.000000   1.000000   1.000000   1.000000   1.000000   \n",
       "\n",
       "             49         56         63         70         77         84   \\\n",
       "count  27.000000  27.000000  27.000000  27.000000  27.000000  27.000000   \n",
       "mean    0.919123   0.917328   0.916520   0.916402   0.916306   0.914462   \n",
       "std     0.056171   0.048310   0.041902   0.035814   0.032484   0.029906   \n",
       "min     0.836735   0.857143   0.857143   0.857143   0.857143   0.869048   \n",
       "25%     0.857143   0.875000   0.888889   0.900000   0.896104   0.898810   \n",
       "50%     0.918367   0.892857   0.888889   0.900000   0.909091   0.916667   \n",
       "75%     0.959184   0.964286   0.960317   0.957143   0.928571   0.916667   \n",
       "max     1.000000   1.000000   1.000000   0.971429   0.974026   0.976190   \n",
       "\n",
       "             365  \n",
       "count  27.000000  \n",
       "mean    0.915068  \n",
       "std     0.047148  \n",
       "min     0.747945  \n",
       "25%     0.894521  \n",
       "50%     0.917808  \n",
       "75%     0.954795  \n",
       "max     0.972603  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#95% PIs\n",
    "cv_preds, cv_test, cv_intervals = results\n",
    "cv_coverage = prediction_int_coverage_cv(cv_test, cv_intervals)\n",
    "df = pd.DataFrame(cv_coverage)\n",
    "df.columns = horizons\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../../results/model_selection/temp/Trust-fbp-arima_coverage_95.csv\n"
     ]
    }
   ],
   "source": [
    "#write 95% coverage to file\n",
    "metric = 'coverage_95'\n",
    "print(f'{TOP_LEVEL}/{STAGE}/{REGION}-{METHOD}_{metric}.csv')\n",
    "df.to_csv(f'{TOP_LEVEL}/{STAGE}/{REGION}-{METHOD}_{metric}.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
