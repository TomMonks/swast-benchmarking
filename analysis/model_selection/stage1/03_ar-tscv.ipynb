{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time series cross validation: Autoregression with seasonal indexes and holidays.\n",
    "\n",
    "This notebook develops Autoregression models to forecast daily Ambulance Response numbers.  \n",
    "\n",
    "Autoregressive models have the following features:\n",
    "\n",
    "* They contain lagged dependent variables\n",
    "* As the model is a regression it can contain season dummy variables at multiple levels in order to capture multiple seasonalitys\n",
    "\n",
    "Note within this Notebook we will ignore correlated errors.  The most likely impact of this is that prediction intervals\n",
    "are too narrow.  A less likely, but possible outcome is that there may also be additional information that improve forecating in the errors.\n",
    "\n",
    "The final section of the notebook analyses prediction interval coverage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "from pandas.plotting import lag_plot\n",
    "import seaborn as sns\n",
    "\n",
    "#forecast error metrics\n",
    "from forecast_tools.metrics import (mean_absolute_scaled_error, \n",
    "                                    root_mean_squared_error,\n",
    "                                    symmetric_mean_absolute_percentage_error)\n",
    "\n",
    "\n",
    "#'sklearn'\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import Lasso, LinearRegression\n",
    "\n",
    "#statsmodels api\n",
    "import statsmodels.api as sm\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.11.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sm.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from amb_forecast.feature_engineering import featurize_time_series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Input\n",
    "\n",
    "The constants `TOP_LEVEL`, `STAGE`, `REGION`,`TRUST` and `METHOD` are used to control data selection and the directory for outputting results.  \n",
    "\n",
    "> Output file is `f'{TOP_LEVEL}/{STAGE}/{REGION}-{METHOD}_{metric}.csv'.csv`.  where metric will be smape, rmse, mase, coverage_80 and coverage_95. Note: `REGION`: is also used to select the correct data from the input dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOP_LEVEL = '../../../results/model_selection'\n",
    "STAGE = 'stage1'\n",
    "REGION = 'Trust'\n",
    "METHOD = 'ar'\n",
    "\n",
    "FILE_NAME = 'Daily_Responses_5_Years_2019_full.csv'\n",
    "\n",
    "#split training and test data.\n",
    "TEST_SPLIT_DATE = '2019-01-01'\n",
    "\n",
    "#second subdivide: train and val\n",
    "VAL_SPLIT_DATE = '2017-07-01'\n",
    "\n",
    "#discard data after 2020 due to coronavirus\n",
    "#this is the subject of a seperate study.\n",
    "DISCARD_DATE = '2020-01-01'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in path\n",
    "path = f'../../../data/{FILE_NAME}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process_daily_data(path, index_col, by_col, \n",
    "                           values, dayfirst=False):\n",
    "    '''\n",
    "    Daily data is stored in long format.  Read in \n",
    "    and pivot to wide format so that there is a single \n",
    "    colmumn for each regions time series.\n",
    "    '''\n",
    "    df = pd.read_csv(path, index_col=index_col, parse_dates=True, \n",
    "                     dayfirst=dayfirst)\n",
    "    df.columns = map(str.lower, df.columns)\n",
    "    df.index.rename(str(df.index.name).lower(), inplace=True)\n",
    "    \n",
    "    clean_table = pd.pivot_table(df, values=values.lower(), \n",
    "                                 index=[index_col.lower()],\n",
    "                                 columns=[by_col.lower()], aggfunc=np.sum)\n",
    "    \n",
    "    clean_table.index.freq = 'D'\n",
    "    \n",
    "    return clean_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>ora</th>\n",
       "      <th>BNSSG</th>\n",
       "      <th>Cornwall</th>\n",
       "      <th>Devon</th>\n",
       "      <th>Dorset</th>\n",
       "      <th>Gloucestershire</th>\n",
       "      <th>OOA</th>\n",
       "      <th>Somerset</th>\n",
       "      <th>Trust</th>\n",
       "      <th>Wiltshire</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>actual_dt</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2013-12-30</th>\n",
       "      <td>415.0</td>\n",
       "      <td>220.0</td>\n",
       "      <td>502.0</td>\n",
       "      <td>336.0</td>\n",
       "      <td>129.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>183.0</td>\n",
       "      <td>2042.0</td>\n",
       "      <td>255.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-12-31</th>\n",
       "      <td>420.0</td>\n",
       "      <td>236.0</td>\n",
       "      <td>468.0</td>\n",
       "      <td>302.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>180.0</td>\n",
       "      <td>1996.0</td>\n",
       "      <td>260.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-01-01</th>\n",
       "      <td>549.0</td>\n",
       "      <td>341.0</td>\n",
       "      <td>566.0</td>\n",
       "      <td>392.0</td>\n",
       "      <td>157.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>213.0</td>\n",
       "      <td>2570.0</td>\n",
       "      <td>351.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-01-02</th>\n",
       "      <td>450.0</td>\n",
       "      <td>218.0</td>\n",
       "      <td>499.0</td>\n",
       "      <td>301.0</td>\n",
       "      <td>115.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>167.0</td>\n",
       "      <td>2013.0</td>\n",
       "      <td>258.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-01-03</th>\n",
       "      <td>419.0</td>\n",
       "      <td>229.0</td>\n",
       "      <td>503.0</td>\n",
       "      <td>304.0</td>\n",
       "      <td>135.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>195.0</td>\n",
       "      <td>2056.0</td>\n",
       "      <td>269.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "ora         BNSSG  Cornwall  Devon  Dorset  Gloucestershire  OOA  Somerset  \\\n",
       "actual_dt                                                                    \n",
       "2013-12-30  415.0     220.0  502.0   336.0            129.0  NaN     183.0   \n",
       "2013-12-31  420.0     236.0  468.0   302.0            128.0  NaN     180.0   \n",
       "2014-01-01  549.0     341.0  566.0   392.0            157.0  NaN     213.0   \n",
       "2014-01-02  450.0     218.0  499.0   301.0            115.0  NaN     167.0   \n",
       "2014-01-03  419.0     229.0  503.0   304.0            135.0  NaN     195.0   \n",
       "\n",
       "ora          Trust  Wiltshire  \n",
       "actual_dt                      \n",
       "2013-12-30  2042.0      255.0  \n",
       "2013-12-31  1996.0      260.0  \n",
       "2014-01-01  2570.0      351.0  \n",
       "2014-01-02  2013.0      258.0  \n",
       "2014-01-03  2056.0      269.0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean = pre_process_daily_data(path, 'Actual_dt', 'ORA', 'Actual_Value', \n",
    "                               dayfirst=False)\n",
    "clean.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ts_train_test_split(data, split_date):\n",
    "    '''\n",
    "    Split time series into training and test data\n",
    "    \n",
    "    Parameters:\n",
    "    -------\n",
    "    data - pd.DataFrame - time series data.  Index expected as datatimeindex\n",
    "    split_date - the date on which to split the time series\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple (len=2) \n",
    "    0. pandas.DataFrame - training dataset\n",
    "    1. pandas.DataFrame - test dataset\n",
    "    '''\n",
    "    train = data.loc[data.index < split_date]\n",
    "    test = data.loc[data.index >= split_date]\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = ts_train_test_split(clean, split_date=TEST_SPLIT_DATE)\n",
    "\n",
    "#exclude data after 2020 due to coronavirus.\n",
    "test, discard = ts_train_test_split(test, split_date=DISCARD_DATE)\n",
    "\n",
    "#split into train and val AFTER creating new years day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1828, 9)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(365, 9)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# autoregressive lags, seasonal indexes and New years day\n",
    "\n",
    "Generate lags + new binary categorical feature representing new years day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exclude interaction as point forecasts are less accurate.\n",
    "lagged, calendar_dummies, new_year = featurize_time_series(train[REGION], \n",
    "                                                    max_lags=7, \n",
    "                                                    include_interactions=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rename column and drop quarters and t from seasonal indexes\n",
    "new_year.columns = ['new_year']\n",
    "calendar_dummies = calendar_dummies[calendar_dummies.columns[:-4]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>actual</th>\n",
       "      <th>actual_lag1</th>\n",
       "      <th>actual_lag2</th>\n",
       "      <th>actual_lag3</th>\n",
       "      <th>actual_lag4</th>\n",
       "      <th>actual_lag5</th>\n",
       "      <th>actual_lag6</th>\n",
       "      <th>actual_lag7</th>\n",
       "      <th>m_2</th>\n",
       "      <th>m_3</th>\n",
       "      <th>...</th>\n",
       "      <th>m_10</th>\n",
       "      <th>m_11</th>\n",
       "      <th>m_12</th>\n",
       "      <th>dow_1</th>\n",
       "      <th>dow_2</th>\n",
       "      <th>dow_3</th>\n",
       "      <th>dow_4</th>\n",
       "      <th>dow_5</th>\n",
       "      <th>dow_6</th>\n",
       "      <th>new_year</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>actual_dt</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2013-12-30</th>\n",
       "      <td>2042.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-12-31</th>\n",
       "      <td>1996.0</td>\n",
       "      <td>2042.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-01-01</th>\n",
       "      <td>2570.0</td>\n",
       "      <td>1996.0</td>\n",
       "      <td>2042.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-01-02</th>\n",
       "      <td>2013.0</td>\n",
       "      <td>2570.0</td>\n",
       "      <td>1996.0</td>\n",
       "      <td>2042.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-01-03</th>\n",
       "      <td>2056.0</td>\n",
       "      <td>2013.0</td>\n",
       "      <td>2570.0</td>\n",
       "      <td>1996.0</td>\n",
       "      <td>2042.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            actual  actual_lag1  actual_lag2  actual_lag3  actual_lag4  \\\n",
       "actual_dt                                                                \n",
       "2013-12-30  2042.0          NaN          NaN          NaN          NaN   \n",
       "2013-12-31  1996.0       2042.0          NaN          NaN          NaN   \n",
       "2014-01-01  2570.0       1996.0       2042.0          NaN          NaN   \n",
       "2014-01-02  2013.0       2570.0       1996.0       2042.0          NaN   \n",
       "2014-01-03  2056.0       2013.0       2570.0       1996.0       2042.0   \n",
       "\n",
       "            actual_lag5  actual_lag6  actual_lag7  m_2  m_3  ...  m_10  m_11  \\\n",
       "actual_dt                                                    ...               \n",
       "2013-12-30          NaN          NaN          NaN    0    0  ...     0     0   \n",
       "2013-12-31          NaN          NaN          NaN    0    0  ...     0     0   \n",
       "2014-01-01          NaN          NaN          NaN    0    0  ...     0     0   \n",
       "2014-01-02          NaN          NaN          NaN    0    0  ...     0     0   \n",
       "2014-01-03          NaN          NaN          NaN    0    0  ...     0     0   \n",
       "\n",
       "            m_12  dow_1  dow_2  dow_3  dow_4  dow_5  dow_6  new_year  \n",
       "actual_dt                                                             \n",
       "2013-12-30     1      0      0      0      0      0      0         0  \n",
       "2013-12-31     1      1      0      0      0      0      0         0  \n",
       "2014-01-01     0      0      1      0      0      0      0         1  \n",
       "2014-01-02     0      0      0      1      0      0      0         0  \n",
       "2014-01-03     0      0      0      0      1      0      0         0  \n",
       "\n",
       "[5 rows x 26 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#combined to single dataframe\n",
    "processed = pd.concat([train[REGION], lagged, calendar_dummies, new_year], \n",
    "                      axis=1)\n",
    "processed.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train validation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train split into train and validation\n",
    "train, val = ts_train_test_split(processed, split_date=VAL_SPLIT_DATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1279, 26)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(549, 26)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation\n",
    "\n",
    "`time_series_cv` implements rolling forecast origin cross validation for time series.  \n",
    "It does not calculate forecast error, but instead returns the predictions, pred intervals and actuals in an array that can be passed to any forecast error function. (this is for efficiency and allows additional metrics to be calculated if needed).\n",
    "\n",
    "\n",
    "> Note: prediction uses an iterative method where ground truth inputs are gradually replaced with forecast values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_series_cv(train, val, horizons, lags, step=1, alpha=0.2, \n",
    "                   regularised=False, reg_weight=0.1):\n",
    "    '''\n",
    "    Time series cross validation across multiple horizons for a autoregressive\n",
    "    model.\n",
    "\n",
    "    Incrementally adds additional training data to the model and tests\n",
    "    across a provided list of forecast horizons. Note that function tests a\n",
    "    model only against complete validation sets.  E.g. if horizon = 15 and \n",
    "    len(val) = 12 then no testing is done.  In the case of multiple horizons\n",
    "    e.g. [7, 14, 28] then the function will use the maximum forecast horizon\n",
    "    to calculate the number of iterations i.e if len(val) = 365 and step = 1\n",
    "    then no. iterations = len(val) - max(horizon) = 365 - 28 = 337.\n",
    "    \n",
    "    Note that when the forecast horizon exceeds the lag number of the model\n",
    "    forecasts will be based of previous forecasts.  I.e. for a lag 7 model the first 7 \n",
    "    forecasts will include at least one ground truth observation, but from step 8 onwards all \n",
    "    forecasts iwll be based on the previous 7 predicted y values.\n",
    "    \n",
    "    Parameters:\n",
    "    --------\n",
    "    train - np.array - vector of training data\n",
    "\n",
    "    val - np.array - vector of validation data\n",
    "\n",
    "    horizon - list of ints, forecast horizon e.g. [7, 14, 28] days\n",
    "\n",
    "    step -- step taken in cross validation \n",
    "            e.g. 1 in next cross validation training data includes next point \n",
    "            from the validation set.\n",
    "            e.g. 7 in the next cross validation training data includes next 7 points\n",
    "            (default=1)\n",
    "            \n",
    "    Returns:\n",
    "    -------\n",
    "    np.array - vector of forecast errors from the CVs.\n",
    "    '''\n",
    "    cv_preds = []\n",
    "    cv_actuals = []\n",
    "    cv_intervals = []\n",
    "    MAX_LAG = lags\n",
    "    split = 1\n",
    "    \n",
    "    print('split => ', end=\"\")\n",
    "    for i in range(0, len(val) - max(horizons) + 1, step):\n",
    "        print(f'{split}, ', end=\"\")\n",
    "        split += 1\n",
    "        \n",
    "        #create and fit on a new training set (train + val)\n",
    "        train_cv = np.concatenate([train, val.iloc[:i, 0:]], axis=0) \n",
    "        X_train, y_train = train_cv[:,1:], train_cv[:,0]       \n",
    "        \n",
    "        # Fit and summarize OLS model\n",
    "        X_train = sm.add_constant(X_train, prepend=False, has_constant='add')\n",
    "        model = sm.OLS(endog=y_train, exog=X_train)\n",
    "        \n",
    "        if not regularised:\n",
    "            results = model.fit()\n",
    "        else:    \n",
    "            results = model.fit_regularized(method='elastic_net',\n",
    "                                            alpha=reg_weight, refit=True)\n",
    "\n",
    "        preds = []\n",
    "        intervals = []\n",
    "        current_batch = val.iloc[i, 1:].to_numpy()\n",
    "\n",
    "        #iteratively predict the next data point\n",
    "        #and update the lags.  Remember that the\n",
    "        #actual lags are replaced by FORECAST values \n",
    "        #as time goes on.\n",
    "        for j in range(max(horizons)): \n",
    "            \n",
    "            #one timestep ahead of historical points\n",
    "            batch_exog = current_batch\n",
    "\n",
    "            batch_exog = sm.add_constant(batch_exog.reshape(-1,1).T, \n",
    "                                         prepend=False,\n",
    "                                         has_constant='add')\n",
    "                       \n",
    "            predictions = results.get_prediction(batch_exog)\n",
    "            \n",
    "            df = predictions.summary_frame(alpha=alpha)\n",
    "            \n",
    "            y_pred = df['mean'].to_numpy()\n",
    "            preds.append(y_pred)\n",
    "            \n",
    "            #prediction intervals\n",
    "            y_intervals = df[['obs_ci_lower', 'obs_ci_upper']].to_numpy()\n",
    "            intervals.append(y_intervals)\n",
    "            \n",
    "            #remove tail lagged value add prediction to head \n",
    "            current_batch[:MAX_LAG] = np.append(y_pred, current_batch[:MAX_LAG-1])\n",
    "            \n",
    "            #get next set of seasonal dummies\n",
    "            current_batch[MAX_LAG:] = val.iloc[i+j+1, MAX_LAG+1:].to_numpy()\n",
    "        \n",
    "        cv_h_preds = []\n",
    "        cv_h_intervals = []\n",
    "        cv_test = []\n",
    "                \n",
    "        for h in horizons:\n",
    "            #store the h-step mean prediction\n",
    "            cv_h_preds.append(np.concatenate(preds[:h]))\n",
    "            \n",
    "            #store the h-step prediction intervals\n",
    "            cv_h_intervals.append(np.concatenate(intervals[:h]))\n",
    "            #store the h-step actual value\n",
    "            cv_test.append(val.iloc[i:i+h,0])                 \n",
    "                     \n",
    "        cv_preds.append(cv_h_preds)\n",
    "        cv_intervals.append(cv_h_intervals)\n",
    "        cv_actuals.append(cv_test)\n",
    "        \n",
    "    \n",
    "    print('done.\\n')\n",
    "    return cv_preds, cv_intervals, cv_actuals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross validation (without regularization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time horizons evaluated (in days) are 7, 14, 28, 56, 84, 365.\n",
    "\n",
    "Note: These are basic regression models and do not have an ARIMA error process.  This means that interpretation of the coefficients is problematic and should be done with caution.\n",
    "\n",
    "**All models contain the seasonal dummy variables, t and exceptional days.**\n",
    "\n",
    "Models evaluated:\n",
    "\n",
    "* lags 1\n",
    "* lags 1, 2\n",
    "* lags 1, 2, 3\n",
    "* lags 1, 2, 3, .... `MAX_LAGS`\n",
    "\n",
    "Notes: the cross validation function is returning the error measure over the different time horizons.  So the summary statistics refer to the distrubution of the test statistic.  Not the individual forecast errors of each point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_cv_error(cv_preds, cv_test, error_func):\n",
    "    n_splits = len(cv_preds)\n",
    "    cv_errors = []\n",
    "    \n",
    "    for split in range(n_splits):\n",
    "        pred_error = error_func(cv_test[split], cv_preds[split])\n",
    "        cv_errors.append(pred_error)\n",
    "        \n",
    "    return np.array(cv_errors)\n",
    "\n",
    "def forecast_errors_cv(cv_preds, cv_test, error_func):\n",
    "    cv_test = np.array(cv_test)\n",
    "    cv_preds = np.array(cv_preds)\n",
    "    n_horizons = len(cv_test)    \n",
    "    \n",
    "    horizon_errors = []\n",
    "    for h in range(n_horizons):\n",
    "        split_errors = split_cv_error(cv_preds[h], cv_test[h], error_func)\n",
    "        horizon_errors.append(split_errors)\n",
    "\n",
    "    return np.array(horizon_errors)\n",
    "\n",
    "def split_coverage(cv_test, cv_intervals):\n",
    "    n_splits = len(cv_test)\n",
    "    cv_errors = []\n",
    "        \n",
    "    for split in range(n_splits):\n",
    "        val = np.asarray(cv_test[split])\n",
    "        lower = cv_intervals[split].T[0]\n",
    "        upper = cv_intervals[split].T[1]\n",
    "        \n",
    "        coverage = len(np.where((val > lower) & (val < upper))[0])\n",
    "        coverage = coverage / len(val)\n",
    "        \n",
    "        cv_errors.append(coverage)\n",
    "        \n",
    "    return np.array(cv_errors)\n",
    "    \n",
    "    \n",
    "def prediction_int_coverage_cv(cv_test, cv_intervals):\n",
    "    cv_test = np.array(cv_test)\n",
    "    cv_intervals = np.array(cv_intervals)\n",
    "    n_horizons = len(cv_test)    \n",
    "    \n",
    "    horizon_coverage = []\n",
    "    for h in range(n_horizons):\n",
    "        split_coverages = split_coverage(cv_test[h], cv_intervals[h])\n",
    "        horizon_coverage.append(split_coverages)\n",
    "\n",
    "    return np.array(horizon_coverage)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_cv_error_scaled(cv_preds, cv_test, y_train):\n",
    "    n_splits = len(cv_preds)\n",
    "    cv_errors = []\n",
    "    \n",
    "    for split in range(n_splits):\n",
    "        pred_error = mean_absolute_scaled_error(cv_test[split], cv_preds[split], \n",
    "                                                y_train, period=7)\n",
    "        \n",
    "        cv_errors.append(pred_error)\n",
    "        \n",
    "    return np.array(cv_errors)\n",
    "\n",
    "def forecast_errors_cv_scaled(cv_preds, cv_test, y_train):\n",
    "    cv_test = np.array(cv_test)\n",
    "    cv_preds = np.array(cv_preds)\n",
    "    n_horizons = len(cv_test)    \n",
    "    \n",
    "    horizon_errors = []\n",
    "    for h in range(n_horizons):\n",
    "        split_errors = split_cv_error_scaled(cv_preds[h], cv_test[h], y_train)\n",
    "        horizon_errors.append(split_errors)\n",
    "        \n",
    "    return np.array(horizon_errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "split => 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, done.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "MAX_LAG = 7\n",
    "horizons = [7, 14, 21, 28, 35, 42, 49, 56, 63, 70, 77, 84, 365]\n",
    "STEP = 7\n",
    "PREFIX = 'actual'\n",
    "\n",
    "y_lags = [PREFIX+'_lag' + str(i) for i in range(1, MAX_LAG+1)]\n",
    "\n",
    "select_cv = ['actual'] + y_lags + list(calendar_dummies.columns) + \\\n",
    "    list(new_year.columns)\n",
    "\n",
    "\n",
    "cv_preds, cv_intervals, cv_test  = time_series_cv(train=train[select_cv][MAX_LAG+1:], \n",
    "                                                  val=val[select_cv], \n",
    "                                                  horizons=horizons,\n",
    "                                                  lags=len(y_lags),\n",
    "                                                  step=STEP,\n",
    "                                                  alpha=0.2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2191.82227067, 2370.77318012],\n",
       "       [2163.29042123, 2342.20535038],\n",
       "       [2066.33364772, 2245.26333601],\n",
       "       [1986.41480171, 2165.22800952],\n",
       "       [1978.7094183 , 2157.47054199],\n",
       "       [2004.52190743, 2183.27170169],\n",
       "       [2056.35436403, 2234.91403217]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_intervals[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>7</th>\n",
       "      <th>14</th>\n",
       "      <th>21</th>\n",
       "      <th>28</th>\n",
       "      <th>35</th>\n",
       "      <th>42</th>\n",
       "      <th>49</th>\n",
       "      <th>56</th>\n",
       "      <th>63</th>\n",
       "      <th>70</th>\n",
       "      <th>77</th>\n",
       "      <th>84</th>\n",
       "      <th>365</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.208020</td>\n",
       "      <td>3.563141</td>\n",
       "      <td>3.795126</td>\n",
       "      <td>4.000028</td>\n",
       "      <td>4.159445</td>\n",
       "      <td>4.302911</td>\n",
       "      <td>4.432084</td>\n",
       "      <td>4.565148</td>\n",
       "      <td>4.672183</td>\n",
       "      <td>4.778804</td>\n",
       "      <td>4.891725</td>\n",
       "      <td>4.993684</td>\n",
       "      <td>5.002033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.122335</td>\n",
       "      <td>1.047744</td>\n",
       "      <td>0.882623</td>\n",
       "      <td>0.863666</td>\n",
       "      <td>0.958771</td>\n",
       "      <td>1.029291</td>\n",
       "      <td>1.074982</td>\n",
       "      <td>1.114456</td>\n",
       "      <td>1.124972</td>\n",
       "      <td>1.146078</td>\n",
       "      <td>1.172122</td>\n",
       "      <td>1.168497</td>\n",
       "      <td>0.220911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.503479</td>\n",
       "      <td>1.897222</td>\n",
       "      <td>2.342834</td>\n",
       "      <td>2.562351</td>\n",
       "      <td>2.783483</td>\n",
       "      <td>2.762655</td>\n",
       "      <td>2.702651</td>\n",
       "      <td>2.766886</td>\n",
       "      <td>2.890617</td>\n",
       "      <td>3.041443</td>\n",
       "      <td>3.083533</td>\n",
       "      <td>3.363906</td>\n",
       "      <td>4.490962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2.533541</td>\n",
       "      <td>2.908424</td>\n",
       "      <td>3.281733</td>\n",
       "      <td>3.340787</td>\n",
       "      <td>3.531046</td>\n",
       "      <td>3.477897</td>\n",
       "      <td>3.725477</td>\n",
       "      <td>3.902301</td>\n",
       "      <td>4.036282</td>\n",
       "      <td>4.128845</td>\n",
       "      <td>4.154850</td>\n",
       "      <td>4.135156</td>\n",
       "      <td>4.858556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3.154700</td>\n",
       "      <td>3.340907</td>\n",
       "      <td>3.618547</td>\n",
       "      <td>3.968077</td>\n",
       "      <td>4.154980</td>\n",
       "      <td>4.173193</td>\n",
       "      <td>4.238100</td>\n",
       "      <td>4.380771</td>\n",
       "      <td>4.355394</td>\n",
       "      <td>4.430484</td>\n",
       "      <td>4.611300</td>\n",
       "      <td>4.720808</td>\n",
       "      <td>5.036206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3.639578</td>\n",
       "      <td>4.001279</td>\n",
       "      <td>4.135099</td>\n",
       "      <td>4.662474</td>\n",
       "      <td>4.872152</td>\n",
       "      <td>4.976073</td>\n",
       "      <td>5.237836</td>\n",
       "      <td>5.274810</td>\n",
       "      <td>5.338967</td>\n",
       "      <td>5.606060</td>\n",
       "      <td>5.865711</td>\n",
       "      <td>5.983976</td>\n",
       "      <td>5.190375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>7.032771</td>\n",
       "      <td>6.416015</td>\n",
       "      <td>5.495095</td>\n",
       "      <td>5.585666</td>\n",
       "      <td>6.412107</td>\n",
       "      <td>6.546856</td>\n",
       "      <td>6.689826</td>\n",
       "      <td>6.794582</td>\n",
       "      <td>7.122329</td>\n",
       "      <td>6.856729</td>\n",
       "      <td>7.134288</td>\n",
       "      <td>7.327150</td>\n",
       "      <td>5.263574</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             7          14         21         28         35         42   \\\n",
       "count  27.000000  27.000000  27.000000  27.000000  27.000000  27.000000   \n",
       "mean    3.208020   3.563141   3.795126   4.000028   4.159445   4.302911   \n",
       "std     1.122335   1.047744   0.882623   0.863666   0.958771   1.029291   \n",
       "min     1.503479   1.897222   2.342834   2.562351   2.783483   2.762655   \n",
       "25%     2.533541   2.908424   3.281733   3.340787   3.531046   3.477897   \n",
       "50%     3.154700   3.340907   3.618547   3.968077   4.154980   4.173193   \n",
       "75%     3.639578   4.001279   4.135099   4.662474   4.872152   4.976073   \n",
       "max     7.032771   6.416015   5.495095   5.585666   6.412107   6.546856   \n",
       "\n",
       "             49         56         63         70         77         84   \\\n",
       "count  27.000000  27.000000  27.000000  27.000000  27.000000  27.000000   \n",
       "mean    4.432084   4.565148   4.672183   4.778804   4.891725   4.993684   \n",
       "std     1.074982   1.114456   1.124972   1.146078   1.172122   1.168497   \n",
       "min     2.702651   2.766886   2.890617   3.041443   3.083533   3.363906   \n",
       "25%     3.725477   3.902301   4.036282   4.128845   4.154850   4.135156   \n",
       "50%     4.238100   4.380771   4.355394   4.430484   4.611300   4.720808   \n",
       "75%     5.237836   5.274810   5.338967   5.606060   5.865711   5.983976   \n",
       "max     6.689826   6.794582   7.122329   6.856729   7.134288   7.327150   \n",
       "\n",
       "             365  \n",
       "count  27.000000  \n",
       "mean    5.002033  \n",
       "std     0.220911  \n",
       "min     4.490962  \n",
       "25%     4.858556  \n",
       "50%     5.036206  \n",
       "75%     5.190375  \n",
       "max     5.263574  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#CV point predictions smape\n",
    "cv_errors = forecast_errors_cv(cv_preds, cv_test, \n",
    "                               symmetric_mean_absolute_percentage_error)\n",
    "df = pd.DataFrame(cv_errors)\n",
    "df.columns = horizons\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../../results/model_selection/stage1/Trust-ar_smape.csv\n"
     ]
    }
   ],
   "source": [
    "#output sMAPE results to file\n",
    "metric = 'smape'\n",
    "print(f'{TOP_LEVEL}/{STAGE}/{REGION}-{METHOD}_{metric}.csv')\n",
    "df.to_csv(f'{TOP_LEVEL}/{STAGE}/{REGION}-{METHOD}_{metric}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>7</th>\n",
       "      <th>14</th>\n",
       "      <th>21</th>\n",
       "      <th>28</th>\n",
       "      <th>35</th>\n",
       "      <th>42</th>\n",
       "      <th>49</th>\n",
       "      <th>56</th>\n",
       "      <th>63</th>\n",
       "      <th>70</th>\n",
       "      <th>77</th>\n",
       "      <th>84</th>\n",
       "      <th>365</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>83.474784</td>\n",
       "      <td>94.278029</td>\n",
       "      <td>101.658720</td>\n",
       "      <td>107.526136</td>\n",
       "      <td>111.630448</td>\n",
       "      <td>115.044179</td>\n",
       "      <td>118.084666</td>\n",
       "      <td>121.280057</td>\n",
       "      <td>123.884020</td>\n",
       "      <td>126.499772</td>\n",
       "      <td>129.132697</td>\n",
       "      <td>131.502520</td>\n",
       "      <td>133.640602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>28.062919</td>\n",
       "      <td>28.841003</td>\n",
       "      <td>27.027198</td>\n",
       "      <td>27.052136</td>\n",
       "      <td>28.112013</td>\n",
       "      <td>28.806439</td>\n",
       "      <td>29.164159</td>\n",
       "      <td>29.370553</td>\n",
       "      <td>29.011602</td>\n",
       "      <td>29.101388</td>\n",
       "      <td>29.254349</td>\n",
       "      <td>28.818805</td>\n",
       "      <td>4.155083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>41.345704</td>\n",
       "      <td>59.869374</td>\n",
       "      <td>57.886822</td>\n",
       "      <td>64.845476</td>\n",
       "      <td>71.293663</td>\n",
       "      <td>75.500070</td>\n",
       "      <td>75.633085</td>\n",
       "      <td>76.522526</td>\n",
       "      <td>78.498440</td>\n",
       "      <td>81.258701</td>\n",
       "      <td>82.865115</td>\n",
       "      <td>90.363652</td>\n",
       "      <td>122.343230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>66.801326</td>\n",
       "      <td>79.788352</td>\n",
       "      <td>87.209081</td>\n",
       "      <td>86.780098</td>\n",
       "      <td>91.542663</td>\n",
       "      <td>96.739832</td>\n",
       "      <td>99.857556</td>\n",
       "      <td>105.049755</td>\n",
       "      <td>106.823819</td>\n",
       "      <td>109.482219</td>\n",
       "      <td>110.109865</td>\n",
       "      <td>110.062406</td>\n",
       "      <td>131.281270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>76.253369</td>\n",
       "      <td>85.522598</td>\n",
       "      <td>94.440390</td>\n",
       "      <td>102.071490</td>\n",
       "      <td>105.258246</td>\n",
       "      <td>112.319927</td>\n",
       "      <td>111.620164</td>\n",
       "      <td>114.039983</td>\n",
       "      <td>114.726025</td>\n",
       "      <td>114.468002</td>\n",
       "      <td>118.954746</td>\n",
       "      <td>120.626988</td>\n",
       "      <td>134.262632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>95.392282</td>\n",
       "      <td>103.468173</td>\n",
       "      <td>111.689863</td>\n",
       "      <td>120.167465</td>\n",
       "      <td>122.107568</td>\n",
       "      <td>131.524221</td>\n",
       "      <td>144.626516</td>\n",
       "      <td>148.373492</td>\n",
       "      <td>147.730523</td>\n",
       "      <td>153.035711</td>\n",
       "      <td>157.033483</td>\n",
       "      <td>157.595905</td>\n",
       "      <td>137.149653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>187.517781</td>\n",
       "      <td>183.725216</td>\n",
       "      <td>169.363947</td>\n",
       "      <td>164.476784</td>\n",
       "      <td>173.408807</td>\n",
       "      <td>171.580125</td>\n",
       "      <td>171.261639</td>\n",
       "      <td>171.915777</td>\n",
       "      <td>178.884418</td>\n",
       "      <td>175.410655</td>\n",
       "      <td>180.645532</td>\n",
       "      <td>183.892585</td>\n",
       "      <td>138.424338</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              7           14          21          28          35          42   \\\n",
       "count   27.000000   27.000000   27.000000   27.000000   27.000000   27.000000   \n",
       "mean    83.474784   94.278029  101.658720  107.526136  111.630448  115.044179   \n",
       "std     28.062919   28.841003   27.027198   27.052136   28.112013   28.806439   \n",
       "min     41.345704   59.869374   57.886822   64.845476   71.293663   75.500070   \n",
       "25%     66.801326   79.788352   87.209081   86.780098   91.542663   96.739832   \n",
       "50%     76.253369   85.522598   94.440390  102.071490  105.258246  112.319927   \n",
       "75%     95.392282  103.468173  111.689863  120.167465  122.107568  131.524221   \n",
       "max    187.517781  183.725216  169.363947  164.476784  173.408807  171.580125   \n",
       "\n",
       "              49          56          63          70          77          84   \\\n",
       "count   27.000000   27.000000   27.000000   27.000000   27.000000   27.000000   \n",
       "mean   118.084666  121.280057  123.884020  126.499772  129.132697  131.502520   \n",
       "std     29.164159   29.370553   29.011602   29.101388   29.254349   28.818805   \n",
       "min     75.633085   76.522526   78.498440   81.258701   82.865115   90.363652   \n",
       "25%     99.857556  105.049755  106.823819  109.482219  110.109865  110.062406   \n",
       "50%    111.620164  114.039983  114.726025  114.468002  118.954746  120.626988   \n",
       "75%    144.626516  148.373492  147.730523  153.035711  157.033483  157.595905   \n",
       "max    171.261639  171.915777  178.884418  175.410655  180.645532  183.892585   \n",
       "\n",
       "              365  \n",
       "count   27.000000  \n",
       "mean   133.640602  \n",
       "std      4.155083  \n",
       "min    122.343230  \n",
       "25%    131.281270  \n",
       "50%    134.262632  \n",
       "75%    137.149653  \n",
       "max    138.424338  "
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#CV point predictions rmse - no interactions\n",
    "cv_errors = forecast_errors_cv(cv_preds, cv_test, root_mean_squared_error)\n",
    "df = pd.DataFrame(cv_errors)\n",
    "df.columns = horizons\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../../results/model_selection/stage1/Trust-ar_rmse.csv\n"
     ]
    }
   ],
   "source": [
    "#output rmse\n",
    "metric = 'rmse'\n",
    "print(f'{TOP_LEVEL}/{STAGE}/{REGION}-{METHOD}_{metric}.csv')\n",
    "df.to_csv(f'{TOP_LEVEL}/{STAGE}/{REGION}-{METHOD}_{metric}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>7</th>\n",
       "      <th>14</th>\n",
       "      <th>21</th>\n",
       "      <th>28</th>\n",
       "      <th>35</th>\n",
       "      <th>42</th>\n",
       "      <th>49</th>\n",
       "      <th>56</th>\n",
       "      <th>63</th>\n",
       "      <th>70</th>\n",
       "      <th>77</th>\n",
       "      <th>84</th>\n",
       "      <th>365</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.860845</td>\n",
       "      <td>0.957338</td>\n",
       "      <td>1.019508</td>\n",
       "      <td>1.074142</td>\n",
       "      <td>1.116464</td>\n",
       "      <td>1.154558</td>\n",
       "      <td>1.189075</td>\n",
       "      <td>1.224854</td>\n",
       "      <td>1.253678</td>\n",
       "      <td>1.282564</td>\n",
       "      <td>1.313194</td>\n",
       "      <td>1.340829</td>\n",
       "      <td>1.332612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.324613</td>\n",
       "      <td>0.311463</td>\n",
       "      <td>0.270448</td>\n",
       "      <td>0.266984</td>\n",
       "      <td>0.289911</td>\n",
       "      <td>0.305962</td>\n",
       "      <td>0.315856</td>\n",
       "      <td>0.324368</td>\n",
       "      <td>0.325762</td>\n",
       "      <td>0.329634</td>\n",
       "      <td>0.334814</td>\n",
       "      <td>0.332434</td>\n",
       "      <td>0.059583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.395840</td>\n",
       "      <td>0.479859</td>\n",
       "      <td>0.587899</td>\n",
       "      <td>0.646535</td>\n",
       "      <td>0.704888</td>\n",
       "      <td>0.707152</td>\n",
       "      <td>0.690331</td>\n",
       "      <td>0.706663</td>\n",
       "      <td>0.739696</td>\n",
       "      <td>0.781307</td>\n",
       "      <td>0.795670</td>\n",
       "      <td>0.869881</td>\n",
       "      <td>1.191133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.677464</td>\n",
       "      <td>0.782805</td>\n",
       "      <td>0.859710</td>\n",
       "      <td>0.880520</td>\n",
       "      <td>0.928069</td>\n",
       "      <td>0.910309</td>\n",
       "      <td>0.988603</td>\n",
       "      <td>1.033898</td>\n",
       "      <td>1.062882</td>\n",
       "      <td>1.086559</td>\n",
       "      <td>1.090179</td>\n",
       "      <td>1.084622</td>\n",
       "      <td>1.294507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.837642</td>\n",
       "      <td>0.902937</td>\n",
       "      <td>0.969446</td>\n",
       "      <td>1.060100</td>\n",
       "      <td>1.102203</td>\n",
       "      <td>1.100208</td>\n",
       "      <td>1.133636</td>\n",
       "      <td>1.161940</td>\n",
       "      <td>1.166508</td>\n",
       "      <td>1.185542</td>\n",
       "      <td>1.217986</td>\n",
       "      <td>1.249135</td>\n",
       "      <td>1.342875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.977719</td>\n",
       "      <td>1.051680</td>\n",
       "      <td>1.117620</td>\n",
       "      <td>1.243641</td>\n",
       "      <td>1.283127</td>\n",
       "      <td>1.360921</td>\n",
       "      <td>1.472157</td>\n",
       "      <td>1.481519</td>\n",
       "      <td>1.492901</td>\n",
       "      <td>1.552234</td>\n",
       "      <td>1.611535</td>\n",
       "      <td>1.635176</td>\n",
       "      <td>1.382550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2.048567</td>\n",
       "      <td>1.912177</td>\n",
       "      <td>1.622885</td>\n",
       "      <td>1.596242</td>\n",
       "      <td>1.773316</td>\n",
       "      <td>1.788436</td>\n",
       "      <td>1.815270</td>\n",
       "      <td>1.837808</td>\n",
       "      <td>1.925149</td>\n",
       "      <td>1.849013</td>\n",
       "      <td>1.926235</td>\n",
       "      <td>1.977432</td>\n",
       "      <td>1.403148</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             7          14         21         28         35         42   \\\n",
       "count  27.000000  27.000000  27.000000  27.000000  27.000000  27.000000   \n",
       "mean    0.860845   0.957338   1.019508   1.074142   1.116464   1.154558   \n",
       "std     0.324613   0.311463   0.270448   0.266984   0.289911   0.305962   \n",
       "min     0.395840   0.479859   0.587899   0.646535   0.704888   0.707152   \n",
       "25%     0.677464   0.782805   0.859710   0.880520   0.928069   0.910309   \n",
       "50%     0.837642   0.902937   0.969446   1.060100   1.102203   1.100208   \n",
       "75%     0.977719   1.051680   1.117620   1.243641   1.283127   1.360921   \n",
       "max     2.048567   1.912177   1.622885   1.596242   1.773316   1.788436   \n",
       "\n",
       "             49         56         63         70         77         84   \\\n",
       "count  27.000000  27.000000  27.000000  27.000000  27.000000  27.000000   \n",
       "mean    1.189075   1.224854   1.253678   1.282564   1.313194   1.340829   \n",
       "std     0.315856   0.324368   0.325762   0.329634   0.334814   0.332434   \n",
       "min     0.690331   0.706663   0.739696   0.781307   0.795670   0.869881   \n",
       "25%     0.988603   1.033898   1.062882   1.086559   1.090179   1.084622   \n",
       "50%     1.133636   1.161940   1.166508   1.185542   1.217986   1.249135   \n",
       "75%     1.472157   1.481519   1.492901   1.552234   1.611535   1.635176   \n",
       "max     1.815270   1.837808   1.925149   1.849013   1.926235   1.977432   \n",
       "\n",
       "             365  \n",
       "count  27.000000  \n",
       "mean    1.332612  \n",
       "std     0.059583  \n",
       "min     1.191133  \n",
       "25%     1.294507  \n",
       "50%     1.342875  \n",
       "75%     1.382550  \n",
       "max     1.403148  "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#mase\n",
    "cv_errors = forecast_errors_cv_scaled(cv_preds, cv_test, train['actual'])\n",
    "df = pd.DataFrame(cv_errors)\n",
    "df.columns = horizons\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../../results/model_selection/stage1/Trust-ar_mase.csv\n"
     ]
    }
   ],
   "source": [
    "metric = 'mase'\n",
    "print(f'{TOP_LEVEL}/{STAGE}/{REGION}-{METHOD}_{metric}.csv')\n",
    "df.to_csv(f'{TOP_LEVEL}/{STAGE}/{REGION}-{METHOD}_{metric}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>7</th>\n",
       "      <th>14</th>\n",
       "      <th>21</th>\n",
       "      <th>28</th>\n",
       "      <th>35</th>\n",
       "      <th>42</th>\n",
       "      <th>49</th>\n",
       "      <th>56</th>\n",
       "      <th>63</th>\n",
       "      <th>70</th>\n",
       "      <th>77</th>\n",
       "      <th>84</th>\n",
       "      <th>365</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.671958</td>\n",
       "      <td>0.611111</td>\n",
       "      <td>0.573192</td>\n",
       "      <td>0.543651</td>\n",
       "      <td>0.522751</td>\n",
       "      <td>0.502646</td>\n",
       "      <td>0.485261</td>\n",
       "      <td>0.467593</td>\n",
       "      <td>0.452675</td>\n",
       "      <td>0.439153</td>\n",
       "      <td>0.425685</td>\n",
       "      <td>0.413139</td>\n",
       "      <td>0.430137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.208963</td>\n",
       "      <td>0.180846</td>\n",
       "      <td>0.152588</td>\n",
       "      <td>0.140200</td>\n",
       "      <td>0.145646</td>\n",
       "      <td>0.151499</td>\n",
       "      <td>0.148286</td>\n",
       "      <td>0.144227</td>\n",
       "      <td>0.140902</td>\n",
       "      <td>0.139048</td>\n",
       "      <td>0.137079</td>\n",
       "      <td>0.134963</td>\n",
       "      <td>0.040028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.214286</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.321429</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.261905</td>\n",
       "      <td>0.224490</td>\n",
       "      <td>0.214286</td>\n",
       "      <td>0.190476</td>\n",
       "      <td>0.214286</td>\n",
       "      <td>0.194805</td>\n",
       "      <td>0.178571</td>\n",
       "      <td>0.386301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.446429</td>\n",
       "      <td>0.414286</td>\n",
       "      <td>0.369048</td>\n",
       "      <td>0.377551</td>\n",
       "      <td>0.357143</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.342857</td>\n",
       "      <td>0.324675</td>\n",
       "      <td>0.303571</td>\n",
       "      <td>0.394521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.642857</td>\n",
       "      <td>0.619048</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.514286</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.469388</td>\n",
       "      <td>0.446429</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.414286</td>\n",
       "      <td>0.389610</td>\n",
       "      <td>0.404762</td>\n",
       "      <td>0.427397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.660714</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.607143</td>\n",
       "      <td>0.581633</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>0.531746</td>\n",
       "      <td>0.528571</td>\n",
       "      <td>0.519481</td>\n",
       "      <td>0.529762</td>\n",
       "      <td>0.456164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.821429</td>\n",
       "      <td>0.771429</td>\n",
       "      <td>0.761905</td>\n",
       "      <td>0.755102</td>\n",
       "      <td>0.732143</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.685714</td>\n",
       "      <td>0.662338</td>\n",
       "      <td>0.630952</td>\n",
       "      <td>0.517808</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             7          14         21         28         35         42   \\\n",
       "count  27.000000  27.000000  27.000000  27.000000  27.000000  27.000000   \n",
       "mean    0.671958   0.611111   0.573192   0.543651   0.522751   0.502646   \n",
       "std     0.208963   0.180846   0.152588   0.140200   0.145646   0.151499   \n",
       "min     0.142857   0.214286   0.333333   0.321429   0.285714   0.261905   \n",
       "25%     0.571429   0.500000   0.428571   0.446429   0.414286   0.369048   \n",
       "50%     0.714286   0.642857   0.619048   0.500000   0.514286   0.500000   \n",
       "75%     0.857143   0.750000   0.666667   0.660714   0.600000   0.607143   \n",
       "max     1.000000   0.928571   0.857143   0.821429   0.771429   0.761905   \n",
       "\n",
       "             49         56         63         70         77         84   \\\n",
       "count  27.000000  27.000000  27.000000  27.000000  27.000000  27.000000   \n",
       "mean    0.485261   0.467593   0.452675   0.439153   0.425685   0.413139   \n",
       "std     0.148286   0.144227   0.140902   0.139048   0.137079   0.134963   \n",
       "min     0.224490   0.214286   0.190476   0.214286   0.194805   0.178571   \n",
       "25%     0.377551   0.357143   0.333333   0.342857   0.324675   0.303571   \n",
       "50%     0.469388   0.446429   0.428571   0.414286   0.389610   0.404762   \n",
       "75%     0.581633   0.562500   0.531746   0.528571   0.519481   0.529762   \n",
       "max     0.755102   0.732143   0.714286   0.685714   0.662338   0.630952   \n",
       "\n",
       "             365  \n",
       "count  27.000000  \n",
       "mean    0.430137  \n",
       "std     0.040028  \n",
       "min     0.386301  \n",
       "25%     0.394521  \n",
       "50%     0.427397  \n",
       "75%     0.456164  \n",
       "max     0.517808  "
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#80% PIs\n",
    "cv_coverage = prediction_int_coverage_cv(cv_test, cv_intervals)\n",
    "df = pd.DataFrame(cv_coverage)\n",
    "df.columns = horizons\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../../results/model_selection/stage1/Trust-ar_coverage_80.csv\n"
     ]
    }
   ],
   "source": [
    "#write 80% coverage to file\n",
    "metric = 'coverage_80'\n",
    "print(f'{TOP_LEVEL}/{STAGE}/{REGION}-{METHOD}_{metric}.csv')\n",
    "df.to_csv(f'{TOP_LEVEL}/{STAGE}/{REGION}-{METHOD}_{metric}.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Repeat for 95% PIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "split => 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, done.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#95% PIs\n",
    "MAX_LAG = 7\n",
    "horizons = [7, 14, 21, 28, 35, 42, 49, 56, 63, 70, 77, 84, 365]\n",
    "STEP = 7\n",
    "PREFIX = 'actual'\n",
    "\n",
    "y_lags = [PREFIX+'_lag' + str(i) for i in range(1, MAX_LAG+1)]\n",
    "\n",
    "select_cv = ['actual'] + y_lags + list(calendar_dummies.columns) + \\\n",
    "    list(new_year.columns)\n",
    "\n",
    "\n",
    "cv_preds, cv_intervals, cv_test  = time_series_cv(train=train[select_cv][MAX_LAG+1:], \n",
    "                                                  val=val[select_cv], \n",
    "                                                  horizons=horizons,\n",
    "                                                  lags=len(y_lags),\n",
    "                                                  step=STEP,\n",
    "                                                  alpha=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>7</th>\n",
       "      <th>14</th>\n",
       "      <th>21</th>\n",
       "      <th>28</th>\n",
       "      <th>35</th>\n",
       "      <th>42</th>\n",
       "      <th>49</th>\n",
       "      <th>56</th>\n",
       "      <th>63</th>\n",
       "      <th>70</th>\n",
       "      <th>77</th>\n",
       "      <th>84</th>\n",
       "      <th>365</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>27.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.899471</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.827160</td>\n",
       "      <td>0.801587</td>\n",
       "      <td>0.775661</td>\n",
       "      <td>0.753968</td>\n",
       "      <td>0.733938</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.699001</td>\n",
       "      <td>0.683598</td>\n",
       "      <td>0.668110</td>\n",
       "      <td>0.655203</td>\n",
       "      <td>0.682496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.147266</td>\n",
       "      <td>0.157243</td>\n",
       "      <td>0.148924</td>\n",
       "      <td>0.139146</td>\n",
       "      <td>0.143012</td>\n",
       "      <td>0.149859</td>\n",
       "      <td>0.156193</td>\n",
       "      <td>0.161476</td>\n",
       "      <td>0.158454</td>\n",
       "      <td>0.156778</td>\n",
       "      <td>0.158704</td>\n",
       "      <td>0.155166</td>\n",
       "      <td>0.027798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.523810</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.457143</td>\n",
       "      <td>0.404762</td>\n",
       "      <td>0.387755</td>\n",
       "      <td>0.357143</td>\n",
       "      <td>0.317460</td>\n",
       "      <td>0.357143</td>\n",
       "      <td>0.337662</td>\n",
       "      <td>0.309524</td>\n",
       "      <td>0.646575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.696429</td>\n",
       "      <td>0.671429</td>\n",
       "      <td>0.690476</td>\n",
       "      <td>0.683673</td>\n",
       "      <td>0.669643</td>\n",
       "      <td>0.650794</td>\n",
       "      <td>0.642857</td>\n",
       "      <td>0.590909</td>\n",
       "      <td>0.559524</td>\n",
       "      <td>0.654795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.821429</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.761905</td>\n",
       "      <td>0.734694</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.698413</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.701299</td>\n",
       "      <td>0.690476</td>\n",
       "      <td>0.682192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.964286</td>\n",
       "      <td>0.952381</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>0.871429</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.836735</td>\n",
       "      <td>0.794643</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.764286</td>\n",
       "      <td>0.772727</td>\n",
       "      <td>0.761905</td>\n",
       "      <td>0.704110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.976190</td>\n",
       "      <td>0.979592</td>\n",
       "      <td>0.964286</td>\n",
       "      <td>0.952381</td>\n",
       "      <td>0.942857</td>\n",
       "      <td>0.948052</td>\n",
       "      <td>0.892857</td>\n",
       "      <td>0.739726</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             7          14         21         28         35         42   \\\n",
       "count  27.000000  27.000000  27.000000  27.000000  27.000000  27.000000   \n",
       "mean    0.899471   0.857143   0.827160   0.801587   0.775661   0.753968   \n",
       "std     0.147266   0.157243   0.148924   0.139146   0.143012   0.149859   \n",
       "min     0.428571   0.428571   0.523810   0.571429   0.457143   0.404762   \n",
       "25%     0.857143   0.785714   0.714286   0.696429   0.671429   0.690476   \n",
       "50%     1.000000   0.928571   0.857143   0.821429   0.800000   0.761905   \n",
       "75%     1.000000   0.964286   0.952381   0.928571   0.871429   0.833333   \n",
       "max     1.000000   1.000000   1.000000   1.000000   1.000000   0.976190   \n",
       "\n",
       "             49         56         63         70         77         84   \\\n",
       "count  27.000000  27.000000  27.000000  27.000000  27.000000  27.000000   \n",
       "mean    0.733938   0.714286   0.699001   0.683598   0.668110   0.655203   \n",
       "std     0.156193   0.161476   0.158454   0.156778   0.158704   0.155166   \n",
       "min     0.387755   0.357143   0.317460   0.357143   0.337662   0.309524   \n",
       "25%     0.683673   0.669643   0.650794   0.642857   0.590909   0.559524   \n",
       "50%     0.734694   0.714286   0.698413   0.714286   0.701299   0.690476   \n",
       "75%     0.836735   0.794643   0.777778   0.764286   0.772727   0.761905   \n",
       "max     0.979592   0.964286   0.952381   0.942857   0.948052   0.892857   \n",
       "\n",
       "             365  \n",
       "count  27.000000  \n",
       "mean    0.682496  \n",
       "std     0.027798  \n",
       "min     0.646575  \n",
       "25%     0.654795  \n",
       "50%     0.682192  \n",
       "75%     0.704110  \n",
       "max     0.739726  "
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#95% PIs\n",
    "cv_coverage = prediction_int_coverage_cv(cv_test, cv_intervals)\n",
    "df = pd.DataFrame(cv_coverage)\n",
    "df.columns = horizons\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../../results/model_selection/stage1/Trust-ar_coverage_95.csv\n"
     ]
    }
   ],
   "source": [
    "#write 95% coverage to file\n",
    "metric = 'coverage_95'\n",
    "print(f'{TOP_LEVEL}/{STAGE}/{REGION}-{METHOD}_{metric}.csv')\n",
    "df.to_csv(f'{TOP_LEVEL}/{STAGE}/{REGION}-{METHOD}_{metric}.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ambo",
   "language": "python",
   "name": "ambo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
