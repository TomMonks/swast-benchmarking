{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmark accuracy notebook.\n",
    "\n",
    "The results in this notebook represent the benchmark accuracy of the modelling.\n",
    "\n",
    "The results are calculated using the 7 subregion time series in the South West.  \n",
    "\n",
    "> **The notebook generates Tables 3 and 4 in the paper.**\n",
    "\n",
    "* Table 3 file name: ./paper/tables/table3.tex  (horizon summary)\n",
    "* Table 4 file name: ./paper/tables/table4.tex  (forecast distribution by region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "from scipy.stats import norm, t\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()\n",
    "#should be in top level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#path to where I want tables saved.\n",
    "TABLE_PATH = './paper/tables/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in individual results files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mypath = './results/benchmark/'\n",
    "result_files = [f for f in listdir(mypath) if isfile(join(mypath, f))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_mean = pd.DataFrame()\n",
    "results_med = pd.DataFrame()\n",
    "results_mean_std = pd.DataFrame()\n",
    "\n",
    "#hold all mase results from all splits by horizon (Columns)\n",
    "results_mase = pd.DataFrame()\n",
    "results_cover_95 = pd.DataFrame()\n",
    "results_cover_80 = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_measures = ['smape', 'rmse', 'mase', 'coverage_60', 'coverage_70', \n",
    "                  'coverage_80', 'coverage_90', 'coverage_95']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in by error metric\n",
    "for metric in error_measures:\n",
    "    to_read = [filename for filename in result_files if metric in filename]\n",
    "    model_names = [name[:name.index('_')] for name in to_read]\n",
    "    \n",
    "    for filename, model_name in zip(to_read, model_names):\n",
    "        df = pd.read_csv(mypath + filename, index_col=0)\n",
    "\n",
    "        prefix = model_name + '_' + metric\n",
    "        results_mean[prefix + '_mean'] = df.mean()\n",
    "        results_mean[prefix  + '_std'] = df.std()\n",
    "        results_med[prefix + '_med'] = df.median()\n",
    "        results_med[prefix + '_iqr'] = df.quantile(0.75) - df.quantile(0.25)\n",
    "        \n",
    "        results_mean_std[prefix] = results_mean[prefix + '_mean'].map('{:,.2f}'.format) \\\n",
    "            + ' (' + results_mean[prefix  + '_std'].map('{:,.2f}'.format) + ')'\n",
    "        \n",
    "        #get all mase results ignoring trust level\n",
    "        if filename[:5] != 'Trust':\n",
    "            if metric == 'mase':\n",
    "                results_mase = pd.concat([results_mase, df.copy()])\n",
    "            elif metric == 'coverage_80':\n",
    "                results_cover_80 = pd.concat([results_cover_80, df.copy()])\n",
    "            elif metric == 'coverage_95':\n",
    "                results_cover_95 = pd.concat([results_cover_95, df.copy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_invalid(df, coverage):\n",
    "    '''Only includes the sub region results.  Trust level is excluded.'''\n",
    "    \n",
    "    valid = ['Cornwall', 'Devon', 'Dorset', 'Wiltshire', 'BNSSG', \n",
    "             'Gloucestershire', 'Somerset']\n",
    "    \n",
    "    valid = [s + '-fbp-arima_coverage_' + coverage + '_mean' for s in valid]\n",
    "    \n",
    "    return df[valid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Is this section still needed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Variability of PI Coverage across regions in the South West of England\n",
    "\n",
    "days = [7, 14, 21, 28, 35, 42, 49, 56, 63, 70, 77, 84]\n",
    "\n",
    "summary_60 = results_mean.filter(like=\"coverage_60\").filter(like=\"fbp-\").filter(like='mean')\n",
    "##########\n",
    "summary_60 = remove_invalid(summary_60, '60')\n",
    "#########\n",
    "\n",
    "summary_70 = results_mean.filter(like=\"coverage_70\").filter(like=\"fbp-\").filter(like='mean')\n",
    "##########\n",
    "#this limits the results to subregions and excludes the trust.\n",
    "summary_70 = remove_invalid(summary_70, '70')\n",
    "#########\n",
    "\n",
    "summary_80 = results_mean.filter(like=\"coverage_80\").filter(like=\"fbp-\").filter(like='mean')\n",
    "##########\n",
    "summary_80 = remove_invalid(summary_80, '80')\n",
    "#########\n",
    "\n",
    "\n",
    "summary_90 = results_mean.filter(like=\"coverage_90\").filter(like=\"fbp-\").filter(like='mean')\n",
    "##########\n",
    "summary_90 = remove_invalid(summary_90, '90')\n",
    "#########\n",
    "\n",
    "\n",
    "summary_95 = results_mean.filter(like=\"coverage_95\").filter(like=\"fbp-\").filter(like='mean')\n",
    "##########\n",
    "summary_95 = remove_invalid(summary_95, '95')\n",
    "#########\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table 4: Prediction Interval coverage by region. - Full forecast dist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def coverage_summary(data, alpha=0.05):\n",
    "    mean = data.mean()\n",
    "    std = data.std(ddof=1)\n",
    "    n = data.shape[0]\n",
    "    se = std / np.sqrt(n)\n",
    "    z = np.abs(t.ppf(alpha / 2, n - 1))\n",
    "    hw = z * se\n",
    "    lower = mean - hw\n",
    "    upper = mean + hw\n",
    "    ci = pd.DataFrame([mean, lower, upper]).T\n",
    "    ci.columns = ['mean', 'lci', 'uci']\n",
    "    \n",
    "    labels = list(ci.index)\n",
    "    post_fix = data.columns[0][-7:]\n",
    "    labels = [s.replace('-fbp-arima_coverage_' + post_fix, '') for s in labels]\n",
    "    ci.index = labels\n",
    "    return ci"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_single_columns(summary_frame, coverage):    \n",
    "    summary_frame[f'{coverage}%'] = summary_frame['mean'].map('{:,.3f}'.format) \\\n",
    "            + ' (' + summary_frame['lci'].map('{:,.3f}'.format) +  ' - ' + \\\n",
    "                summary_frame['uci'].map('{:,.3f}'.format)+ ')'\n",
    "    \n",
    "    return summary_frame.drop(['mean', 'lci', 'uci'], axis=1).sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_data_frame(coverage_frames):\n",
    "    summary_frame = coverage_frames[0]\n",
    "    \n",
    "    for i in range(1, len(coverage_frames)):\n",
    "        summary_frame = pd.concat([summary_frame, coverage_frames[i]], \n",
    "                                  ignore_index=True, axis=1)\n",
    "        \n",
    "        \n",
    "    return summary_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coverage_frames = []\n",
    "\n",
    "targets = ['60', '70', '80', '90', '95']\n",
    "labels = ['60%', '70%', '80%', '90%', '95%']\n",
    "for target in targets:\n",
    "    raw = results_mean.filter(like=f\"coverage_{target}\").filter(like=\"fbp-\").filter(like='mean')\n",
    "    \n",
    "    ##########\n",
    "    raw = remove_invalid(raw, target)\n",
    "    #########\n",
    "    \n",
    "    summary_frame = coverage_summary(raw)\n",
    "    coverage_frame = convert_to_single_columns(summary_frame, target)\n",
    "    coverage_frames.append(coverage_frame)\n",
    "    \n",
    "#single_data_frame(coverage_frames)\n",
    "\n",
    "summary_frame = pd.concat(coverage_frames, ignore_index=True, axis=1)\n",
    "summary_frame.columns = labels\n",
    "\n",
    "#drop Trust as not necessary for benchmark\n",
    "#summary_frame = summary_frame.drop(['Trust'])\n",
    "\n",
    "summary_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_frame.to_latex(f'{TABLE_PATH}table4.tex')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#is this section still needed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_invalid(df, metric):\n",
    "    valid = ['Cornwall', 'Devon', 'Dorset', 'Wiltshire', 'BNSSG', \n",
    "             'Gloucestershire', 'Somerset']\n",
    "    \n",
    "    valid = [s + '-fbp-arima_' + metric + '_mean' for s in valid]\n",
    "    print(valid)\n",
    "    return df[valid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Variability of MASE across regions in the South West of England\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(12,4), sharey='row')\n",
    "\n",
    "days = [7, 14, 21, 28, 35, 42, 49, 56, 63, 70, 77, 84]\n",
    "\n",
    "summary_mase = results_mean.filter(like=\"mase\").filter(like=\"fbp-\").filter(like='mean')\n",
    "\n",
    "##########\n",
    "summary_mase = remove_invalid(summary_mase, 'mase')\n",
    "#########\n",
    "\n",
    "ax.boxplot(x=summary_mase, labels=days);\n",
    "\n",
    "ax.xaxis.grid(False)\n",
    "ax.set_xlabel('horizon (days)')\n",
    "\n",
    "\n",
    "#fig.savefig('mase_cv_by_region.png', dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#summary of mase by forecast horizon - pool the regions (trust level excluded)\n",
    "#NOTE THESE ARE a summary of MEAN MASE for each forecast horizon.\n",
    "\n",
    "alpha = 0.05\n",
    "\n",
    "mean = summary_mase.mean(axis=1)\n",
    "std = summary_mase.std(axis=1, ddof=1)\n",
    "median = summary_mase.quantile(0.5, axis=1)\n",
    "lowerq = summary_mase.quantile(0.25, axis=1)\n",
    "upperq = summary_mase.quantile(0.75, axis=1)\n",
    "maximum = summary_mase.max(axis=1)\n",
    "minimum = summary_mase.min(axis=1)\n",
    "per_5 = summary_mase.quantile(0.05, axis=1)\n",
    "per_95 = summary_mase.quantile(0.95, axis=1)\n",
    "\n",
    "n = 7\n",
    "\n",
    "#Confidence interval calculation\n",
    "se = std / np.sqrt(n)\n",
    "z = np.abs(t.ppf(alpha / 2, n - 1))\n",
    "hw = z * se\n",
    "lower = mean - hw\n",
    "upper = mean + hw\n",
    "\n",
    "horizon_results = pd.DataFrame(mean, columns=['mean'])\n",
    "horizon_results['lower'] = lower\n",
    "horizon_results['upper'] = upper\n",
    "horizon_results['median'] = median\n",
    "horizon_results['lowerq'] = lowerq\n",
    "horizon_results['upperq'] = upperq\n",
    "horizon_results['upperq'] = upperq\n",
    "horizon_results[r'$P_5$'] = per_5.map('{:,.2f}'.format)\n",
    "horizon_results[r'$P_{95}$'] = per_95.map('{:,.2f}'.format)\n",
    "\n",
    "horizon_results['mean (95% CI)'] = horizon_results['mean'].map('{:,.2f}'.format) \\\n",
    "            + ' (' + horizon_results['lower'].map('{:,.2f}'.format) +  ' - ' + \\\n",
    "                horizon_results['upper'].map('{:,.2f}'.format)+ ')'\n",
    "\n",
    "horizon_results['median (IQR)'] = horizon_results['median'].map('{:,.2f}'.format) \\\n",
    "            + ' (' + horizon_results['lowerq'].map('{:,.2f}'.format) +  ' - ' + \\\n",
    "                horizon_results['upperq'].map('{:,.2f}'.format)+ ')'\n",
    "\n",
    "columns = horizon_results.columns[-2:].to_list()\n",
    "columns += horizon_results.columns[-4:-2].to_list()\n",
    "horizon_results[columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table 3: A summary of forecast accuracy by horizon (all regions pooled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#analyse using data from ALL splits for all regions\n",
    "\n",
    "#summary of mase by forecast horizon - pool the regions (trust level excluded)\n",
    "alpha = 0.05\n",
    "\n",
    "summary_mase = results_mase\n",
    "\n",
    "mean = summary_mase.mean(axis=0)\n",
    "std = summary_mase.std(axis=0, ddof=1)\n",
    "median = summary_mase.quantile(0.5, axis=0)\n",
    "lowerq = summary_mase.quantile(0.25, axis=0)\n",
    "upperq = summary_mase.quantile(0.75, axis=0)\n",
    "maximum = summary_mase.max(axis=0)\n",
    "minimum = summary_mase.min(axis=0)\n",
    "per_5 = summary_mase.quantile(0.05, axis=0)\n",
    "per_95 = summary_mase.quantile(0.95, axis=0)\n",
    "\n",
    "n = results_mase.shape[0]\n",
    "\n",
    "#Confidence interval calculation\n",
    "se = std / np.sqrt(n)\n",
    "z = np.abs(t.ppf(alpha / 2, n - 1))\n",
    "hw = z * se\n",
    "lower = mean - hw\n",
    "upper = mean + hw\n",
    "\n",
    "horizon_results = pd.DataFrame(mean, columns=['mean'])\n",
    "horizon_results['lower'] = lower\n",
    "horizon_results['upper'] = upper\n",
    "horizon_results['median'] = median\n",
    "horizon_results['lowerq'] = lowerq\n",
    "horizon_results['upperq'] = upperq\n",
    "horizon_results['upperq'] = upperq\n",
    "horizon_results[r'$Q_5$'] = per_5.map('{:,.2f}'.format)\n",
    "horizon_results[r'$Q_{95}$'] = per_95.map('{:,.2f}'.format)\n",
    "\n",
    "horizon_results['mean (95% CI)'] = horizon_results['mean'].map('{:,.2f}'.format) \\\n",
    "            + ' (' + horizon_results['lower'].map('{:,.2f}'.format) +  ' - ' + \\\n",
    "                horizon_results['upper'].map('{:,.2f}'.format)+ ')'\n",
    "\n",
    "horizon_results['median (IQR)'] = horizon_results['median'].map('{:,.2f}'.format) \\\n",
    "            + ' (' + horizon_results['lowerq'].map('{:,.2f}'.format) +  ' - ' + \\\n",
    "                horizon_results['upperq'].map('{:,.2f}'.format)+ ')'\n",
    "\n",
    "horizon_results.index.name = 'h'\n",
    "\n",
    "columns = horizon_results.columns[-2:].to_list()\n",
    "columns += horizon_results.columns[-4:-2].to_list()\n",
    "horizon_mase = horizon_results[columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#summary of COVERAGE 80% by forecast horizon - pool the regions (trust level excluded)\n",
    "alpha = 0.05\n",
    "\n",
    "mean = results_cover_80.mean(axis=0)\n",
    "std = results_cover_80.std(axis=0, ddof=1)\n",
    "median = results_cover_80.quantile(0.5, axis=0)\n",
    "lowerq = results_cover_80.quantile(0.25, axis=0)\n",
    "upperq = results_cover_80.quantile(0.75, axis=0)\n",
    "maximum = results_cover_80.max(axis=0)\n",
    "minimum = results_cover_80.min(axis=0)\n",
    "per_5 = results_cover_80.quantile(0.05, axis=0)\n",
    "per_95 = results_cover_80.quantile(0.95, axis=0)\n",
    "\n",
    "n = results_cover_80.shape[0]\n",
    "\n",
    "\n",
    "#Confidence interval calculation\n",
    "se = std / np.sqrt(n)\n",
    "z = np.abs(t.ppf(alpha / 2, n - 1))\n",
    "hw = z * se\n",
    "lower = mean - hw\n",
    "upper = mean + hw\n",
    "\n",
    "horizon_results = pd.DataFrame(mean, columns=['mean'])\n",
    "horizon_results['lower'] = lower\n",
    "horizon_results['upper'] = upper\n",
    "horizon_results['median'] = median\n",
    "horizon_results['lowerq'] = lowerq\n",
    "horizon_results['upperq'] = upperq\n",
    "horizon_results['upperq'] = upperq\n",
    "horizon_results[r'$Q_5$'] = per_5.map('{:,.2f}'.format)\n",
    "horizon_results[r'$Q_{95}$'] = per_95.map('{:,.2f}'.format)\n",
    "\n",
    "horizon_results['mean (95% CI)'] = horizon_results['mean'].map('{:,.3f}'.format) \\\n",
    "            + ' (' + horizon_results['lower'].map('{:,.3f}'.format) +  ' - ' + \\\n",
    "                horizon_results['upper'].map('{:,.3f}'.format)+ ')'\n",
    "\n",
    "horizon_results['median (IQR)'] = horizon_results['median'].map('{:,.2f}'.format) \\\n",
    "            + ' (' + horizon_results['lowerq'].map('{:,.2f}'.format) +  ' - ' + \\\n",
    "                horizon_results['upperq'].map('{:,.2f}'.format)+ ')'\n",
    "\n",
    "horizon_results.index.name = 'h'\n",
    "\n",
    "columns = horizon_results.columns[-2:].to_list()\n",
    "columns += horizon_results.columns[-4:-2].to_list()\n",
    "horizon_80 = horizon_results[columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#summary of COVERAGE 95% by forecast horizon - pool the regions (trust level excluded)\n",
    "alpha = 0.05\n",
    "\n",
    "mean = results_cover_95.mean(axis=0)\n",
    "std = results_cover_95.std(axis=0, ddof=1)\n",
    "median = results_cover_95.quantile(0.5, axis=0)\n",
    "lowerq = results_cover_95.quantile(0.25, axis=0)\n",
    "upperq = results_cover_95.quantile(0.75, axis=0)\n",
    "maximum = results_cover_95.max(axis=0)\n",
    "minimum = results_cover_95.min(axis=0)\n",
    "per_5 = results_cover_95.quantile(0.05, axis=0)\n",
    "per_95 = results_cover_95.quantile(0.95, axis=0)\n",
    "\n",
    "n = results_cover_95.shape[0]\n",
    "\n",
    "#Confidence interval calculation\n",
    "se = std / np.sqrt(n)\n",
    "z = np.abs(t.ppf(alpha / 2, n - 1))\n",
    "hw = z * se\n",
    "lower = mean - hw\n",
    "upper = mean + hw\n",
    "\n",
    "horizon_results = pd.DataFrame(mean, columns=['mean'])\n",
    "horizon_results['lower'] = lower\n",
    "horizon_results['upper'] = upper\n",
    "horizon_results['median'] = median\n",
    "horizon_results['lowerq'] = lowerq\n",
    "horizon_results['upperq'] = upperq\n",
    "horizon_results['upperq'] = upperq\n",
    "horizon_results[r'$Q_5$'] = per_5.map('{:,.2f}'.format)\n",
    "horizon_results[r'$Q_{95}$'] = per_95.map('{:,.2f}'.format)\n",
    "\n",
    "horizon_results['mean (95% CI)'] = horizon_results['mean'].map('{:,.3f}'.format) \\\n",
    "            + ' (' + horizon_results['lower'].map('{:,.3f}'.format) +  ' - ' + \\\n",
    "                horizon_results['upper'].map('{:,.3f}'.format)+ ')'\n",
    "\n",
    "horizon_results['median (IQR)'] = horizon_results['median'].map('{:,.3f}'.format) \\\n",
    "            + ' (' + horizon_results['lowerq'].map('{:,.3f}'.format) +  ' - ' + \\\n",
    "                horizon_results['upperq'].map('{:,.3f}'.format)+ ')'\n",
    "\n",
    "horizon_results.index.name = 'h'\n",
    "\n",
    "columns = horizon_results.columns[-2:].to_list()\n",
    "columns += horizon_results.columns[-4:-2].to_list()\n",
    "horizon_95 = horizon_results[columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "horizon_results = pd.concat([horizon_mase['mean (95% CI)'], \n",
    "                             horizon_80['mean (95% CI)'],\n",
    "                             horizon_95['mean (95% CI)']], axis=1)\n",
    "\n",
    "horizon_results.columns = ['MASE', 'Coverage 80%', \n",
    "                           'Coverage 95%']\n",
    "\n",
    "horizon_results.to_latex(f'{TABLE_PATH}table3.tex')\n",
    "horizon_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Produce an overall mean and 'rule of thumb' benchmark for forecasters\n",
    "\n",
    "## MASE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.05\n",
    "\n",
    "#overall MASE\n",
    "mean = summary_mase.to_numpy().flatten().mean()\n",
    "print(mean)\n",
    "\n",
    "#sample std\n",
    "std = summary_mase.to_numpy().flatten().std(ddof=1)\n",
    "\n",
    "n = summary_mase.to_numpy().flatten().shape[0]\n",
    "\n",
    "#Confidence interval calculation\n",
    "se = std / np.sqrt(n)\n",
    "z = np.abs(t.ppf(alpha / 2, n - 1))\n",
    "hw = z * se\n",
    "lower = mean - hw\n",
    "upper = mean + hw\n",
    "\n",
    "#lower and upper 95% CI\n",
    "print(lower, upper)\n",
    "\n",
    "#median\n",
    "print(np.percentile(summary_mase.to_numpy().flatten(), 50))\n",
    "print(np.percentile(summary_mase.to_numpy().flatten(), 75) - np.percentile(summary_mase.to_numpy().flatten(), 25))\n",
    "\n",
    "#middle 90% of data lies between\n",
    "print(np.percentile(summary_mase.to_numpy().flatten(), 5))\n",
    "print(np.percentile(summary_mase.to_numpy().flatten(), 95))\n",
    "\n",
    "plt.hist(summary_mase.to_numpy().flatten());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sMAPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Variability of sMAPE across regions in the South West of England\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(12,4), sharey='row')\n",
    "\n",
    "days = [7, 14, 21, 28, 35, 42, 49, 56, 63, 70, 77, 84]\n",
    "\n",
    "summary_smape = results_mean.filter(like=\"smape\").filter(like=\"fbp-\").filter(like='mean')\n",
    "ax.boxplot(x=summary_smape, labels=days);\n",
    "\n",
    "ax.xaxis.grid(False)\n",
    "ax.set_xlabel('horizon (days)')\n",
    "\n",
    "#fig.savefig('mase_cv_by_region.png', dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.05\n",
    "\n",
    "#overall MASE\n",
    "mean = summary_smape.to_numpy().flatten().mean()\n",
    "print(mean)\n",
    "\n",
    "#sample std\n",
    "std = summary_smape.to_numpy().flatten().std(ddof=1)\n",
    "print(std)\n",
    "\n",
    "n = summary_smape.to_numpy().flatten().shape[0]\n",
    "\n",
    "#Confidence interval calculation\n",
    "se = std / np.sqrt(n)\n",
    "z = np.abs(t.ppf(alpha / 2, n - 1))\n",
    "hw = z * se\n",
    "lower = mean - hw\n",
    "upper = mean + hw\n",
    "\n",
    "#lower and upper 95% CI\n",
    "print(lower, upper)\n",
    "\n",
    "#median and IQR\n",
    "print(np.percentile(summary_smape.to_numpy().flatten(), 50))\n",
    "print(np.percentile(summary_smape.to_numpy().flatten(), 75) - np.percentile(summary_smape.to_numpy().flatten(), 25))\n",
    "\n",
    "#middle 90% of data lies between\n",
    "print(np.percentile(summary_smape.to_numpy().flatten(), 5))\n",
    "print(np.percentile(summary_smape.to_numpy().flatten(), 95))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Variability of coverage 80 across regions in the South West of England\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(12,4), sharey='row')\n",
    "\n",
    "days = [7, 14, 21, 28, 35, 42, 49, 56, 63, 70, 77, 84]\n",
    "\n",
    "summary_coverage_80 = results_mean.filter(like=\"coverage_80\").filter(like=\"fbp-\").filter(like='mean')\n",
    "ax.boxplot(x=summary_coverage_80, labels=days);\n",
    "\n",
    "ax.xaxis.grid(False)\n",
    "ax.set_xlabel('horizon (days)')\n",
    "\n",
    "#fig.savefig('mase_cv_by_region.png', dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.05\n",
    "\n",
    "#overall cover 80\n",
    "mean = summary_coverage_80.to_numpy().flatten().mean()\n",
    "print(mean)\n",
    "\n",
    "#sample std\n",
    "std = summary_coverage_80.to_numpy().flatten().std(ddof=1)\n",
    "\n",
    "n = summary_coverage_80.to_numpy().flatten().shape[0]\n",
    "\n",
    "#Confidence interval calculation\n",
    "se = std / np.sqrt(n)\n",
    "z = np.abs(t.ppf(alpha / 2, n - 1))\n",
    "hw = z * se\n",
    "lower = mean - hw\n",
    "upper = mean + hw\n",
    "\n",
    "#lower and upper 95% CI\n",
    "print(lower, upper)\n",
    "\n",
    "#median and IQR\n",
    "print(np.percentile(summary_coverage_80.to_numpy().flatten(), 50))\n",
    "print(np.percentile(summary_coverage_80.to_numpy().flatten(), 75) - np.percentile(summary_coverage_80.to_numpy().flatten(), 25))\n",
    "\n",
    "#middle 90% of data lies between\n",
    "print(np.percentile(summary_coverage_80.to_numpy().flatten(), 5))\n",
    "print(np.percentile(summary_coverage_80.to_numpy().flatten(), 95))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Variability of coverage 95 across regions in the South West of England\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(12,4), sharey='row')\n",
    "\n",
    "days = [7, 14, 21, 28, 35, 42, 49, 56, 63, 70, 77, 84]\n",
    "\n",
    "summary_coverage_95 = results_mean.filter(like=\"coverage_95\").filter(like=\"fbp-\").filter(like='mean')\n",
    "ax.boxplot(x=summary_coverage_95, labels=days);\n",
    "\n",
    "ax.xaxis.grid(False)\n",
    "ax.set_xlabel('horizon (days)')\n",
    "\n",
    "#fig.savefig('mase_cv_by_region.png', dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.05\n",
    "\n",
    "#overall cover 95\n",
    "mean = summary_coverage_95.to_numpy().flatten().mean()\n",
    "print(mean)\n",
    "\n",
    "#sample std\n",
    "std = summary_coverage_95.to_numpy().flatten().std(ddof=1)\n",
    "\n",
    "n = summary_coverage_95.to_numpy().flatten().shape[0]\n",
    "\n",
    "#Confidence interval calculation\n",
    "se = std / np.sqrt(n)\n",
    "z = np.abs(t.ppf(alpha / 2, n - 1))\n",
    "hw = z * se\n",
    "lower = mean - hw\n",
    "upper = mean + hw\n",
    "\n",
    "#lower and upper 95% CI\n",
    "print(lower, upper)\n",
    "\n",
    "#median and IQR\n",
    "print(np.percentile(summary_coverage_95.to_numpy().flatten(), 50))\n",
    "print(np.percentile(summary_coverage_95.to_numpy().flatten(), 75) - np.percentile(summary_coverage_95.to_numpy().flatten(), 25))\n",
    "\n",
    "#middle 90% of data lies between\n",
    "print(np.percentile(summary_coverage_95.to_numpy().flatten(), 5))\n",
    "print(np.percentile(summary_coverage_95.to_numpy().flatten(), 95))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overall accuracy summary by region (not in paper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "region_means = results_mean.filter(like=\"mase\").filter(like='mean').mean().sort_index()\n",
    "region_std = results_mean.filter(like=\"mase\").filter(like='std').mean().sort_index()\n",
    "region_95_mean = results_mean.filter(like=\"coverage_95\").filter(like='mean').mean().sort_index()\n",
    "region_80_mean = results_mean.filter(like=\"coverage_80\").filter(like='mean').mean().sort_index()\n",
    "region_95_std = results_mean.filter(like=\"coverage_95\").filter(like='std').mean().sort_index()\n",
    "region_80_std = results_mean.filter(like=\"coverage_80\").filter(like='std').mean().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparisons = list(region_means.index)\n",
    "idx = [i.replace('_mase_mean', '') for i in comparisons]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_regions = pd.DataFrame(region_means.to_numpy(), columns=['mean'])\n",
    "df_regions['std'] = region_std.to_numpy()\n",
    "df_regions['mean_80'] = region_80_mean.to_numpy()\n",
    "df_regions['std_80'] = region_80_std.to_numpy()\n",
    "df_regions['mean_95'] = region_95_mean.to_numpy()\n",
    "df_regions['std_95'] = region_95_std.to_numpy()\n",
    "df_regions['MASE'] = df_regions['mean'].map('{:,.2f}'.format) \\\n",
    "            + ' (' + df_regions['std'].map('{:,.2f}'.format) + ')'\n",
    "\n",
    "df_regions['Coverage 80'] = df_regions['mean_80'].map('{:,.2f}'.format) \\\n",
    "            + ' (' + df_regions['std_80'].map('{:,.2f}'.format) + ')'\n",
    "\n",
    "df_regions['Coverage 95'] = df_regions['mean_95'].map('{:,.2f}'.format) \\\n",
    "            + ' (' + df_regions['std_95'].map('{:,.2f}'.format) + ')'\n",
    "\n",
    "df_regions.index = idx\n",
    "df_regions = df_regions.drop(['mean', 'std', 'mean_80', 'std_80', \n",
    "                              'mean_95', 'std_95'], axis=1)\n",
    "\n",
    "idx = df_regions.index\n",
    "labels = [s.replace('-fbp-arima', '') for s in idx]\n",
    "df_regions.index = labels\n",
    "df_regions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
